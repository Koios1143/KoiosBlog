const nt="ENTRIES",V="KEYS",T="VALUES",F="";class D{set;_type;_path;constructor(t,s){const n=t._tree,u=Array.from(n.keys());this.set=t,this._type=s,this._path=u.length>0?[{node:n,keys:u}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===F)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==F).join("")}value(){return E(this._path).node.get(F)}result(){switch(this._type){case T:return this.value();case V:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],ut=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const u=t.length+1,o=u+s,i=new Uint8Array(o*u).fill(s+1);for(let r=0;r<u;++r)i[r]=r;for(let r=1;r<o;++r)i[r*u]=r;return R(e,t,s,n,i,1,u,""),n},R=(e,t,s,n,u,o,i,r)=>{const d=o*i;t:for(const l of e.keys())if(l===F){const a=u[d-1];a<=s&&n.set(r,[e.get(l),a])}else{let a=o;for(let h=0;h<l.length;++h,++a){const m=l[h],p=i*a,f=p-i;let c=u[p];const g=Math.max(0,a-s-1),_=Math.min(i-1,a+s);for(let y=g;y<_;++y){const b=m!==t[y],z=u[f+y]+ +b,A=u[f+y+1]+1,w=u[p+y]+1,L=u[p+y+1]=Math.min(z,A,w);L<c&&(c=L)}if(c>s)continue t}R(e.get(l),t,s,n,u,a,i,r+l)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[u,o]=M(n);for(const i of u.keys())if(i!==F&&i.startsWith(o)){const r=new Map;return r.set(i.slice(o.length),u.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ot(this._tree,t)}entries(){return new D(this,nt)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return ut(this._tree,t,s)}get(t){const s=I(this._tree,t);return s!==void 0?s.get(F):void 0}has(t){const s=I(this._tree,t);return s!==void 0&&s.has(F)}keys(){return new D(this,V)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,O(this._tree,t).set(F,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);return n.set(F,s(n.get(F))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);let u=n.get(F);return u===void 0&&n.set(F,u=s()),u}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,u]of t)s.set(n,u);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==F&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},I=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==F&&t.startsWith(s))return I(e.get(s),t.slice(s.length))},O=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const o of e.keys())if(o!==F&&t[n]===o[0]){const i=Math.min(s-n,o.length);let r=1;for(;r<i&&t[n+r]===o[r];)++r;const d=e.get(o);if(r===o.length)e=d;else{const l=new Map;l.set(o.slice(r),d),e.set(t.slice(n,n+r),l),e.delete(o),e=l}n+=r;continue t}const u=new Map;return e.set(t.slice(n),u),u}return e},ot=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(F),s.size===0)W(n);else if(s.size===1){const[u,o]=s.entries().next().value;q(n,u,o)}}},W=e=>{if(e.length===0)return;const[t,s]=M(e);if(t.delete(s),t.size===0)W(e.slice(0,-1));else if(t.size===1){const[n,u]=t.entries().next().value;n!==F&&q(e.slice(0,-1),n,u)}},q=(e,t,s)=>{if(e.length===0)return;const[n,u]=M(e);n.set(u+t,s),n.delete(u)},M=e=>e[e.length-1],it=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},rt=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,S="or",$="and",ct="and_not",lt=(e,t)=>{e.includes(t)||e.push(t)},P=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},N=({score:e},{score:t})=>t-e,ht=()=>new Map,k=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,dt={[S]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:u,terms:o,match:i}=t.get(s);n.score=n.score+u,n.match=Object.assign(n.match,i),P(n.terms,o)}}return e},[$]:(e,t)=>{const s=new Map;for(const n of t.keys()){const u=e.get(n);if(u==null)continue;const{score:o,terms:i,match:r}=t.get(n);P(u.terms,i),s.set(n,{score:u.score+o,terms:u.terms,match:Object.assign(u.match,r)})}return s},[ct]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},at=(e,t,s,n,u,o)=>{const{k:i,b:r,d}=o;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/u)))},ft=e=>(t,s,n)=>{const u=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,o=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:u,prefix:o}},H=(e,t,s,n)=>{for(const u of Object.keys(e._fieldIds))if(e._fieldIds[u]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${u}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},gt=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const u=e._index.fetch(n,ht),o=u.get(t);o==null||o.get(s)==null?H(e,s,t,n):o.get(s)<=1?o.size<=1?u.delete(t):o.delete(s):o.set(s,o.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},mt={k:1.2,b:.7,d:.5},pt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(rt),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:S,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:mt},Ft={combineWith:$,prefix:(e,t,s)=>t===s.length-1},_t={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},yt={..._t,...U},Y=(e,t=S)=>{if(e.length===0)return new Map;const s=t.toLowerCase();return e.reduce(dt[s])||new Map},B=(e,t,s,n,u,o,i,r,d=new Map)=>{if(u==null)return d;for(const l of Object.keys(o)){const a=o[l],h=e._fieldIds[l],m=u.get(h);if(m==null)continue;let p=m.size;const f=e._avgFieldLength[h];for(const c of m.keys()){if(!e._documentIds.has(c)){gt(e,h,c,s),p-=1;continue}const g=i?i(e._documentIds.get(c),s,e._storedFields.get(c)):1;if(!g)continue;const _=m.get(c),y=e._fieldLength.get(c)[h],b=at(_,p,e._documentCount,y,f,r),z=n*a*g*b,A=d.get(c);if(A){A.score+=z,lt(A.terms,t);const w=G(A.match,s);w?w.push(l):A.match[s]=[l]}else d.set(c,{score:z,terms:[t],match:{[s]:[l]}})}}return d},At=(e,t,s)=>{const n={...e._options.searchOptions,...s},u=(n.fields||e._options.fields).reduce((c,g)=>({...c,[g]:G(n.boost,g)||1}),{}),{boostDocument:o,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:l,prefix:a}={...J.weights,...i},h=e._index.get(t.term),m=B(e,t.term,t.term,1,h,u,o,d);let p,f;if(t.prefix&&(p=e._index.atPrefix(t.term)),t.fuzzy){const c=t.fuzzy===!0?.2:t.fuzzy,g=c<1?Math.min(r,Math.round(t.term.length*c)):c;g&&(f=e._index.fuzzyGet(t.term,g))}if(p)for(const[c,g]of p){const _=c.length-t.term.length;if(!_)continue;f?.delete(c);const y=a*c.length/(c.length+.3*_);B(e,t.term,c,y,g,u,o,d,m)}if(f)for(const c of f.keys()){const[g,_]=f.get(c);if(!_)continue;const y=l*c.length/(c.length+_);B(e,t.term,c,y,g,u,o,d,m)}return m},X=(e,t,s={})=>{if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(m=>X(e,m,a));return Y(h,a.combineWith)}const{tokenize:n,processTerm:u,searchOptions:o}=e._options,i={tokenize:n,processTerm:u,...o,...s},{tokenize:r,processTerm:d}=i,l=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(ft(i)).map(a=>At(e,a,i));return Y(l,i.combineWith)},K=(e,t,s={})=>{const n=X(e,t,s),u=[];for(const[o,{score:i,terms:r,match:d}]of n){const l=r.length,a={id:e._documentIds.get(o),score:i*l,terms:Object.keys(d),match:d};Object.assign(a,e._storedFields.get(o)),(s.filter==null||s.filter(a))&&u.push(a)}return u.sort(N),u},Ct=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:o,terms:i}of K(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=o,d.count+=1):n.set(r,{score:o,terms:i,count:1})}const u=[];for(const[o,{score:i,terms:r,count:d}]of n)u.push({suggestion:o,terms:r,score:i/d});return u.sort(N),u};class Et{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?yt:t.autoVacuum;this._options={...pt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...Ft,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const u={};for(const[o,i]of n)u[o]=Object.fromEntries(i);t.push([s,u])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:u,fieldLength:o,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:l},a)=>{if(l!==1&&l!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new Et(a);h._documentCount=t,h._nextId=s,h._documentIds=k(n),h._idToShortId=new Map,h._fieldIds=u,h._fieldLength=k(o),h._avgFieldLength=i,h._storedFields=k(r),h._dirtCount=d||0,h._index=new C;for(const[m,p]of h._documentIds)h._idToShortId.set(p,m);for(const[m,p]of e){const f=new Map;for(const c of Object.keys(p)){let g=p[c];l===1&&(g=g.ds),f.set(parseInt(c,10),k(g))}h._index.set(m,f)}return h},Q=Object.entries,wt=Object.fromEntries,j=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),u=[];let o=0,i=0;const r=(l,a=!1)=>{let h="";i===0?h=l.length>20?`… ${l.slice(-20)}`:l:a?h=l.length+i>100?`${l.slice(0,100-i)}… `:l:h=l.length>20?`${l.slice(0,20)} … ${l.slice(-20)}`:l,h&&u.push(h),i+=h.length,a||(u.push(["mark",t]),i+=t.length,i>=100&&u.push(" …"))};let d=s.indexOf(n,o);if(d===-1)return null;for(;d>=0;){const l=d+n.length;if(r(e.slice(o,d)),o=l,i>100)break;d=s.indexOf(n,o)}return i<100&&r(e.slice(o),!0),u},Z=/[\u4e00-\u9fa5]/g,tt=(e={})=>({fuzzy:.2,prefix:!0,processTerm:t=>{const s=t.match(Z)||[],n=t.replace(Z,"").toLowerCase();return n?[n,...s]:[...s]},...e}),xt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),kt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),et=(e,t,s={})=>{const n={};return K(t,e,tt({boost:{h:2,t:1,c:4},...s})).forEach(u=>{const{id:o,terms:i,score:r}=u,d=o.includes("@"),l=o.includes("#"),[a,h]=o.split(/[#@]/),m=i.sort((f,c)=>f.length-c.length).filter((f,c)=>i.slice(c+1).every(g=>!g.includes(f))),{contents:p}=n[a]??={title:"",contents:[]};if(d)p.push([{type:"customField",key:a,index:h,display:m.map(f=>u.c.map(c=>j(c,f))).flat().filter(f=>f!==null)},r]);else{const f=m.map(c=>j(u.h,c)).filter(c=>c!==null);if(f.length&&p.push([{type:l?"heading":"title",key:a,...l&&{anchor:h},display:f},r]),"t"in u)for(const c of u.t){const g=m.map(_=>j(c,_)).filter(_=>_!==null);g.length&&p.push([{type:"text",key:a,...l&&{anchor:h},display:g},r])}}}),Q(n).sort(([,u],[,o])=>"max"==="total"?xt(u,o):kt(u,o)).map(([u,{title:o,contents:i}])=>{if(!o){const r=it(t,u);r&&(o=r.h)}return{title:o,contents:i.map(([r])=>r)}})},st=(e,t,s={})=>Ct(t,e,tt(s)).map(({suggestion:n})=>n),v=wt(Q(JSON.parse("{\"/\":{\"documentCount\":265,\"nextId\":265,\"documentIds\":{\"0\":\"v-184f4da6\",\"1\":\"v-184f4da6#skills\",\"2\":\"v-184f4da6#競賽成績\",\"3\":\"v-184f4da6#活動參與\",\"4\":\"v-620a6165\",\"5\":\"v-620a6165#比賽題目\",\"6\":\"v-620a6165#比賽過程\",\"7\":\"v-620a6165#datasets-處理\",\"8\":\"v-620a6165#答案產出\",\"9\":\"v-620a6165#結果\",\"10\":\"v-620a6165#報告\",\"11\":\"v-620a6165#總結\",\"12\":\"v-620a6165@0\",\"13\":\"v-620a6165@1\",\"14\":\"v-3caeec67\",\"15\":\"v-3caeec67#basic-information\",\"16\":\"v-3caeec67#問題描述\",\"17\":\"v-3caeec67#related-works\",\"18\":\"v-3caeec67#never-give-up\",\"19\":\"v-3caeec67#intrinsic-reward\",\"20\":\"v-3caeec67#uvfa\",\"21\":\"v-3caeec67#rl-loss\",\"22\":\"v-3caeec67#ngu-agent\",\"23\":\"v-3caeec67#ngu-的問題\",\"24\":\"v-3caeec67#methodology\",\"25\":\"v-3caeec67#state-action-value-function-parameterization\",\"26\":\"v-3caeec67#adaptive-exploration-over-a-family-of-policies-bandit\",\"27\":\"v-3caeec67#upper-confidence-bound-algorithm-ucb\",\"28\":\"v-3caeec67#sliding-window-ucb\",\"29\":\"v-3caeec67#simplified-sliding-window-ucb\",\"30\":\"v-3caeec67#backprop-through-time-window-size\",\"31\":\"v-3caeec67#high-level-architecture\",\"32\":\"v-3caeec67#actors\",\"33\":\"v-3caeec67#results\",\"34\":\"v-3caeec67#settings\",\"35\":\"v-3caeec67#state-action-value-function-parameterization-1\",\"36\":\"v-3caeec67#backprop-through-time-window-size-1\",\"37\":\"v-3caeec67#adaptive-exploration\",\"38\":\"v-3caeec67#summary\",\"39\":\"v-3caeec67#discussion\",\"40\":\"v-3caeec67#contribution\",\"41\":\"v-3caeec67#值得一看的文章們\",\"42\":\"v-3caeec67@0\",\"43\":\"v-3caeec67@1\",\"44\":\"v-c0336012\",\"45\":\"v-c0336012#basic-information\",\"46\":\"v-c0336012#what-is-domain-adaption\",\"47\":\"v-c0336012#問題描述\",\"48\":\"v-c0336012#related-works\",\"49\":\"v-c0336012#domain-alignment\",\"50\":\"v-c0336012#pseudo-labelling-or-self-training\",\"51\":\"v-c0336012#mixing\",\"52\":\"v-c0336012#methodology\",\"53\":\"v-c0336012#naive-mixing-to-uda\",\"54\":\"v-c0336012#domain-adaption-via-corss-domain-mixed-sampling-dacs\",\"55\":\"v-c0336012#results\",\"56\":\"v-c0336012#實驗設定\",\"57\":\"v-c0336012#dataset\",\"58\":\"v-c0336012#cityscapes\",\"59\":\"v-c0336012#gta5\",\"60\":\"v-c0336012#synthia\",\"61\":\"v-c0336012#gta5-cityscapes\",\"62\":\"v-c0336012#synthia-cityscapes\",\"63\":\"v-c0336012#some-issues-about-evaluation\",\"64\":\"v-c0336012#contribution\",\"65\":\"v-c0336012#值得一看的文章們\",\"66\":\"v-c0336012@0\",\"67\":\"v-c0336012@1\",\"68\":\"v-6fdb6976\",\"69\":\"v-6fdb6976#basic-information\",\"70\":\"v-6fdb6976#問題描述\",\"71\":\"v-6fdb6976#related-works\",\"72\":\"v-6fdb6976#methodology\",\"73\":\"v-6fdb6976#self-training-for-uda\",\"74\":\"v-6fdb6976#daformer-network-architecture\",\"75\":\"v-6fdb6976#rare-class-sampling-rcs\",\"76\":\"v-6fdb6976#thing-class-imagenet-feature-distance-fd\",\"77\":\"v-6fdb6976#learning-rate-warmup-for-uda\",\"78\":\"v-6fdb6976#results\",\"79\":\"v-6fdb6976#實驗設定\",\"80\":\"v-6fdb6976#summary\",\"81\":\"v-6fdb6976#learning-rate-warmup\",\"82\":\"v-6fdb6976#rare-class-sampling-rcs-1\",\"83\":\"v-6fdb6976#thing-class-imagenet-feature-distance-fd-1\",\"84\":\"v-6fdb6976#daformer-decoder\",\"85\":\"v-6fdb6976#contribution\",\"86\":\"v-6fdb6976#值得一看的文章們\",\"87\":\"v-6fdb6976@0\",\"88\":\"v-6fdb6976@1\",\"89\":\"v-32d63a0d\",\"90\":\"v-32d63a0d#basic-information\",\"91\":\"v-32d63a0d#問題描述\",\"92\":\"v-32d63a0d#related-works\",\"93\":\"v-32d63a0d#q-networks\",\"94\":\"v-32d63a0d#td-gammon\",\"95\":\"v-32d63a0d#收斂性相關研究\",\"96\":\"v-32d63a0d#nfq\",\"97\":\"v-32d63a0d#methodology\",\"98\":\"v-32d63a0d#results\",\"99\":\"v-32d63a0d#實驗設定\",\"100\":\"v-32d63a0d#評估方式\",\"101\":\"v-32d63a0d#比較基準\",\"102\":\"v-32d63a0d#contribution\",\"103\":\"v-32d63a0d@0\",\"104\":\"v-32d63a0d@1\",\"105\":\"v-073f61cf\",\"106\":\"v-073f61cf#basic-information\",\"107\":\"v-073f61cf#問題描述\",\"108\":\"v-073f61cf#related-works\",\"109\":\"v-073f61cf#denoising-autoencoders-daes\",\"110\":\"v-073f61cf#methodology\",\"111\":\"v-073f61cf#overview\",\"112\":\"v-073f61cf#model-description\",\"113\":\"v-073f61cf#learning-dropout-nets\",\"114\":\"v-073f61cf#results\",\"115\":\"v-073f61cf#datasets\",\"116\":\"v-073f61cf#result-on-image-datasets\",\"117\":\"v-073f61cf#result-on-speech-recognition\",\"118\":\"v-073f61cf#result-on-text-dataset\",\"119\":\"v-073f61cf#how-dropout-effect-network\",\"120\":\"v-073f61cf#hyperparameter\",\"121\":\"v-073f61cf#effect-of-data-size\",\"122\":\"v-073f61cf#contribution\",\"123\":\"v-073f61cf#值得一看的文章們\",\"124\":\"v-073f61cf@0\",\"125\":\"v-073f61cf@1\",\"126\":\"v-25c9f246\",\"127\":\"v-25c9f246#basic-information\",\"128\":\"v-25c9f246#問題描述\",\"129\":\"v-25c9f246#related-works\",\"130\":\"v-25c9f246#methodology\",\"131\":\"v-25c9f246#preliminary\",\"132\":\"v-25c9f246#overview\",\"133\":\"v-25c9f246#context-and-detail-crop\",\"134\":\"v-25c9f246#multi-resolution-fusion\",\"135\":\"v-25c9f246#pseudo-label-generation-with-overlapping-sliding-window\",\"136\":\"v-25c9f246#results\",\"137\":\"v-25c9f246#實驗設定\",\"138\":\"v-25c9f246#overview-1\",\"139\":\"v-25c9f246#influence-of-resolution-and-crop-size-on-uda\",\"140\":\"v-25c9f246#crop-size-selection\",\"141\":\"v-25c9f246#memory-usage-comparison\",\"142\":\"v-25c9f246#ablation-study\",\"143\":\"v-25c9f246#contribution\",\"144\":\"v-25c9f246#值得一看的文章們\",\"145\":\"v-25c9f246@0\",\"146\":\"v-25c9f246@1\",\"147\":\"v-5b18c8c4\",\"148\":\"v-5b18c8c4#basic-information\",\"149\":\"v-5b18c8c4#問題描述\",\"150\":\"v-5b18c8c4#related-works\",\"151\":\"v-5b18c8c4#parameter-space-noise-for-exploration\",\"152\":\"v-5b18c8c4#dqn\",\"153\":\"v-5b18c8c4#double-dqn\",\"154\":\"v-5b18c8c4#dueling-dqn\",\"155\":\"v-5b18c8c4#a3c\",\"156\":\"v-5b18c8c4#methodology\",\"157\":\"v-5b18c8c4#基本想法\",\"158\":\"v-5b18c8c4#減少產-random-number-時間\",\"159\":\"v-5b18c8c4#dqn-dueling-dqn\",\"160\":\"v-5b18c8c4#distributed-a3c\",\"161\":\"v-5b18c8c4#results\",\"162\":\"v-5b18c8c4#experiments\",\"163\":\"v-5b18c8c4#analysis\",\"164\":\"v-5b18c8c4#contribution\",\"165\":\"v-5b18c8c4#值得一看的文章們\",\"166\":\"v-5b18c8c4@0\",\"167\":\"v-5b18c8c4@1\",\"168\":\"v-d4413c4c\",\"169\":\"v-d4413c4c#basic-information\",\"170\":\"v-d4413c4c#問題描述\",\"171\":\"v-d4413c4c#related-works\",\"172\":\"v-d4413c4c#methodology\",\"173\":\"v-d4413c4c#基本的-uda-loss-設定\",\"174\":\"v-d4413c4c#pixel-wise-contrastive-learning\",\"175\":\"v-d4413c4c#patch-wise-contrastive-learning\",\"176\":\"v-d4413c4c#結合\",\"177\":\"v-d4413c4c#results\",\"178\":\"v-d4413c4c#實驗設定\",\"179\":\"v-d4413c4c#quantitative-comparison\",\"180\":\"v-d4413c4c#qualitative-results\",\"181\":\"v-d4413c4c#ablation-studies\",\"182\":\"v-d4413c4c#contribution\",\"183\":\"v-d4413c4c@0\",\"184\":\"v-d4413c4c@1\",\"185\":\"v-0fd9e004\",\"186\":\"v-0fd9e004#basic-information\",\"187\":\"v-0fd9e004#問題描述\",\"188\":\"v-0fd9e004#related-works\",\"189\":\"v-0fd9e004#methodology\",\"190\":\"v-0fd9e004#preliminary\",\"191\":\"v-0fd9e004#target\",\"192\":\"v-0fd9e004#prototypical-pseudo-label-denoising\",\"193\":\"v-0fd9e004#權重計算\",\"194\":\"v-0fd9e004#prototype-計算\",\"195\":\"v-0fd9e004#loss-計算\",\"196\":\"v-0fd9e004#structure-learning-by-enforcing-consistency\",\"197\":\"v-0fd9e004#distillation-to-self-supervised-model\",\"198\":\"v-0fd9e004#整體流程\",\"199\":\"v-0fd9e004#results\",\"200\":\"v-0fd9e004#實驗設定\",\"201\":\"v-0fd9e004#gta5-cityscapes\",\"202\":\"v-0fd9e004#synthia-cityscapes\",\"203\":\"v-0fd9e004#contribution\",\"204\":\"v-0fd9e004#值得一看的文章們\",\"205\":\"v-0fd9e004@0\",\"206\":\"v-0fd9e004@1\",\"207\":\"v-ed4def16\",\"208\":\"v-ed4def16#basic-information\",\"209\":\"v-ed4def16#問題描述\",\"210\":\"v-ed4def16#rnn\",\"211\":\"v-ed4def16#cnn\",\"212\":\"v-ed4def16#self-attention\",\"213\":\"v-ed4def16#related-works\",\"214\":\"v-ed4def16#recurrent-neural-network-rnn\",\"215\":\"v-ed4def16#seq2seq\",\"216\":\"v-ed4def16#attention\",\"217\":\"v-ed4def16#methodology\",\"218\":\"v-ed4def16#scaled-dot-product-attention\",\"219\":\"v-ed4def16#masking\",\"220\":\"v-ed4def16#multi-head-attention\",\"221\":\"v-ed4def16#encoder-and-decoder-stacks\",\"222\":\"v-ed4def16#position-wise-feed-forward-networks\",\"223\":\"v-ed4def16#embeddings-and-softmax\",\"224\":\"v-ed4def16#positional-encoding\",\"225\":\"v-ed4def16#results\",\"226\":\"v-ed4def16#why-self-attention\",\"227\":\"v-ed4def16#實驗設定\",\"228\":\"v-ed4def16#model-variations-實驗結果\",\"229\":\"v-ed4def16#english-constituency-parsing-實驗結果\",\"230\":\"v-ed4def16#contribution\",\"231\":\"v-ed4def16#值得一看的文章們\",\"232\":\"v-ed4def16@0\",\"233\":\"v-ed4def16@1\",\"234\":\"v-3501ffcb\",\"235\":\"v-3501ffcb#契機\",\"236\":\"v-3501ffcb#啟程\",\"237\":\"v-3501ffcb#生活安頓\",\"238\":\"v-3501ffcb#持續安頓生活和探索\",\"239\":\"v-3501ffcb#在不安與焦躁當中的放鬆\",\"240\":\"v-3501ffcb#後記\",\"241\":\"v-3501ffcb@0\",\"242\":\"v-3501ffcb@1\",\"243\":\"v-ad1b5b16\",\"244\":\"v-ad1b5b16#basic-information\",\"245\":\"v-ad1b5b16#問題描述\",\"246\":\"v-ad1b5b16#related-works\",\"247\":\"v-ad1b5b16#knowledge-augumented-agent-planning\",\"248\":\"v-ad1b5b16#methodology\",\"249\":\"v-ad1b5b16#preliminaries\",\"250\":\"v-ad1b5b16#整體流程\",\"251\":\"v-ad1b5b16#task-knowledge-synthesis\",\"252\":\"v-ad1b5b16#state-knowledge-summarization\",\"253\":\"v-ad1b5b16#model-training\",\"254\":\"v-ad1b5b16#agent-planning-with-world-knowledge-model-1\",\"255\":\"v-ad1b5b16#results\",\"256\":\"v-ad1b5b16#實驗設定\",\"257\":\"v-ad1b5b16#dataset-與環境\",\"258\":\"v-ad1b5b16#模型架構與-baseline\",\"259\":\"v-ad1b5b16#training-and-inference-setups\",\"260\":\"v-ad1b5b16#實驗結果\",\"261\":\"v-ad1b5b16#contribution\",\"262\":\"v-ad1b5b16@0\",\"263\":\"v-ad1b5b16@1\",\"264\":\"v-e1e3da16\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[2,29],\"1\":[1,35],\"2\":[1,80],\"3\":[1,56],\"4\":[4,15],\"5\":[1,42],\"6\":[1,86],\"7\":[2,35],\"8\":[1,13],\"9\":[1,13],\"10\":[1,20],\"11\":[1,24],\"12\":[null,null,1],\"13\":[null,null,2],\"14\":[6],\"15\":[2,13],\"16\":[1,115],\"17\":[2],\"18\":[3,18],\"19\":[2,75],\"20\":[1,46],\"21\":[2,114],\"22\":[2,41],\"23\":[2,58],\"24\":[1],\"25\":[5,132],\"26\":[9,91],\"27\":[6,83],\"28\":[3,56],\"29\":[4,52],\"30\":[5,30],\"31\":[3,13],\"32\":[1,80],\"33\":[1],\"34\":[1,56],\"35\":[5,137],\"36\":[5,39],\"37\":[2,144],\"38\":[1,63],\"39\":[1],\"40\":[1,22],\"41\":[1,41],\"42\":[null,null,1],\"43\":[null,null,5],\"44\":[8],\"45\":[2,19],\"46\":[4,49],\"47\":[1,49],\"48\":[2],\"49\":[2,77],\"50\":[6,108],\"51\":[1,66],\"52\":[1],\"53\":[4,54],\"54\":[9,67],\"55\":[1],\"56\":[1,37],\"57\":[1,15],\"58\":[1,13],\"59\":[1,19],\"60\":[1,28],\"61\":[3,48],\"62\":[3,39],\"63\":[4,52],\"64\":[1,19],\"65\":[1,52],\"66\":[null,null,1],\"67\":[null,null,7],\"68\":[12],\"69\":[2,24],\"70\":[1,49],\"71\":[2,10],\"72\":[1],\"73\":[4,149],\"74\":[3,166],\"75\":[5,60],\"76\":[7,112],\"77\":[5,13],\"78\":[1],\"79\":[1,42],\"80\":[1,45],\"81\":[3,22],\"82\":[5,48],\"83\":[7,63],\"84\":[2,43],\"85\":[1,22],\"86\":[1,20],\"87\":[null,null,1],\"88\":[null,null,7],\"89\":[6],\"90\":[2,16],\"91\":[1,60],\"92\":[2,16],\"93\":[2,85],\"94\":[2,20],\"95\":[1,32],\"96\":[1,25],\"97\":[1,94],\"98\":[1],\"99\":[1,52],\"100\":[1,50],\"101\":[1,49],\"102\":[1,19],\"103\":[null,null,1],\"104\":[null,null,5],\"105\":[10],\"106\":[2,14],\"107\":[1,43],\"108\":[2],\"109\":[4,35],\"110\":[1],\"111\":[1,33],\"112\":[2,82],\"113\":[3,28],\"114\":[1],\"115\":[1,22],\"116\":[4,115],\"117\":[4,26],\"118\":[4,29],\"119\":[4,81],\"120\":[1,43],\"121\":[4,28],\"122\":[1,6],\"123\":[1,3],\"124\":[null,null,1],\"125\":[null,null,4],\"126\":[9],\"127\":[2,14],\"128\":[1,75],\"129\":[2,10],\"130\":[1],\"131\":[1,164],\"132\":[1,44],\"133\":[4,123],\"134\":[3,154],\"135\":[7,52],\"136\":[1],\"137\":[1,34],\"138\":[1,53],\"139\":[8,65],\"140\":[3,43],\"141\":[3,50],\"142\":[2,54],\"143\":[1,28],\"144\":[1,25],\"145\":[null,null,1],\"146\":[null,null,7],\"147\":[4],\"148\":[2,13],\"149\":[1,40],\"150\":[2,19],\"151\":[5,78],\"152\":[1,70],\"153\":[2,43],\"154\":[2,92],\"155\":[1,129],\"156\":[1],\"157\":[1,86],\"158\":[4,48],\"159\":[2,55],\"160\":[2,71],\"161\":[1],\"162\":[1,73],\"163\":[1,61],\"164\":[1,12],\"165\":[1,49],\"166\":[null,null,1],\"167\":[null,null,5],\"168\":[13],\"169\":[2,12],\"170\":[1,67],\"171\":[2,6],\"172\":[1,12],\"173\":[4,103],\"174\":[4,82],\"175\":[4,63],\"176\":[1,29],\"177\":[1],\"178\":[1,26],\"179\":[2,63],\"180\":[2,16],\"181\":[2,64],\"182\":[1,21],\"183\":[null,null,1],\"184\":[null,null,8],\"185\":[13],\"186\":[2,22],\"187\":[1,83],\"188\":[2,10],\"189\":[1],\"190\":[1,33],\"191\":[1,75],\"192\":[4,86],\"193\":[1,43],\"194\":[2,46],\"195\":[2,59],\"196\":[5,117],\"197\":[5,45],\"198\":[1,52],\"199\":[1],\"200\":[1,35],\"201\":[4,37],\"202\":[4,11],\"203\":[1,19],\"204\":[1,37],\"205\":[null,null,1],\"206\":[null,null,7],\"207\":[5],\"208\":[2,17],\"209\":[1],\"210\":[1,37],\"211\":[1,25],\"212\":[2,33],\"213\":[2,6],\"214\":[5,36],\"215\":[1,31],\"216\":[1,119],\"217\":[1,20],\"218\":[4,55],\"219\":[1,51],\"220\":[3,110],\"221\":[4,104],\"222\":[5,26],\"223\":[3,23],\"224\":[2,37],\"225\":[1],\"226\":[4,78],\"227\":[1,102],\"228\":[3,20],\"229\":[4,18],\"230\":[1,25],\"231\":[1,33],\"232\":[null,null,1],\"233\":[null,null,6],\"234\":[3,17],\"235\":[1,13],\"236\":[1,95],\"237\":[1,52],\"238\":[1,37],\"239\":[1,33],\"240\":[1,27],\"241\":[null,null,1],\"242\":[null,null,6],\"243\":[6],\"244\":[2,22],\"245\":[1,91],\"246\":[2],\"247\":[4,50],\"248\":[1],\"249\":[1,108],\"250\":[1,12],\"251\":[3,59],\"252\":[3,69],\"253\":[2,137],\"254\":[6,139],\"255\":[1],\"256\":[1],\"257\":[2,135],\"258\":[2,106],\"259\":[4,43],\"260\":[1,188],\"261\":[1,56],\"262\":[null,null,1],\"263\":[null,null,5],\"264\":[1]},\"averageFieldLength\":[2.4436674283601683,52.09461518986487,0.8156354907453711],\"storedFields\":{\"0\":{\"h\":\"About Me\",\"t\":[\"本名林禾堃，一個喜愛資訊領域的人。目前就讀於清華大學資訊工程學系，過去曾擔任臺南一中資訊社社長，也是 SCIST 的共同創辦人之一。\",\"高中接觸了演算法、資訊安全、網路管理等領域，目前正在朝向 Deep Learning 領域發展，關注的主題包含 Computer Vision、Reinforcement Learning 以及 Large Language Model。\",\"希望透過這個 blog 紀錄學習的點滴，也歡迎一起來討論 ML 領域的各種知識！\"]},\"1\":{\"h\":\"Skills\",\"t\":[\"Programming Languagues\",\"C/C++, Python, JavaScripts\",\"Frameworks\",\"React, Hexo, LINE BOT, PyTorch\",\"Machine Learning\",\"Computer Vision, Reinforcement Learning\",\"Miscellaneous\",\"UNIX Programming, Cryptography, Reverse engineering, Git, Markdown, Vim\",\"Languages\",\"Mandarin (Native), English (TOEFL 81), Japanese (JLPT N1)\"]},\"2\":{\"h\":\"競賽成績\",\"t\":[\"日期\",\"競賽名稱\",\"成績\",\"心得\",\"2024年01月\",\"2024 TSMC CareerHack\",\"無\",\"心得\",\"2022年10月\",\"梅竹黑客松\",\"分組第三名\",\"2022年07月\",\"YTP Final Project\",\"專題第三名\",\"2020年07月\",\"青年黑克松\",\"網頁組第3名\",\"2020年06月\",\"AIS3 pre-exam\",\"第96名\",\"2020年06月\",\"MyFirstCTF\",\"第22名\",\"2020年04月\",\"Google Code Jam Qualification Round\",\"第29755名(30/100分)\",\"2020年03月\",\"Kick Start Round A 2020\",\"第5695名(21/100分)\",\"2020年03月\",\"TOI初選\",\"第62名(212/500分)\",\"2020年02月\",\"TOI校內賽\",\"第7名(314/600分)\",\"2020年01月\",\"TOI海選\",\"300/300分\",\"2019年12月\",\"NPSC決賽\",\"全國第六名\",\"2019年11月\",\"金盾獎\",\"進入決賽\",\"2019年10月\",\"Kick Start Round G 2019\",\"第2427名(10/100分)\",\"2019年07月\",\"FML-based Machine Learning Competition for Human\",\"世界第三名\",\"2019年06月\",\"第七屆高一生程式設計排名賽\",\"128/800\"]},\"3\":{\"h\":\"活動參與\",\"t\":[\"日期\",\"活動\",\"心得/筆記\",\"2023年04月-\",\"機器學習讀書會 成員\",\"2022年10月-\",\"日語學習小組 成員\",\"2023年09月\",\"NTHU CS Camp 講師\",\"2022年07月\",\"IONCamp 營長\",\"2022年\",\"資訊之芽 C 班講師\",\"2022年08月\",\"索拉教育 Python 課程講師\",\"2022年07月\",\"索拉教育 C++ 課程講師\",\"2020年09月\",\"AIS3 CLUB\",\"2020年08月\",\"奧義科技參訪\",\"2020年08月\",\"SITCON 2020\",\"2020年07月\",\"AIS3 2020\",\"2020年07月\",\"2020 ISIP SummerCamp\",\"筆記\",\"2020年-2021年\",\"SCIST\",\"2020年01月\",\"IOICamp\",\"2019年11月\",\"臺灣好厲駭 Machine Learning & Security\",\"筆記\",\"2019年08月\",\"HITCON 2019\",\"2019年08月\",\"HITCON HackDoor\",\"2019年07月\",\"2019 My FirstSecurity Summer Camp\",\"2019年07月\",\"第八屆成功大學大學生活體驗營\",\"2019年03月\",\"SITCON 2019\"]},\"4\":{\"h\":\"2024 TSMC CareerHack 心得\",\"t\":[\"前幾天去參加了 2024 台積電的黑客松，大概是人生第一次走進台積辦公室。\",\"這場比賽是一組四人的比賽，前面有一個預賽，需要解出一些簡單的演算法題目。每個人題目會不太相同，但基本上都不會太難，簡單的 Sort、Greedy、Graph、DP。\"]},\"5\":{\"h\":\"比賽題目\",\"t\":[\"我們這一組拿到的是 AI 看圖說故事 的題目，基本上就是會有一些工地的照片，希望我們可以去找到\",\"照片中有多少人\",\"有多少人有戴安全帽\",\"有多少人沒戴安全帽\",\"安全帽是甚麼顏色\",\"有些圖片上面會有 warning message，所以會有額外的提問\",\"warning message 寫了什麼\",\"有沒有任何人違反了 warning message 的敘述\",\"基本上他們期待我們會運用 LLM 去解決這個問題，TSMC 也有先提供了一些資源\",\"GCP 運算及儲存資源 \",\"L4 GPU (24 GB RAM) x2\",\"100 GB CPU RAM\",\"100 GB Disk\",\"100 GB Bucket storage\",\"LLaVA pretrained model\",\"LLaVA finetune script\",\"一些 Datasets\"]},\"6\":{\"h\":\"比賽過程\",\"t\":[\"比賽會在正式開賽前一周公布題目，也開放資源，因此不少組別在實際進到 HackDay 前就已經做了不少，也有聽說有部分的組別 HackDay 就是拿來做簡報，也是頗有趣。\",\"雖然說是黑客松，不過場地因為是辦公室，所以晚上 6 點就要回家，隔天早上 9 點半再來報到，實際上在辦公室的時間沒有想像中的還要多。\",\"一開始進去到辦公室的感覺就很舒服，可以自己使用的免費咖啡機、電動的升降桌、超級舒服的人體工學椅、超大的雙螢幕。\",\"只能說在設備上直接贏了。\",\"中間還有提供午餐、點心、飲料，都相當地好吃，覺得很開心。\",\"我們這一組在 HackDay 之前的想法是先去做一些 paper research，去調查看看有哪些其他還不錯的 LLM 可以嘗試。\",\"雖然說在比賽之前我們的想法是，如果只會問那些固定的問題的話，那我們不要用 LLM，用其他 CV 的 model 去解決也許會比 LLM 還要強許多，不過寄信去詢問之後得到希望還是使用到 LLM 的回覆，所以我們後續的方向都著重在 LLM 以及 Fine-tune 的研究。\",\"大致上大家看過了幾個 LLM\",\"miniGPT4、miniGPT4_v2\",\"BLIP、InstructBLIP\",\"Flamingo\",\"Fine-tune 的部分主要是參考各個 paper 自己的 fine-tune 說明，其他的大概就是 proxy-tuning。\",\"其實讀 paper 都覺得很好理解，也想說難得有不錯的運算資源，是也可以都 train 看看結果如何。不過理想很美好，現實很骨感。\",\"實際上我們挑了最小的 13B 模型，丟進去訓練還是出現 CUDA out of memory，估計也是沒救，最後只有 LLaVA 活下來，所以我們後續就主要專攻 LLaVA。\"]},\"7\":{\"h\":\"Datasets 處理\",\"t\":[\"我們在 Dataset 處理上花了不少的時間。\",\"在提供的 5 份 datasets 當中，我們發現其中兩份都是教室的監視器錄影畫面，我們想說這裡根本沒人戴安全帽，超級懷疑這個 dataset 的實用程度。\",\"此外，其他的 dataset 也是有些包含浮水印，或是看起來安全帽是後製貼上去的，彩度跟亮度跟環境有些落差。\",\"我們也發現到說在回答顏色的那一題，模型會傾向回答 None，但實際上有顏色才對，所以在前面加上了一些 prefix string，試圖讓 LLM 吐出更多結果。\",\"實驗上為了檢測拿掉兩個 datasets 以及加上 prefix string 是否有比較好，交叉做了一些 fintune，也寫了一個評分程式去評估好壞。\"]},\"8\":{\"h\":\"答案產出\",\"t\":[\"我們最後決定要把多個模型的輸出拿去做類似 Bagging 的操作。\",\"簡單來說，這七個問題，我們相信那些回答分數比較高的模型可以做得比較好，所以就讓他專門來回答這個問題。\",\"如此一來就可以得到完整的輸出結果，也可以盡可能至少在 validation set 上看起來很棒 ouo\"]},\"9\":{\"h\":\"結果\",\"t\":[\"最後在 private set 上的分數大概是 59.7\",\"加權分數: 44.5/70\",\"Bonus: 2.7/5\",\"其實不太好啦XD\"]},\"10\":{\"h\":\"報告\",\"t\":[\"其實 TSMC 的大家都很友善，報告期間也都會跟我們分享他們覺得在每個地方有哪些比較好的做法也許可以嘗試看看。\",\"此外，其他組的報告我們也可以透過實時的直播去看，認識到其他組都用了怎樣的方法去解決。\",\"印象中有人用了 YOLO，也有人套了 OCR，把輸出結果套入 LLM 的輸入。有些組別完成度很高，甚至連 FrontEnd 都完成了，相當佩服。\"]},\"11\":{\"h\":\"總結\",\"t\":[\"我覺得這次到 TSMC 的比賽經驗很不錯，也讓我認識到 TSMC 的 IT team，跟過去自己想像當中在無塵室裏面處理晶圓的那種印象是完全不同，我也能感受到每個員工對我們都很友善。\",\"當時有遇到問題去找他們的時候都可以得到即時的 feedback，也能感受到他們對我們的提問的重視，是一個讓人很喜歡的環境。\",\"這次大概是第一次實際碰 LLM，過去基本上就是看過 paper，讀的時候都覺得嗯嗯嗯很有道理，不過實際上在實作的時候光是硬體的資源可能就是一大障礙，也就很難往下一步去發展。\",\"但總結來說這次有還蠻有趣的體驗，感到很開心，也很期待未來也還有機會可以繼續了解和開發 LLM。\"]},\"12\":{\"c\":[\"Feedbacks\"]},\"13\":{\"c\":[\"TSMC\",\"CareerHack\"]},\"14\":{\"h\":\"Agent57: Outperforming the Atari Human Benchmark\"},\"15\":{\"h\":\"Basic Information\",\"t\":[\"Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, et al. @ Google DeepMind\",\"2020 ICML\"]},\"16\":{\"h\":\"問題描述\",\"t\":[\"在 RL 當中，Atari games 是一個相當重要的 benchmark。過去的 RL 模型已經能夠在大多的 atari games 當中獲得相當不錯的 performance，例如 MuZero、R2D2，分別在 57 個遊戲當中有 51 和 52 個遊戲是 outperform 人類的。不過可惜的是，在剩下的遊戲當中這些 SoTA 就通常完全沒辦法學習。\",\"Info\",\"稍微翻了一下 MuZero 以及 R2D2 兩篇 paper 的結果，分別是這些遊戲 performance 不太好。\",\"MuZero \",\"montezuma revenge, pitfall, private eye, skiing, solaris, venture\",\"R2D2 \",\"montezuma revenge, pitfall, private eye, skiing, solaris\",\"那麼，剩下這些遊戲有怎樣的共通點呢？\",\"skiing 和 solaris 這兩款遊戲中我們發現到他們都需要相當長的時間之後才會得到 reward，在遊戲的過程當中並不會馬上知道現在這個操作對未來會有正面或是負面的影響。\",\"Skiing game on Atari 2600. Video from TheLimeyDragon\",\"以 Skiing 這款遊戲來說，玩家要操作角色滑雪，途中要盡可能快速通過指定數量的 gates。每忽略一個 gate 就會多 5 秒的 penalty。Reward 會一直到遊戲的最後依照最後通過的時間決定。\",\"剩下的四款遊戲則是因為環境太大，又有不少的 negative reward，需要相當大量的探索之後才能得到 positive reward。\",\"Pitfall game on Atari 2600. Video from The No Swear Gamer\",\"以 Ptifall 這款遊戲來說，玩家要操作主角在 20 分鐘的時間探索 255 個遊戲場景，去找到藏在地圖當中的寶藏。過程中有許多陷阱，找到寶藏可以加分，最後分數越多越好。\",\"從這些觀察當中可以得到兩個待改善的地方\",\"long-term credit assignment 如何決定哪些 action 應該要給 positive 或是 negative reward\",\"exploration 如何讓 agent 能夠盡可能去正確探索環境 \",\"之所以說\\\"正確\\\"，是因為即便是在很多 negative reward 的地方，也需要嘗試越過那些障礙，也許才有機會遇到 positive reward。\",\"這一篇 paper 希望改善這兩個對 RL 相當重要的問題，也提出了一個可以在所有 57 Atari games 都 outperform 人類的 RL 模型。\"]},\"17\":{\"h\":\"Related Works\"},\"18\":{\"h\":\"Never Give Up\",\"t\":[\"Never Give Up(NGU) 目的也是希望能夠讓 RL agent 能夠在上述 hard-exploration 的環境當中有更好的成效。具體來說 NGU 包含了幾個重要的部分。\",\"Intrinsic Reward\",\"UVFA\",\"RL Loss\",\"NGU Agent\"]},\"19\":{\"h\":\"Intrinsic Reward\",\"t\":[\"在 Intrinsic Reward 的部分目的也是希望能夠促使 agent 多多探索，他們將 reward 分成了兩個部分，分別是 per-episode noveltyrtepisodic​ 以及 life-long noveltyαt​。這兩者分別會讓 agent 鼓勵去探索那些在 episode 當中、在整個訓練過程當中沒有踏足過的狀態。而整體 intrinsic Reward 如下。\",\"rti​=rtepisodic​⋅min{max{αt​,1},L}(L=5)\",\"min 和 max 只是用來限制 life-long novelty 的範圍，避免太大或是太小。\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"而整體的 reward 依照過去 curiosity-driven exploration 的研究，設定如下。\",\"rtβi​​=rte​+βi​rti​\",\"rte​ 是 Extrinsic Reward，在 RL 當中就是環境給予的 reward\",\"rti​ 是 Intrinsic Reward，也就是前面定義的 reward\",\"βi​ 用來調整兩種 reward 的影響程度\",\"不同的環境下需要的 exploration 以及 exploitation 是不同的。當 β 比較大的時候，intrinsic reward 會使得 agent 比較傾向去試試看那些不熟的 state，反之則會去走那些比較熟悉的。\"]},\"20\":{\"h\":\"UVFA\",\"t\":[\"NGU 接下來用 Universal Value Function Approximator, UVFA 去近似 action value function Q。\",\"Q(st​,at​,βi​)=E[rt+1βi​​+γi​rt+2βi​​+γ2rt+3βi​​+…∣st​,at​,βi​]\",\"針對不同的 βi​，NGU 會選擇不同的 γ。\",\"βi​ 大，傾向 exploration，不需要看太遠，γ 選小一些\",\"βi​ 小，傾向 exploitation，需要看遠一些，γ 選大一些\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"左邊是 β 選擇的分布，右邊是 γ 的分布。\"]},\"21\":{\"h\":\"RL Loss\",\"t\":[\"既然有 NN 去逼近，那也就會有 Loss。NGU 計算 Loss 的方式是採用 Transformed Retrace Double Q-learning Loss。\",\"Retrace 是一個可以用來評估或是用在 control 上的 RL 演算法。在這邊我們在意的是評估的部分，Retrace 可以幫助我們去評估如果我們 follow policy μ，在目標的 policy π 的 action value function Qπ 可以拿到多少 Reward。\",\"首先定義從 policy μ 當中取得的 trajectories τ\",\"τ=(xt​,at​,rt​,xt+1​)t∈N​\",\"考慮有限的 sampled sequences，定義 Retrace operator\",\"T^Q(xt​,at​)=Q(xt​,at​)+s=t∑t+k−1​γs−t(i=t+1∏s​ci​)δs​\",\"其中\",\"δt​cs​​=rt​+γa∈A∑​π(a∣xt+1​)Q(xt+1​,a)−Q(xt​,at​)=λmin(1,μ(as​∣xs​)π(as​∣xs​)​)​\",\"實際上訓練的 NN 會有兩個，就跟 DQN 一樣，一個是 target network，一個是 online network。Target network 就可以透過 Retrace operation 去得到目標 yt​^​\",\"yt​^​=T^Q(xt​,at​;θ−)\",\"θ− 是 target network 的 parameter。\",\"有了目標，也就能夠得到 Loss\",\"L(xt​,at​,θ)=(Q(xt​,at​,θ)−yt​^​)2\",\"Tips\",\"上面提及的是單純的 Retrace Double Q-learning Loss，實際上還會為了讓 NN 更好學習，改成 Transformed 版本。\",\"ThQ(x,a)=Eμ​[h(h−1(Q(x,a))+t≥0∑​γt(s=1∏t​cs​)δth​)]\",\"其中\",\"δth​=rt​+γa∈A∑​π(a∣xt+1​)h−1(Q(xt+1​,a)−h−1Q(xt​,at​))\",\"∀z∈R,h(z)∀z∈R,h−1(z)​=sgn(z)(∣z∣+1​−1)+ϵz=sgn(z)((2ϵ1+4ϵ(∣z∣+1+ϵ)​−1​)−1)​\",\"但數學有點太難，我還沒有理解這一段做了什麼。\"]},\"22\":{\"h\":\"NGU Agent\",\"t\":[\"NGU 基本上使用了 R2D2，只不過輸入上會丟\",\"Action at−1​\",\"Extrinsic Reward rt−1e​\",\"Intrinsic Reward rt−1i​\",\"βi​\",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"NGU 採用分散式學習，有許多的 actor 使用不同的 βi​ 取得不同的 experience 丟在 replay buffer，然後再讓 learner 使用 experience 去更新參數學習。\",\"最後只需要設定 β=0，就可以得到單純 exploitation 的模型當成最後的結果。\"]},\"23\":{\"h\":\"NGU 的問題\",\"t\":[\"實作上 NGU 有時會很不穩定、難以收斂，尤其當 rti​ 和 rte​ 的大小、分布相當不同時 \",\"Agent57 的作者認為是因為 NGU 只用了一個 NN 去學習導致\",\"不是那麼地 general \",\"解決了一些 hard-exploration 的問題，卻在一些簡單的問題做得很差 \",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\",\"每種 policy(不同 βi​ 的選擇) sample 的 experience 數量相同 \",\"有些 policy 對於學習是並沒有幫助的，但是卻跟其他人有同樣的影響力\",\"有些環境需要更多的 exploration，有些則不需要\",\"無法好好處理 long-term credit assignment 問題 \",\"例如在 skiiing 以及 solaris 就做得頗差 \",\"Image from Adrià Puigdomènech Badia, Pablo Sprechmann et al. (2020)\"]},\"24\":{\"h\":\"Methodology\"},\"25\":{\"h\":\"State-Action Value Function Parameterization\",\"t\":[\"Agent57 首先針對 State-Action Value Function 拆開來，用兩個 NN 分別去針對 Extrinsic 以及 Intrinsic Reward 處理。\",\"Q(x,a,j;θ)=Q(x,a,j;θe)+βj​Q(x,a,j;θi)\",\"x: state\",\"a: action\",\"j: 表示使用的是哪一個 policy 的 one-hot vector\",\"θe: 近似 Extrinsic Reward re 的 NN\",\"θi: 近似 Intrinsic Reward ri 的 NN\",\"θ: θe∪θi\",\"兩個 Q-Network 都會接收同樣的 state 和 action，並且也是 follow 相同的 policy π。\",\"π(x)=arga∈Amax​Q(x,a,j;θ)\",\"兩個模型都是使用 Transformed Retrace Loss，跟 NGU 是一樣的，不過在計算 Loss 時 reward 的部分是分別給 re 和 ri。\",\"細節上，因為是一次更新 B 個 batch，每個 batch sample 的 sequence 大小為 H，因此 Loss 會有兩組總和。\",\"L(D,θ,θ−,π,μ,r,h)=b=0∑B−1​s=t∑t+H−1​(Q(xsb​,asb​;θ)−T^r,hμ,π​Q(xsb​,asb​;θ−))2\",\"D 表示從 μ sample 出來的 trajectories\",\"θ 為 online network 的參數\",\"θ− 為 target network 的參數\",\"π 為目標 policy\",\"μ 為當前 policy\",\"r 表示 reward，上面的差異就是這裡傳入的分別是 re 和 ri\",\"h 為 Transformed Retrace Operator 的 h\",\"xsb​ 是在 batch b、時間 s 的 state\",\"asb​ 是在 batch b、時間 s 的 action\",\"於是 Agent57 的模型變成底下的樣子。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"Note: 雖然兩個模型都會把 intrinsic 以及 extrinsic reward 輸入進去，但 Loss 在計算上分別都只會拿自己的。\",\"Tips\",\"作者也在論文當中證明了這種拆開來訓練的方法是等價於沒有拆開來訓練的樣子，也就是說這種做法的正確性是被確保的(無論是否有使用 Transformed 的版本)。\",\"不過實際上訓練時因為拆開來訓練，能夠使模型更好去學習各自的 reward，以達到更好的訓練成效。\",\"透過拆開訓練，解決了 NGU 不穩定、難以收斂的問題。\"]},\"26\":{\"h\":\"Adaptive Exploration over a Family of Policies (Bandit)\",\"t\":[\"「每種 policy sample 的 experience 數量相同」這個問題 Agent57 透過加上 Meta-controller 來解決。\",\"Tips\",\"如果每個 actor 都能夠學習什麼時候該 exploit、什麼時候該 explore，選擇出現傾向，不同 policy 就有不同重要程度了\",\"舉一個例子來說，NGU 會把每個 actor 都當成是工廠生產出來的機器人，每一個 actor 一開始都是一樣的。\",\"接下來依照你的需求不同，你分別把這幾個 actor 加上不同的偏好，有些傾向 exploration，有些傾向 exploitation。\",\"這些 actor 就會去環境當中互動，蒐集一些 experience 給你學習。\",\"另一方面，Agent57 的 actor 天生就有一些自己的偏好，有人天生愛探險，有人天生愛保險。\",\"但是他們在跟環境互動的過程當中會慢慢發現到自己的性格怎樣調整會在這個環境當中獲得更好的 reward。\",\"最後你一樣可以透過這些 actor 蒐集的 experience 去學習。但是 policy 不會被固定下來，具有更高的靈活性。\",\"照著這樣的想法，Agent57 讓每個 actor 前面都加上一組 Meta-controller，在每一個 episode 開始之前，透過它決定接下來要使用的 (βj​,γj​)。此外，Meta-controller 也會依據得到的 reward 去調整選擇不同 j 的機率。\",\"如此一來，每個 actor 就會因為 Meta-controller 的存在，產生出選擇 policy 的傾向，進而使得整體訓練採用的 experience 中 policy 的比例改變。\",\"Warning\",\"細節上，每個 actor 選擇 action 都是採用 ϵl​-greedy，其中的 l 表示不同的 actor。亦即，不同 actor 採用不同的 ϵ 大小，也因為如此，Meta-controller 是每個 actor 各有一個。\"]},\"27\":{\"h\":\"Upper Confidence Bound Algorithm (UCB)\",\"t\":[\"Agent57 把 Meta-controller 簡單設計成一個 Multi-Arm Bandit (MAB) 問題，也就是說我現在面前有 N 個 action {0,…,N−1} 可以選擇，在時間 k 你選擇 Ak​，目標是在整個 horizon K 當中你可以得到最好的 return，也就是讓底下的期望值最大化。\",\"Eπ​[k=0∑K−1​Rk​(Ak​)]\",\"過去對於 MAB 在 reward 的分布是固定的狀況下會使用 UCB 來解決它。基本的想法是對每個不同的選擇去評估每個決策的信賴區間的上界，把這個上界當成是它預期的 return，選擇其中最大的當成這次的選擇。\",\"未知/嘗試次數少的選擇 (不確定性高，要傾向 exploration) \",\"平均 Return 低 ➡️ UCB 高 ➡️ 探索機率高\",\"平均 Return 高 ➡️ UCB 更高 ➡️ 探索機率更高\",\"已知/嘗試次數多的選擇 (不確定性低，要傾向 exploitation) \",\"平均 Return 低 ➡️ UCB 低 ➡️ 嘗試機率低\",\"平均 Return 高 ➡️ UCB 高 ➡️ 嘗試機率高\",\"Ak​={kargmax1≤a≤N​μ^​k−1​(a)+βNk−1​(a)log(k−1)​​​∀0≤k≤N−1∀N≤k≤K−1​\",\"其中\",\"Nk​(a)μ^​k​(a)​=m=0∑k−1​1{Am​=a}​=Nk​(a)1​m=0∑k−1​Rk​(a)1{Am​=a}​​\",\"也就是說\",\"Nk​(a) 用來表示一個 action a 至今被嘗試的次數\",\"μ^​k​(a) 用來表示一個 action a 至今平均的 Return\",\"從式子當中也可以觀察到，確實它會傾向讓 平均 Return 高 或是 嘗試次數少 的選項有更高機率被選擇到。\"]},\"28\":{\"h\":\"Sliding-Window UCB\",\"t\":[\"然而，如果 reward 的分布會變動的話，單純的 UCB 並不會是一個好的選項，因為過去的經驗即便在現實狀況改變仍然有大影響力。而隨著 agent 更新、行為模式改變，reward 的分布也會變動。\",\"這裡的經驗指的是一個 action 採取的次數以及得到的 Return 平均 (Nk​(a) 和 μ^​k​(a))。\",\"因此 Sliding-Window UCB 加上了一個 window length τ∈N∗ 來限制要考慮多久之前的經驗。\",\"τ 的選擇應遠比 K 小。\",\"Ak​={kargmax1≤a≤N​μ^​k−1​(a,τ)+βNk−1​(a,τ)log(min(k−1,τ))​​​∀0≤k≤N−1∀N≤k≤K−1​\",\"其中\",\"Nk​(a,τ)μ^​k​(a,τ)​=m=max(0,k−τ)∑k−1​1{Am​=a}​=Nk​(a,τ)1​m=max(0,k−τ)∑k−1​Rk​(a)1{Am​=a}​​\",\"僅僅是加上 τ 而已，剩餘的都是相同的。\"]},\"29\":{\"h\":\"Simplified Sliding-Window UCB\",\"t\":[\"最後，Agent57 對 Sliding-Window UCB 做了兩個小修正\",\"log 對於結果並不會有影響，可以移除\",\"多加上 ϵ-greedy\",\"Ak​=⎩⎨⎧​kargmax1≤a≤N​μ^​k−1​(a,τ)+βNk−1​(a,τ)1​​Yk​​∀0≤k≤N−1∀N≤k≤K−1,Uk​≥ϵUCB​∀N≤k≤K−1,Uk​<ϵUCB​​\",\"其中\",\"ϵUCB​ 是一個 hyperparameter\",\"Uk​ 是一個 [0,1] 之間均勻分布的隨機值\",\"Yk​ 是一個 {0,…,N−1} 之間均勻分布的隨機 action\",\"Tips\",\"透過 Bandit，每個 actor 能夠調整自己的 (γ,β)，解決了 NGU「不是那麼地 general」、「每種 policy sample 的 experience 數量相同」這兩個問題。\"]},\"30\":{\"h\":\"Backprop Through Time Window Size\",\"t\":[\"原先 R2D2 在 Replay buffer 的設計是採用 trace length 80 搭配 replay period 40，作者在實驗當中發現如果採用 trace length 160 搭配 replay period 80，也就是 long trace 的話，對於 long-term credit assignment 的問題似乎能夠得到改善。\",\"Tips\",\"透過 long trace 解決了 NGU「無法好好處理 long-term credit assignment」的問題。\"]},\"31\":{\"h\":\"High-level architecture\",\"t\":[\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"32\":{\"h\":\"Actors\",\"t\":[\"每個 episode 開始前，透過各自的 Meta-Controller 選擇出一組 (γj​,βj​)\",\"透過上一個 trajectory (xt​,rt−1e​,rt−1i​,at−1​,ht−1​) 估計當前 state-action value Q(xt​,⋅,j,θl​)\",\"透過 ϵl​-greedy 選擇 action\",\"計算 intrinsic reward rti​\",\"環境中取得 observation xt+1​, extrinsic reward rte​\",\"若已經又經過 400 個 frames，更新模型參數\",\"重複 2 直到 episode 結束\",\"將 trajectories 交給 replay buffer\",\"ϵl​ 的選擇根據 Dan Horgan, John Quan, David Budden, et al. (2018) 如下\",\"ϵl​=ϵ1+αL−11​\",\"其他部分基本上都跟 NGU 相同。總之，Actors 去跟環境互動，取得 experience 之後交給 replay buffer，Learner 會從 replay buffer 當中 sample 一些 experience 學習，然後繼續跟環境互動。\"]},\"33\":{\"h\":\"Results\"},\"34\":{\"h\":\"Settings\",\"t\":[\"Agent57 在 γ 的分布上有做了一點調整，範圍變成 [0.99,0.9999]，具體來說如下圖\",\"image\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"其他 Hyperparameter 的設定詳閱論文的 Appendix G，這裡就不贅述。\",\"對於每個實驗的 Agent 都另外加上一個 Evaluator 去紀錄訓練過程當中的 undiscounted episode returns。\",\"此外，他們並不是採用 Human Normalized Scores (HNS)，而是 Capped Human Normalized Scores (CHNS)，這個測量標準比較強調那些 HNS 比較差的結果，也限制了數值範圍，因此會比較能夠好好評估 general performance。\",\"CHNS=max{min{HNS,1},0}\",\"其中\",\"HNS=Humanscore​−Randomscore​Agentscore​−Randomscore​​\"]},\"35\":{\"h\":\"State-Action Value Function Parameterization\",\"t\":[\"我們透過 intrinsic 以及 extrinsic 拆開來解決 NGU 的缺陷，這裡要來實驗這一個做法實際上帶來多少影響。\",\"作者建構一個簡單的 15×15 Gridworld random coin。在每個 episode 開始之前他們把一個 agent 以及一個 coin 隨機地放在地圖上的任意格子。Agent 能夠上、下、左、右移動，並且每個 episode 最多 200 個 steps。當 Agent 走到 coin 會得到 reward 1，然後結束這個 episode。\",\"接著作者比較 NGU 以及 NGU 加上 separate network 的做法。如同前面提及 βj​ 如果選擇較大，由於 intrinsic reward 有較大的影響，agent 會偏向 exploration，反之則是 exploitation。細節上，βj​ 的設定會透過 β 來調整整體 βj​ 的大小。\",\"image\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"作者比較兩個模型在不同 β 的大小下，各自最傾向 exploration (βj​=maxj​βj​) 以及最傾向 exploitation (βj​=0) 的設定取得的 Extrinsic Reward。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"x 軸表示 β (注意並不是 βj​)\",\"y 軸表示 extrinsic reward\",\"紫色圓點表示 βj​=0，最傾向 exploitation 的狀況\",\"綠色圓點表示 βj​=maxj​βj​，最傾向 exploration 的狀況\",\"從結果可以發現到 NGU 在不同 β 的設定下會大程度影響到最終 exploitation 的結果，即便這個環境設定是相當簡單的，最終 Return 的趨勢仍然是隨著 β 越大變得越小。\",\"另一方面，加上了 separate network 的狀況下 exploitation 的 return 基本上都相當接近 1.0，也就是說能夠順利到達 coin 所在的位置。\",\"在 exploration 的部分也可以發現到兩者的發展方向會稍有不同。但整體來說兩者都能在最後趨近於 0.0。\",\"由此可見，當 β 提升，由於 intrinsic reward 與 extrinsic reward 的大小相差越來越懸殊，導致 NGU 並無法好好只透過一個 NN 去學習，進而影響到結果，較不具有彈性。相對的，增加 separate network 確實能夠帶來相當好的效益。\",\"此外，作者也發現如果把 Agent57 的 separate network 移除，performance 會掉 20% 以上，可見 separate network 的重要性。\",\"作者也發現到 separate network 在最傾向 exploration 的模型會盡可能避開 coin，反之會走出最短路。\",\"Tips\",\"值得一提的是，這個結果如果在取得 coin 之後仍然不會停止的話就不會出現。\"]},\"36\":{\"h\":\"Backprop Through Time Window Size\",\"t\":[\"在 trace length 以及對應的 replay period 有多少影響呢？\",\"作者將 R2D2 以及 Agent57 分別用 small trace 以及 long trace 來比較，作者認為在這兩者都有一個共通點：Long trace 會導致訓練前期較為緩慢，但最後能取得更好的 performance。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"在 10 個比較難的遊戲當中測試的結果\",\"尤其在 Solaris 這一款遊戲，可以看到比較明顯的結果。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"37\":{\"h\":\"Adaptive Exploration\",\"t\":[\"最後是針對 Meta-Controller 的實驗。作者將 R2D2+sep. network 以及 NGU+sep. network 拿來比較加上 Meta-Conroller 以及沒有的狀況。\",\"在 10 個比較困難的遊戲當中，可以發現到加上 Meta-Controller(圖片中以 bandit 表示)後可以得到更好的成效。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"此外，從上面的圖片中也可以觀察到這樣的 improvement 在 NGU 當中是小許多的。可以認為 separate networks 跟 meta-controller 之間有一些重疊的 benifit\",\"另一方面，有了 meta-controller 之後，即便 discount factor γ 異常地大(如 γ=0.9999)，模型還是能夠順利學習。在下表當中可以看到 high gamma 的 R2D2，在搭配了 meta-controller 之後得到的成效在 10 款比較困難的遊戲當中有些甚至是能夠比 Average Human 還要強。\",\"Games\",\"R2D2(Retrace) high gamma\",\"Average Human\",\"beam rider\",\"349971.96 ± 5595.38\",\"16926.50\",\"freeway\",\"32.84 ± 0.06\",\"29.60\",\"montezuma revenge\",\"1664.89 ± 1177.26\",\"4753.30\",\"pitfall\",\"0.00 ± 0.00\",\"6463.70\",\"pong\",\"21.00 ± 0.00\",\"14.60\",\"private eye\",\"22480.31 ± 10362.99\",\"69571.30\",\"skiing\",\"-4596.26 ± 601.04\",\"-4336.90\",\"solaris\",\"14814.76 ± 11361.16\",\"12326.70\",\"surround\",\"10.00 ± 0.00\",\"6.50\",\"venture\",\"1774.89 ± 83.79\",\"1187.50\",\"因此作者認為 meta-controller 提供了更大的普遍性，即便在參數比較異常的狀況下仍然能有很不錯的學習成果。\",\"最後，作者也觀察了在幾款遊戲訓練過程中當中 Meta-Controller 在每個 bandit 選擇中最大的 return 分別落在哪個 bandit，可以發現到不同的遊戲會有不同的偏好。從這裡也可以了解到實際上讓每個 actor 自己調整 policy、適應不同的環境，實際上是有幫助的。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"38\":{\"h\":\"Summary\",\"t\":[\"最後比較 R2D2、NGU、Agent57、MuZero 在所有 Atari games 的優劣，可以發現到 MuZero 雖然在 uncapped mean 有最好的結果，但是在 capped mean 卻是最差的。顯示了 MuZero 在限定幾款遊戲有特別出色的成效，但並不 general。\",\"同時也可以看到 Agent57 有最大的 Capped Mean 100，亦即 Agent57 能夠在所有的 Atari games 當中獲得比人類平均還要好的成果，除了展現驚人的成果以外，也說明了 Agent57 的普遍性。\",\"同時也能在 R2D2 與 R2D2 bandit 的比較當中明顯看到在所有的成績都有所提升，再次說明了 Meta-Controller 帶來的效益。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\",\"最後，Agent57 透過 separate networks、Meta-Controller、long trace 解決了 NGU 的四個缺陷，最終在所有的 Atari games 當中都獲得了超過人類的成效。\",\"Image from Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski et al. (2020)\"]},\"39\":{\"h\":\"Discussion\"},\"40\":{\"h\":\"Contribution\",\"t\":[\"提出透過 separate networks 解決訓練不穩定、難以收斂的問題\",\"提出 Meta-Controller 來讓每個 actor 自適應不同環境，使模型具有更好的普遍性，並且不同 policy 得以有不同程度的影響\",\"第一個能夠在所有 Atari games 都獲得比 Average Human 更好的成效\"]},\"41\":{\"h\":\"值得一看的文章們\",\"t\":[\"Agent57: Outperforming the human Atari benchmark\",\"Recurrent Neural Networks in Reinforcement Learning\",\"MAB - UCB <> TS 基本概念\",\"Safe and efficient off-policy reinforcement learning.\",\"Never Give Up: Learning Directed Exploration Strategies\",\"Adapting Behaviour for Learning Progress\",\"Agent57\",\"Distributed Prioritized Experience Replay\",\"Recurrent experience replay in distributed reinforcement learning\"]},\"42\":{\"c\":[\"Note\"]},\"43\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"ICML\"]},\"44\":{\"h\":\"DACS: Domain Adaptation via Cross-domain Mixed Sampling\"},\"45\":{\"h\":\"Basic Information\",\"t\":[\"2020 Release\",\"2021 WACV(Winter Conference on Applications of Computer Vision)\",\"Chalmers University of Technology(查爾摩斯理工大學)與 Volvo Cars 共同發表\"]},\"46\":{\"h\":\"What is Domain Adaption\",\"t\":[\"Image from Medium\",\"所謂的 Domain 就是用來描述一群資料他們的分布狀況。\",\"Domain Adaption 的目標是把兩個不同分佈的 Domain (Source Domain 以及 Target Domain) 投射到同一個平面上，使得同類型的資料會相近，反之則相遠。\",\"舉一個在 CV 上的例子。如果我們想要訓練一個模型去做自駕車的街景物件偵測，很多時候我們並不會直接去蒐集真實的資料，像是直接有一台車會去蒐集真實街景資料，這樣所需要的成本會過大。時常我們會訓練在合成資料上(synethic data)，然後再應用在真實的世界當中。\",\"Image from Medium\",\"不過這種情況下一個直覺的問題是，在 虛擬世界(Source Domain) 上也許我們能夠對各種物件去做標記 label，但是對於真實世界(Target Domain)往往會有許多我們沒有的 label、環境與虛擬世界有差距，這種差距被描述為 Domain Shift。當兩個 Domain 相差過大，Domain Shift 過高，就會導致單純在 Source Domain 上訓練的模型難以直接 apply 到 Target Domain 上。\",\"因此，Domain Adaption 想解決的就是盡可能地將 Domain Shift 降低，讓我們得以用較低的成本在虛擬環境中訓練模型，然後應用在真實的環境當中。\"]},\"47\":{\"h\":\"問題描述\",\"t\":[\"近年來透過 CNN 處理 semantic segmentation(影像分割) 的模型雖然有許多，也獲得不錯的成果，不過如果遇到新的 domain，往往就會 work 不太好，尤其是從 synethic data 轉變到 real data 上的時候。\",\"問題在於不同的 domain，各自的 domain distribution 會不同。只訓練在 source domain 的模型對於 target domain 的狀況缺乏認知，導致預測失準。\",\"Info\",\"這就像是同理心，因為缺乏對他人的理解，擅自用自己的思維解讀，就會導致互相的不理解。\",\"Image from Liang-ChiehChen et al. (2015)\",\"可以發現單純用 CNN 就可以得到相當好的影像分割結果。\",\"Image from Yiheng Zhang et al. (2018)\",\"直接把訓練在虛擬環境的模型應用在真實環境，結果相當糟糕。\"]},\"48\":{\"h\":\"Related Works\"},\"49\":{\"h\":\"Domain Alignment\",\"t\":[\"透過 adversarial learning (對抗式學習) 去拉近 source domain 以及 target domain。\",\"我們可以想成現在 Segmentation Network 就是 GAN 的 Generator，然後會有一個 Discriminator 去判別現在給我的究竟是 source domain 還是 target domain 的預測結果。\",\"Image from Yi-Hsuan Tsai et al. (2018)\",\"兩個 Domain 中各取圖片，經過相同的 Segmentation Network，將產出的 semantic maps 做對抗式學習\",\"Info\",\"依照 alignment 的不同，可以分成 pixel level, feature map level, semantic level 等不同的做法。\",\"這樣的做法之所以可行，是源自於即便 domain 不同，在 semantic maps 上的 spatial layout 以及 local context 通常並不會差太多。\",\"DACS 的做法之所以能夠成功，也有部分是源自於這樣的相似性帶來的好處。\",\"Tips\",\"同樣以自駕車的例子來說，即便 synethic data 和 real data 的 domain 有相當大的差異，不過像是馬路、汽車、行人都還是會跟地板黏在一起，其他像是路燈、號誌、天空之類的就通常會像是在半空中。這類的 spatial layout 就相當地雷同。\",\"Image from Yi-Hsuan Tsai et al. (2018)\"]},\"50\":{\"h\":\"pseudo labelling (or self-training)\",\"t\":[\"Adversarial Learning for Semi-Supervised Semantic Segmentation\",\"最初是為了解決 半監督式學習(Semi-Supervise Learning, SSL) 而被提出的。\",\"Info\",\"所謂的半監督式學習也就是說 target domain 的資料上只有一些 labeled data，其他絕大多都是 unlabeled data，這種狀況下訓練模型就被稱為半監督式學習。\",\"而半監督式學習困難的點在於雖然對於 Target Domain 有部分的認知，但是並不全面。\",\"一個簡單的方法是想辦法給這些 unlabeled data 一些 pseudo label。那我們就可以用 supervise learning 的方法解決了。\",\"舉例來說，先在 labeled data 上訓練一個模型，透過這個模型我們就有辦法給 unlabeled data 做 prediction，而 prediction 的結果就當作是他的 pseudo label，就可以再拿去 fine-tune model 了。\",\"Image from Sylwia Majchrowska et al. (2021)\",\"但主要的問題來自於 Domain Shift，畢竟 Source Domain 和 Target Domain 還是存在差異的，並不是所有的 Target Data 都能夠透過 Source Data 去轉移出來。\",\"尤其在 Unsupervised Domain Adaption(UDA) 來說是相當大的問題，在 UDA 當中通常 Domain Shift 都會特別大。\",\"Info\",\"所謂的 UDA 也就是說我們對於 Target Domain 的資料不存在任何 label。換句話說，我們對於 Target Domain 缺乏 label 上的認知。\",\"對於 UDA 來說由於缺乏對於 Target Domain 的認識，一個常見的問題是產出的結果通常會傾向去預測結果為常見的 class。\",\"Info\",\"對陌生人的認識，往往先從貼標籤開始。\",\"例如說在自駕車的道路辨識當中鄰近人行道這種時常出現的 class，如果出現道路或甚至機車，有可能就被誤判成人行道。或是汽車比卡車更常見，導致卡車時常被預測成汽車。\",\"Image from Yang Zou et al (2018)\",\"看 column 4，只有 pseudo labeling 的例子\",\"Info\",\"雖然已經有 paper 提出如 CBST 的方法來降低這種問題，但在邊界上往往還是難以有好的結果。\"]},\"51\":{\"h\":\"Mixing\",\"t\":[\"Mixing 基本上就是從 training image 拿出兩張，透過一些方式混在一起，產生一個新的 training image。最初被用於把 unlabeled image 混合成新的圖片，是一種 data augumentation 的技巧。\",\"像是 Mixup 這種 data augumentation 方法也是屬於 Mixing 的一種。\",\"DACS 當中使用的是 ClassMix 這種 Mixing 方法。\",\"具體來說，ClassMix 的步驟\",\"把兩個圖片 (A,B) 先轉成 semantic map (SA​,SB​)\",\"把 SA​ 其中一半的 classes 對應的 semantic map 做出一個 binary mask (M)\",\"把 mask M apply 在 A 上，跟 B 合成出 XA​。\",\"把 mask M apply 在 SA​ 上，跟 SB​ 合成出 XA​ 對應的 semantic map YA​\",\"Image from Viktor Olsson et al. (2020)\",\"這樣的做法有趣的是能夠將 semantic segmentation 在邊界上往往會出現誤差的問題解決。\",\"邊界上的判斷會因為圖片跟相鄰環境的相似導致模糊不清。但透過剪貼則可以造成不同環境的突兀感，進而解決這個問題。因此這時候 pseudo labelling 就能夠比較好發揮作用。\",\"Image from Viktor Olsson et al. (2020)\"]},\"52\":{\"h\":\"Methodology\"},\"53\":{\"h\":\"Naive Mixing to UDA\",\"t\":[\"最 Naive 的做法就是照著 ClassMix 的方法，將 unlebelled dataset Mixing 成新的 dataset，把 labelled dataset 以及 mixed dataset 拿去訓練。\",\"Info\",\"在 UDA 當中，unlabelled dataset 就是 target domain dataset。\",\"Image from Wilhelm Tranheden at al.\",\"但是這種做法實際上效果很糟糕。像是 sidewalk 被預測成 road，rider 被預測成 person 之類的，許多的 class 都被其他 class 覆蓋。這樣的問題只在 target domain 上會發生，這跟前面提到只使用 pseudo labelling to UDA 會造成的問題是吻合的。\",\"Image from Wilhelm Tranheden at al.\",\"單純的 Naive Mixing 往往在邊界上會有許多誤判的 class\",\"Tips\",\"這種相似的 class 相鄰而導致的誤判被稱為 class conflation\"]},\"54\":{\"h\":\"Domain Adaption via Corss-domain mixed Sampling (DACS)\",\"t\":[\"DACS 的核心做法是不單只是跟 Target Domain 去 mixing，而是將 Source 跟 Target 一起 Mix。如此一來， Target Domain 以及 Source Domain 的關聯性就能被連結起來，降低 Domain Shift。\",\"Image from Wilhelm Tranheden at al.\",\"詳細的步驟具體來說\",\"從 Source Domain (DS​) 取出圖片與 lebel (XS​,YS​)\",\"從 Target Domain (DT​) 取出圖片 XT​\",\"透過 segmentation network fθ​ 取得 XT​ 的 pseudo label YT​^​\",\"將 (XS​,YS​),(XT​,YT​^​) 經過 ClassMix 得到 (XM​,YM​)\",\"把 (XS​,YS​),(XM​,YM​) 拿去訓練。\",\"Image from Wilhelm Tranheden at al.\",\"在 Loss 的設計上也相當直覺，就是希望 XS​ 的預測結果要接近 YS​，XM​ 的結果要接近 YM​。\",\"H: Cross-Entropy\",\"λ: 調整 Mixing 部分的影響程度\",\"L(θ)=E[H(fθ​(XS​),YS​)+λH(fθ​(XM​),YM​)]\"]},\"55\":{\"h\":\"Results\"},\"56\":{\"h\":\"實驗設定\",\"t\":[\"在 segmentation network 的設定上參考了許多過去的研究，選擇採用 DeepLab v2 搭配 ResNet101 作為 backbone。\",\"ResNet101 是 pretrained on ImageNet 跟 MSCOCO。而 Hyperparameter 的設定基本上跟 Yi-Hsuan Tsai et al. (2018) 一樣。\",\"在 Mixing 的方法上雖然任何 based on binary mask 的 Mixing 都可以使用，不過這裡最主要都是使用 ClassMix。\"]},\"57\":{\"h\":\"Dataset\",\"t\":[\"在 synthetic-to-real 有一些常見的 benchmarks。\",\"GTA5 -> Cityscapes\",\"SYNTHIA -> Cityscapes\",\"GTA5 以及 SYNTHIA 都是虛擬世界當中的影像，而 Cityscapes 則是現實世界當中的影像。\"]},\"58\":{\"h\":\"Cityscapes\",\"t\":[\"照片是在城市當中開車拍下的各種照片\",\"Image from Marius Cordts et al. (2016)\",\"2975 training images\",\"19 classes\"]},\"59\":{\"h\":\"GTA5\",\"t\":[\"照片是在 GTA5 下拍攝的\",\"Image from Stephan R. Richter et al.\",\"24966 synthetic training images\",\"19 classes \",\"可對應到 Cityscapes 的 classes\"]},\"60\":{\"h\":\"SYNTHIA\",\"t\":[\"照片是在 Unity 建構的 virtual city 下拍攝\",\"Image from GermanRos et al. (2016)\",\"9400 synthetic training images\",\"16(or 13) classes \",\"都會對到 Cityscapes 的 classes\",\"13 個 classes 的版本是少了 Wall, Fence, Pole\"]},\"61\":{\"h\":\"GTA5 -> Cityscapes\",\"t\":[\"Image from Wilhelm Tranheden at al.\",\"其他的 Model 都是 DeepLab-v2，他們選擇其中 Performance 最好的，但 Backbone 並不一定要是 ResNet 101\",\"Image from Wilhelm Tranheden at al.\",\"Source 是只有使用 source domain 去 train 的模型\",\"Info\",\"可以發現\",\"單純用 source domain 去訓練顯然很糟糕，只對簡單的 class 像是 Road, Build, Veg, Sky, Person, Car 這些普遍做得不錯的 class 有還不錯的 Performance\",\"DACS 在絕大多數並非是最佳的結果上都不會離最佳太遠，除了 SW 有點偏以及 Train 真的很糟\"]},\"62\":{\"h\":\"SYNTHIA -> Cityscapes\",\"t\":[\"考慮到 SYNTHIA 有些 paper 使用 16 個 classes，有些是 13 個 class 的版本，所以在數據上 mIoU 有兩列分別表示 13 個平均跟 16 個的平均。\",\"Image from Wilhelm Tranheden at al.\",\"Info\",\"可以發現\",\"單純用 source domain 去訓練顯然很糟糕，甚至對 Road 的 Performance 都不太好\",\"DACS 在絕大多數並非是最佳的結果上都不會離最佳太遠，除了 SW 頗偏\"]},\"63\":{\"h\":\"Some issues about evaluation\",\"t\":[\"他們認為在其他的 paper 有不少人最後給的結果之所以那麼好看是因為\",\"Cityscapes 並沒有 testset\",\"他們選擇用 validation set 判斷要不要 early stop，這個 validation set 也跟最後評估的 set 是一樣的\",\"針對 validation set 挑選 hyperparameters (?)\",\"所以他們認為這樣不太公平，畢竟在 Validation set 做得很棒不能直接表達在整體會表達很棒。 他們也試著用相同的手段訓練模型，然後拿到了\",\"GTA5 \",\"Baseline: 35.68% (+2.83%)\",\"DACS: 53.84% (+1.7%) (BEST)\",\"SYNTHIA \",\"DACS (13 classes): 55.98% (+1.17%) (1.02% to BEST)\",\"DACS (16 classes): 49.10% (+0.76%) (0.7% to BEST)\"]},\"64\":{\"h\":\"Contribution\",\"t\":[\"Apply SSL method on ClassMix to UDA\",\"Introduce a simple framework with high-performance\",\"Beat SOTA in GTA5 to Cityscape\"]},\"65\":{\"h\":\"值得一看的文章們\",\"t\":[\"【Day 24】半監督式學習（Semi-supervised Learning）（上）\",\"【Day 25】半監督式學習（Semi-supervised Learning）（下）\",\"Notes on “DACS: Domain Adaptation via Cross-domain Mixed Sampling”\",\"物件偵測的領域自適應 (Domain Adaptation)\",\"Adversarial Learning for Semi-Supervised Semantic Segmentation\",\"Domain Adaptation in Computer Vision: Everything You Need to Know\",\"Semi-supervised semantic segmentation needs strong, varied perturbations\",\"ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning\",\"Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training\",\"Learning to Adapt Structured Output Space for Semantic Segmentation\"]},\"66\":{\"c\":[\"Note\"]},\"67\":{\"c\":[\"Paper Read\",\"Domain Adaptation\",\"Computer Vision\",\"WACV\"]},\"68\":{\"h\":\"DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\"},\"69\":{\"h\":\"Basic Information\",\"t\":[\"Lukas Hoyer, Dengxin Dai, Luc Van Gool @ ETH Zurich & MPI for Informatics\",\"2022 CVPR\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"與過去的方法相比， DAFormer 在 UDA semantic segmentation 領域當中做出了劃時代的貢獻。\"]},\"70\":{\"h\":\"問題描述\",\"t\":[\"如同過去看過的 UDA 問題描述，這一篇同樣也是先說明了 semantic segmentation 在 UDA 上的重要性。由於標記 semantic segmentation labels 的成本過高，以致於開始將研究的方向轉向如 weak-supervised 或是 semi-supervised learning，最終則是 unsuvervised learning 的 UDA。\",\"在這一篇論文當中主要探討的是過去 UDA 的模型都是採用如 DeepLab 搭配 ResNet 或是 VGG 等架構，但是這些架構在 semantic segmentation 領域都已經是過時的產物，有許多新的架構可以得到更高的 mIoU。作者懷疑會不會其實我們應該要試著採用更好的 backbone 去訓練，可以得到更好的結果。\",\"不過直覺上，如果我們用更加強大的 backbone，那麼就會有更高的機會在 source domain 上 overfitting，因此這一篇 paper 的目標是在改採用更佳的 backbone 的同時，避免 overfitting 的問題。\"]},\"71\":{\"h\":\"Related Works\",\"t\":[\"Sematic Image Segmentation\",\"Unsupervised Domain Adaptation (UDA)\",\"Transformer\",\"Self-training\"]},\"72\":{\"h\":\"Methodology\"},\"73\":{\"h\":\"Self training for UDA\",\"t\":[\"一開始我們一樣先看一下這一篇論文當中會用到的 Notation 以及他對於 self training 的描述。這裡已經預設包含了 Knowledge Distillation。\",\"gθ​ 表示 student model\",\"hϕ​ 表示 teacher model\",\"NS​ 表示 Source Domain 的資料數量\",\"NT​ 表示 Target Domain 的資料數量\",\"XS​={xS(i)​}i=1NS​​ 表示 Source Domain 的資料\",\"XT​={xT(i)​}i=1NT​​ 表示 Target Domain 的資料\",\"YS​={yS(i)​}i=1NS​​ 表示 Source Domain 對應的 labels\",\"YT​={yT(i)​}i=1NT​​ 表示 Target Domain 對應的 labels，在 UDA 預設是不會知道的\",\"H,W 分別表示圖片的高寬\",\"YS​,YT​ 都具有 C 個共通的 classes\",\"最 Naive 的方法是把套上 Categorical Cross Entropy Loss (CCE Loss) 期待預測的 label 跟目標相同。\",\"LS(i)​=−j=1∑H×W​c=1∑C​yS(i,j,c)​loggθ​(xS(i)​)(j,c)\",\"然而這種方法的 performance 以及一般性都並不是很理想。Self training 的方法會使用 pseudo labelling，透過產生假想的 label 去學習。於是 pseudo label 就不是單純的 one-hot，而是包含了機率的概念，我們會選其中最大的當成是最後的答案 pT(i,j,c)​。\",\"pT(i,j,c)​=[c=argmaxc′​hϕ​(xT(i)​)(j,c′)]\",\"此外，我們也可以去定義當前 pseudo label 信心度的標準 qT(i)​。也就是說，會期待預測出來的 label 至少信心度要超過 τ，這樣的結果有多少。\",\"qT(i)​=H⋅W∑j=1H×W​[maxc′​hϕ​(xT(i)​)(j,c′)>τ]​\",\"Info\",\"這裡的 [⋅] 是 Iverson Bracket，只是單純符合條件給 1，否則給 0 的符號。\",\"[P]={10​ifPistrueotherwise​\",\"有了評斷信心水平的標準，就可以結合起來形成新的 Loss。\",\"LT(i)​=−j=1∑H×W​c=1∑C​qT(i)​pT(i,j,c)​loggθ​(xT(i)​)(j,c)\",\"也就是說我們會期待產生出來的 pseudo label 除了越準確越好，也會期待其信心水平也要是高的。\",\"Pseudo label 的產生方式可以是 offline 也可以是 online，這裡考慮到 online 的實作比較簡單，所以採用這個方法。與 ProDA 相同，根據過去的研究，這裡會採用 Exponential Moving Average (EMA) 去更新 teacher model。\",\"ϕt+1​←αϕt​+(1−α)θt​\",\"此外，student model 的訓練上也是使用 augumented data。包含了 DAFormer、Color Jitter、Gaussian Blur、ClassMix。\"]},\"74\":{\"h\":\"DAFormer Network Architecture\",\"t\":[\"首先，針對 backbone network 過於老舊的部分作者先透過一些實驗去尋找好的架構，他們後來發現 Transformer based model 會有更好的 mIoU。這裡選用的 Transformer 是 SegFormer。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"Src-Only: 只訓練在 GTA dataset\",\"UDA: 使用 GTA dataset 作為 source domain 加上 UDA 方法 adapt Cityscape dataset\",\"Oracle: 直接使用 supervised learning 訓練 Cityscape dataset\",\"上述的三者分數都是以 Cityscape dataset 去評估取得 Rel 用來比較 UDA 在 Oracle 的 scale 下有多強。\",\"Rel=OracleUDA​\",\"可以發現到 SegFormer 的表現都比起其他架構來得好許多，並且有趣的是 DeepLabV3+ 並沒有得到比 DeepLabV2 更好的表現。\",\"更多關於模型選擇的實驗\",\"由於 backbone 實際上包含了 Encoder 以及 Decoder 兩個部分，作者進一步去分析究竟是哪一個部分使最後得到好的結果。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"可以發現到當 Encoder 不採用 MiT-B5 這種包含了 Transformer 的 encoder，得出的 performance 會有大量的下降，也就是說，Transformer 在這裡能夠提供更好的幫助。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"進一步去研究不同大小的 Encoder 會有怎樣的影響，可以發現到通常越大的模型能夠提供更好的效益。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"使用 Transformer based encoder 有另一個有趣的好處是，他可以很好地把不同的 classes 分開，即便這些 classes 有許多相像的地方。圖中圈起來的是各種交通工具，可以發現到 MiT-B5 可以有更好的 feature separation。\",\"此外，Transformer 當中包含的 self-attention 與傳統的 CNN 不同，即便在 testing 階段能夠動態地依據當下的輸入資料的相似性來產生對應的Affinity-map，再依據得到的Affinity-map做出預測。\",\"中文敘述參考 [論文筆記] DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\",\"於是，根據實驗的結果我們選擇使用 SegFormer 做為新的 backbone。\",\"不過過去使用 Transformer based backbone 解決 semantic segmentation 通常會有個通病是在 decoder 的部分只能取得 local information。於是作者嘗試修改 decoder 的部分，把 encoder 給出不同 level 的 feature maps 處理成相同 channels 數量以及大小，再使用不同的 dilation rates 去處理。如下圖所示。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"Training Strategies for UDA\",\"這一篇 paper 最主要的貢獻，就是解決了使用更好的 backbone 同時避免 overfitting source domain 的方法，具體而言有三個部分：Rare Class Sampling(RCS), Thing-Class ImageNet Feature Distance(FD), Learning Rate Warmup for UDA。以下就分別說明這三個部分的作法。\"]},\"75\":{\"h\":\"Rare Class Sampling (RCS)\",\"t\":[\"作者在實驗的過程當中發現到 DAFormer 在 Rare Classes 的 performance 在不同 random seed 的情況下有很大的不同。作者認為這是因為若這些 rare classes 在訓練後期才出現，模型很可能已經被 common classes 干擾形成 bias，以致於難以 re-learn。\",\"於是，對於這些 rare classes，我們就希望讓他在訓練過程當中出現的頻率可以更高，也就有更高的機會可以學更多次、更早看到它。\",\"定義一個 source domain class c 出現的頻率 fc​ 如下。\",\"fc​=NS​⋅H⋅W∑i=1NS​​∑j=1H×W​[yS(i,j,c)​]​\",\"而一個 class c 被 sample 到的機率 P(c) 就可以用 softmax with temperature 去定義如下。\",\"P(c)=∑c′=1C​e1−fc′​/Te(1−fc​)/T​\",\"也就是說，我們會盡可能讓出現頻率越低的 class 有較高的機會被 sample 到。\"]},\"76\":{\"h\":\"Thing-Class ImageNet Feature Distance (FD)\",\"t\":[\"通常在 UDA 的 backbone 所使用的 semantic segmentation network 都會使用 ImageNet pretrained models 去初始化權重。我們理想上會預期那些 ImageNet 當中有包含的 classes 理應因此得到較好的結果。\",\"然而，如 train 和 bus 這兩個 classes 卻反而往往得到很糟糕的結果。並且透過觀察訓練過程作者發現到，其實在訓練初期其實是能夠辨別這些 classes 的，但卻隨著訓練過程慢慢地變糟。\",\"作者認為這是好的 features 都被 Loss function LS​ 搞壞所導致。\",\"因此，作者把這些 \\\"bottleneck features\\\" 拿出來，希望他們在 ImageNet 的 feature 以及訓練模型的 feature 之間的距離可以拉近，避免模型\\\"忘記\\\"這些 features。\",\"不過也考慮到 ImageNet 幾乎都是訓練在 Thing-Class 上，Stuff-Class 如 road, sky 就基本上沒有。因此這裡的拉近只會針對 Thing-Classes Cthings​ 處理。\",\"定義 xS(i)​ 的第 j 個 pixel 的 Feature Distance d(i,j) 如下。\",\"d(i,j)=∥FImageNet​(xS(i)​)(j)−Fθ​(xS(i)​)(j)∥2​\",\"定義 Mask Mthings(i,j)​ 如下。\",\"Mthings(i,j)​=c′=1∑C​yS,small(i,j,c′)​⋅[c′∈Cthings​]\",\"這裡的 yS,small(i,j,c′)​ 只是為了 downsample size，採用了 Average Pooling。\",\"yS,small(i,j,c′)​=[AvgPool(ySc​,H/HF​,W/WF​)>r]\",\"如此一來就能在 Loss 上多加上一項去 regularize。\",\"LFD(i)​=∑j​Mthings(i,j)​∑j=1HF​×WF​​d(i,j)⋅Mthings(i,j)​​\",\"於是乎最後的整體 Loss function 也就形成。\",\"L=LS​+LT​+λFD​LFD​\",\"λFD​ 是一個 hyperparameter。\"]},\"77\":{\"h\":\"Learning Rate Warmup for UDA\",\"t\":[\"過去訓練 CNN 或是 Transformer 都會習慣使用 linear learning rate warmup，這裡也加進來，他們透過實驗發現這很不錯。\",\"ηt​=ηbase​⋅t/twarm​\"]},\"78\":{\"h\":\"Results\"},\"79\":{\"h\":\"實驗設定\",\"t\":[\"在 Dataset 的使用上如同過去我們看過的 DACS 與 ProDA，都是採用 UDA 當中常見的 datasets：Cityscapes、GTA5、SYNTHIA。\",\"實作上採用了常見的 mmsegmentation framework，Network 的架構如同前面所述，encoder 採用 MiT-B5 encoder，decoder 的部分作者另外的調整時選用的 dilation rate 分別是 1, 6, 12, 18。Encoder 已經 pretrain 在 ImageNet-1K 上。\",\"至於詳細的 hyperparameter 設定請詳閱 paper。\"]},\"80\":{\"h\":\"Summary\",\"t\":[\"這裡先簡單總結一下。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"首先看到上面的表格，可以觀察到\",\"加上 Warmup 之後 performance 提升了約 6.4 mIoU (row 1 & 2)\",\"加上 RCS 之後 performance 提升了約 5.8 mIoU (row 2 & 4)\",\"加上 FD 之後 performance 提升了約 3.5 mIoU (row 2 & 6)\",\"加上 Warmup、RCS、FD 之後 performance 提升了約 14.4 mIoU (row 1 & 7)\",\"再多一點調整後可以再提升約 0.8 mIoU\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"作者也給出每個 classes 在加上不同的調整後得出的結果，可以看到所有 class 經過 DAFormer 都可以有獲得提升，甚至那些 rare classes 也變得能夠預測了。\"]},\"81\":{\"h\":\"Learning Rate Warmup\",\"t\":[\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"經過實驗後發現無論是採用 DeepLabV2 或是 SegFormer，如果搭配 Learning Rate Warmup 都對於 performance 有所提升。\"]},\"82\":{\"h\":\"Rare Class Sampling (RCS)\",\"t\":[\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"上圖展現出 Rider 和 Bicycle 這兩個 Class 預測的結果，在沒有使用 RCS 的情況下(藍色線)，IoU 的變化很大程度跟 Random Seed 的選用有關，這一點尤其在 Bicycle 最明顯。並且也可以觀察到那些比較早開始有所提升的 Random Seed 最後得到的 IoU 也會最大。\",\"因此作者認為這是跟圖片被 sample 到的時間有所相關，進而提出 RCS 去提升 Rare Class 被 Sample 的機率(橘色線)，可以發現搭配了 RCS 後，IoU 的變化就比較不與 Random Seed 的選擇相關，並且普遍最後的 IoU 都會高過於原本的狀況。\"]},\"83\":{\"h\":\"Thing-Class ImageNet Feature Distance (FD)\",\"t\":[\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"在上圖的橘色線是原本的模型隨著訓練後對於不同 class 預測的 IoU 變化。可以觀察到 Train 這個類別居然會隨著訓練時間預測結果越糟糕，而最一開始的結果其實是還不錯的。\",\"作者認為這是因為 MiT-B5 太強，導致 overfit source domain，進而產生這樣的結果。\",\"透過加上 FD 之後，可以看到在綠色線的部分，成功避免了預測結果變差的狀況。\",\"此外，作者也注意到 Cityscapes 的圖片由於是透過車子上裝設攝影鏡頭去蒐集的，所以圖片底下的部分實際上並不是跟街景相關，而是自駕車車體。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"此外，在畫面的上方也有部分的影像因為影像校正導致的失真如下圖所示。\",\"Image from ResearchGate\",\"因此，作者進一步去忽略畫面上方 15 pixels 以及畫面下方 120 pixels 的 pseudo label。另外也考慮到 Transformer 的表達能力可以更強，進一步提高 α 到 0.999，最終得到更好的結果，如上面 summary 所示。\"]},\"84\":{\"h\":\"DAFormer Decoder\",\"t\":[\"Image from ResearchGate\",\"作者進一步去比較自己改良的 decoder 跟其他架構相比，發現到 DAFormer 搭配 Depthwise Separable Convolution 確實能夠得到好的結果。儘管 UperNet 在 Oracle 上可以得到較好的結果，但是在 UDA 上 DAFormer 仍然有更好的 performance。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"最終也可以看到，與過去的 SOTA 相較之下，DAFormer 成功在幾乎所有的 class 上 outperform 其他 SOTA，並且最終的 mIoU 與過去的 SOTA 都有相當大的改進。\"]},\"85\":{\"h\":\"Contribution\",\"t\":[\"研究不同的 backbone 架構對於 UDA performance 的影響\",\"成功將 Transformer 的成功帶進 UDA 領域 \",\"提出了三個方法避免 overfitting 的問題\",\"只需要一張 RTX 2080 Ti GPU 訓練 16 個小時，與過去的資源消耗相較減輕甚多\"]},\"86\":{\"h\":\"值得一看的文章們\",\"t\":[\"[論文筆記] DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation\",\"[CVPR22] DAFormer: Improving Network Architectures for Domain-Adaptive Semantic Segmentation\",\"DAFormer Github\",\"DAFormer(CVPR2022)阅读笔记\",\"DAFormer Extension Paper\"]},\"87\":{\"c\":[\"Note\"]},\"88\":{\"c\":[\"Paper Read\",\"Domain Adaptation\",\"Computer Vision\",\"CVPR\"]},\"89\":{\"h\":\"Playing Atari with Deep Reinforcement Learning\"},\"90\":{\"h\":\"Basic Information\",\"t\":[\"2013 NeurIPS\",\"Volodymyr Mnih, Koray Kavukcuoglu David Silver et al.\",\"這個論文提出的做法稱為 DQN(Deep Q-Networks)\"]},\"91\":{\"h\":\"問題描述\",\"t\":[\"過去在 RL 領域當中把一些 high-dimensional 的感官資料（如：視覺影像、語音資料等）作為 agent 的輸入去學習一直是一個很大的挑戰。然而我們也看到近幾年 Deep Learning 已經能夠在這種資料上去擷取特徵，進而去完成許多複雜的任務。\",\"所以「能不能把 Deep Learning 的成功也放進 RL 當中呢？」這樣的想法自然而然就出現了。\",\"不過從 Deep Learning 的角度來看 RL 的話，會有幾個明顯的問題。\",\"RL 的訓練資料（如：Reward）需要透過與環境互動取得，但數值範圍往往很 sparse，而且也往往會經過一段時間的延遲才取得 與 Deep Learning 相較之下，DL 的資料通常都會先 Label 好，可以直接把資料之間的關聯建構起來。\",\"RL 的訓練資料具有高度相關性 在 DL 當中我們會預設資料之間是沒有什麼相依性的，但在 RL 當中同一個 episode 的 state、action、reward 之間都會具有相當高的相關性。\",\"RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化 DL 往往假設資料的分布會維持住。\",\"這一篇論文成功將 CNN 應用在 RL 上，也避免了上述提及的幾個問題。\"]},\"92\":{\"h\":\"Related Works\",\"t\":[\"Q-Networks\",\"TD-gammon\",\"收斂性相關研究 \",\"Residual algorithms: Reinforcement learning with function approximation.\",\"Q-learning\",\"Neural fitted Q-learning (NFQ)\"]},\"93\":{\"h\":\"Q-Networks\",\"t\":[\"在 RL 當中我們會透過 MDP 去 model 整個問題，而 RL 的目標就是要讓整體的 reward 總和最大化。\",\"定義 optimal action value function Q∗ 如下\",\"Q∗(s,a)=πmax​E[Rt​∣st​=s,at​=a,π]\",\"也就是在 state s 採取 action a 並 follow policy π 得到的最大 return。其中 Return 的定義如下，這裡考慮有 discount 的版本。\",\"Rt​=t′=t∑T​γt′−trt′​\",\"γ 為 discount factor\",\"rt​ 表示在時間 t 取得的 reward\",\"既然 RL 的目的是要讓整體的 return 最大化，也就是要找到 Q∗ 了。Q-Network 就是用 Neural Network 來近似 Q∗，也就是要讓底下的 Loss 最小化。\",\"Li​(θi​)yi​​=Es,a∼ρ(⋅)​[(yi​−Q(s,a;θi​))2]=Es′∼ε​[r+γa′max​Q(s′,a′;θi−1​)]​\",\"需要特別注意到對於 θi​ 來說，他要去近似的是 θi−1​ 的模型得出來的結果，也就是說，Q-Network 透過固定訓練的目標(Target Network)，解決了前面提及的第三個問題「RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化」。\"]},\"94\":{\"h\":\"TD-gammon\",\"t\":[\"一個 model-free RL 算法，透過一個 multi-layer perceptron 和一層 hidden layer 去預測 value function，成功在雙陸棋上面 outperform 人類。\",\"不過這裡的成功只停止在雙陸棋上，並無法繼續擴充到其他的領域。\"]},\"95\":{\"h\":\"收斂性相關研究\",\"t\":[\"過去的研究當中發現到如果是 model-free 搭配 non-linear function approximators 或是 off-policy learning 的話會導致 Q-network 發散，無法收斂。\",\"後續的研究中則發現到 Q-network 無法收斂的問題可以透過 gradient TD 舒緩，並且證明了底下兩個狀況是可以確保收斂。\",\"固定 policy，使用非線性的 approximator\",\"使用線性的 approximator 去學 control\",\"然而這些研究都並未能夠給出用非線性去學 control 的方法。\"]},\"96\":{\"h\":\"NFQ\",\"t\":[\"跟這一篇 paper 最相近的一個研究，他們會先透過 Computer Vision 的模型萃取出圖片的特徵，然後再把這些特徵丟去給 RL 訓練。\",\"不過 DQN 與 NFQ 不同的地方在於 DQN 是 end-to-end，也就是說可以直接從 visual input 去訓練，而 NFQ 不是。\"]},\"97\":{\"h\":\"Methodology\",\"t\":[\"在 TD-gammon 當中我們看到了使用 Neural Network 去學習 value function 有還不錯的成效，DQN 稍微修改了這個做法，將 Q-Network 和 Experience Replay 結合起來。\",\"Experience Replay 會將 agent 跟環境的互動過程當中的 experience 記錄在 replay memory D 當中。當要去更新模型的時候，我們是從 replay memory 當中取得隨機幾筆去更新。\",\"Info\",\"在時間 t 的 experience 包含了 state, action, reward, next state\",\"et​=(st​,at​,rt​,st+1​)\",\"因此 experience 就定義成\",\"D=e1​,e2​,…,eN​\",\"在經過 experience replay 之後，agent 會透過 ϵ-greedy 去選擇 action。\",\"實作上 experience 只會儲存最後 N 筆，並且 history 當中的 frames 只會取出最後 4 個，拿出來做一些 preprocess ϕ(s) 之後作為實際上儲存進 experience 的 state。\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\",\"透過 Q-Network 中的 Target Network 以及 Experience Replay，DQN 順利避免了最初提及的兩個問題「RL 的訓練資料具有高度相關性」以及「RL 的資料分佈會隨著 policy 的改變而有巨大幅度的變化」。\"]},\"98\":{\"h\":\"Results\"},\"99\":{\"h\":\"實驗設定\",\"t\":[\"實驗做在 7 個 Atari games，Beam Rider, Breakout, Enduro, Pong, Q*bert, Seaquest 以及 Space Invaders。\",\"每個遊戲的 reward 一開始都不太相同，實驗上調整了 reward 的大小，使得所有 positive reward 固定為 1；negative reward 為 −1，其餘則為 0 表示不影響。\",\"並且會使用 frame skipping 的技巧，讓 agent 只會每經過 k 個 frames 才會去擷取畫面，並且做出相對應的 action。至於那些被忽略的 frames，就持續上一個做出的 action。除了 Space Invaders 因為遊戲當中的雷射會跑很快，所以設定 k=3，其他遊戲則都是 k=4。\"]},\"100\":{\"h\":\"評估方式\",\"t\":[\"在 Deep Learning 當中如果要評估一個 Network 的好壞，可以單純透過觀察模型在 validation set 上的 performance 即可，但是在 RL 當中並沒有 validation set，因此評估一個 agent 的好壞就相對困難。\",\"過去會透過多次遊戲中 agent 獲得的 reward 平均去評估，也就是說理想上每經過一輪更新，模型能夠得到的 reward 應該要慢慢變大。不過作者發現在他們的模型得出來的結果往往會是很不穩定的。作者推測是因為權重即便只有小的變化也會對 policy distribution 有大的影響，導致接下來會經過的 state 就很不相同。\",\"因此作者改成 Q 的平均去評估，也確實發現會平滑許多。\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\"]},\"101\":{\"h\":\"比較基準\",\"t\":[\"最後將 DQN 跟幾個 Baseline 去比較\",\"Sarsa \",\"On-control policy\",\"Linear approximator\",\"人工提取 features\",\"Contingency \",\"跟 Sarsa 類似，但有包含了部分的學習過程\",\"HNeat Best \",\"訓練的過程包含了一點專家系統的概念\",\"事先標記好 object 的位置以及類型\",\"HNeat Pixel \",\"訓練的過程包含了一點專家系統的概念\",\"事先處理好了 8 color channel representation\",\"Human\",\"image\",\"Image from Volodymyr Mnih, Koray Kavukcuoglu, David Silver et al. (2015)\",\"最後得出來的結果，DQN 在幾乎所有的遊戲當中都 outperform 所有的算法，證明了 DQN 的成功。\"]},\"102\":{\"h\":\"Contribution\",\"t\":[\"成功結合 Deep Learning 以及 Reinforcement Learning\",\"直接從 raw RGB 當作輸入，不需要事先經過其他的分解\",\"透過 Experience Replay 以及 Target Network 解決過去 Deep Learning 結合 RL 時訓練不佳的問題\"]},\"103\":{\"c\":[\"Note\"]},\"104\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"NeurIPS\"]},\"105\":{\"h\":\"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\"},\"106\":{\"h\":\"Basic Information\",\"t\":[\"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov @ Toronto University\",\"2014 JMLR\"]},\"107\":{\"h\":\"問題描述\",\"t\":[\"在近年來發現到 Neural Network 參數越多就有越強大的表達能力，並且通常會有更好的表現。不過隨著參數量的上升，我們也發現到模型越來越會傾向於 Overfitting。\",\"這樣的狀況之所以會出現有可能有幾個原因\",\"資料不足，導致過度複雜的模型直接學習到 dataset 本身\",\"資料具有偏差，導致複雜的模型學習過於有偏差\",\"一個理想上必定能夠避免 Overfitting 問題的解法是把所有可能的 Network Parameters 給出的輸出取平均，那就必然會考慮到所有的面向。然而，很顯然地，這樣的方法會有過多的計算量。\",\"作者從 theory of the role of sex in evolution 當中獲得啟發。在有性生殖的過程當中會融合雙親的基因，過程當中也會有一些機率出現 random mutation。從生物學的角度來看，有性生殖能夠讓子代有部分雙親的優勢，使得子代能夠適應整個環境。\",\"作者從啟發當中發想提出了 Dropout，有效地避免 Overfitting。\"]},\"108\":{\"h\":\"Related Works\"},\"109\":{\"h\":\"Denoising Autoencoders (DAEs)\",\"t\":[\"Dropout 在做的事情可以視為是在 Neural Network 的 hidden units 加上 noise。\",\"過去類似的作法出現在 DAEs 當中。DAEs 作為一種 Autoencoder 的類別，同樣也包含了 Encoder 以及 Decoder。理想上將輸入經過 Encoder 與 Decoder 之後能夠還原出輸入的原貌。\",\"DAE 的做法是對 input 加上 noise，藉此讓模型能夠學習到更多的特徵，避免了 Overfitting。\",\"Image from wikipedia\",\"與 DAE 不同，Dropout 針對 hidden units 加上 noise，並且發現這樣的做法實際上對於訓練有相當好的幫助。\"]},\"110\":{\"h\":\"Methodology\"},\"111\":{\"h\":\"Overview\",\"t\":[\"整個 Dropout 的做法簡單來說就是兩件事情。\",\"訓練過程中讓每個 hidden units 都有 p 的機率不被使用到\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"測試過程中讓每個 hidden units 的輸出結果都乘上機率 p 作為輸出\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"每個 hidden units 有機率 p 不被選到，n 個 units 組成的模型就像是有 2n 種不同模型一樣，卻又不會有過高的計算量。\"]},\"112\":{\"h\":\"Model Description\",\"t\":[\"首先說明這篇 paper 使用的一些 notation 跟對 Neural Network 的描述。\",\"L 表示 Neural Network hidden layer 數量\",\"l∈{1,…,L} 表示每個 hidden layer 的編號\",\"z(l) 表示 layer l 的輸入\",\"y(l) 表示 layer l 的輸出 \",\"y(0)=x 表示 input\",\"W(l),b(l) 表示 layer l 的 weights 和 biases\",\"f 表示 activation function\",\"於是對於 hidden unit i 可以描述其輸入與輸出如下\",\"zi(l+1)​yi(l+1)​​=wi(l+1)​y(l)+bi(l+1)​=f(zi(l+1)​)​\",\"加上了 dropout 之後，每個 hidden unit 都會有機率 p 在訓練過程中不被考慮到，因此對於上一層的 hidden unit j 來說，他的輸出會被 p 所影響如下。\",\"rj(l)​y~​(l)​∼Bernoulli(p)=r(l)∗y(l)​\",\"而對於這一層 hidden unit i 的輸入輸出就會變成底下的樣子。\",\"zi(l+1)​yi(l+1)​​=wi(l+1)​y~​(l)+bi(l+1)​=f(zi(l+1)​)​\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"而在測試階段，則是把輸出乘上機率 p 如下\",\"Wtest(l)​=pW(l)\"]},\"113\":{\"h\":\"Learning Dropout Nets\",\"t\":[\"Backpropagation\",\"使用 dropout 的模型在訓練過程當中那些被暫時移除的 hidden unit 梯度會是 0\",\"Dropout 也可以搭配其他的 Regularization 方法如 L1，加上後會再進一步提升效果\",\"Unsupervised Pretraining\",\"Dropout 也可以應用在 pretrained model fine-tuning\",\"由於 dropout 最後輸出會乘上 p，因此 pretrained weight 要先乘 1/p\"]},\"114\":{\"h\":\"Results\"},\"115\":{\"h\":\"Datasets\",\"t\":[\"為了測試 Dropout 的性能以及通用性，作者選了許多不同領域的資料集如下\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"針對不同的 datasets，作者去挑了在該領域的 SOTA 加上 Dropout 去看最終的效能是不是確實有提升。\"]},\"116\":{\"h\":\"Result on Image Datasets\",\"t\":[\"MNIST\",\"MNIST 是一個用於手寫辨識的資料集，包含了許多 28×28 的手寫數字，目的是要辨認出每個圖片是對應到哪個數字 0~9。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"可以看到 baseline 的 error rate 是 1.60%，單純加上 Dropout 之後就可以降低到 1.35%。\",\"如同前面描述，我們也可以再加上其他的技巧去做更多的處理，可以得到更好的結果。\",\"而在這些許多不同的架構下，在相同的 hyperparameters、相同的 Dropout rate p，可以發現到對應的 test error 在 Dropout 使用的有無有相當大的不同。使用了 Dropout 的架構都可以得到更好的結果。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"SVHN\",\"SVHN(Street View House Number) dataset 包含了許多 32×32 的門牌號碼彩色照片，目標是要辨識出門牌號碼。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"我們將 Conv Net 當作 baseline，也就是對應到 error rate 3.95%。單純在 fully connected layers 加上 dropout 之後可以來到 3.02%。而在所有的 layer 都採用的話則可以進一步降低至 2.55%。\",\"當然，也可以再進一步加上其他的技巧去降低。\",\"CIFAR-10 / CIFAR-100\",\"CIFAR-10 包含了許多 32×32 的彩色圖片，這些圖片會被分類成 10 種不同的類別，目的是要分類每張圖片至正確的類別。而 CIFAR-100 只是類別增加到 100。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"同樣可以觀察到在加上 Dropout 可以得到更好的結果，並且進一步透過其他的技巧可以再做得更好。\",\"ImageNet\",\"ImageNet 包含了許多的高解析度彩色圖片，總共有 22,000 個類別，目標同樣是將每個圖片分類到正確的類別當中。\",\"因為 ImageNet 的類別超級多，因此在評分上我們可以考慮 Top-1 以及 Top-5 兩種方法。也就是說，只要正確的 label 出現在預測機率最高的前 5 個當中，對 Top-5 而言就算他答對。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"同樣可以觀察到 dropout 帶來的好處。\",\"Caution\",\"不過這裡的比較卻乏單純的 Conv Net 去當 baseline，好像不太能直接看出 Dropout 帶來的影響耶。\"]},\"117\":{\"h\":\"Result on Speech Recognition\",\"t\":[\"TIMIT\",\"這個 dataset 當中包含了 680 個講者的演講資料，當中包含了 8 種大主題，目標是要去把每個演講分類到正確的主題當中。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"無論是單純加上 Dropout，或是使用其他的優化手段都有辦法獲得更好的 error rate。\"]},\"118\":{\"h\":\"Result on Text Dataset\",\"t\":[\"Reuters-RCV1\",\"當中包含 800000 篇 newswire 上的文章，50 個主題，目標同樣是要分類。\",\"加上 Dropout 後從 baseline 的 31.05% error rate 降低到了 29.62%。\",\"Info\",\"後續還有一些類似的實驗用來驗證在各個領域採用 Dropout 後都可以獲得很棒的結果，但因為結論都是可以看到 Dropout 或是再額外加上其他優化的技巧都可以得到更好的結果，所以就不再贅述。\"]},\"119\":{\"h\":\"How Dropout Effect Network\",\"t\":[\"Dropout 確實已經透過前面的各個實驗證實了他的效度是相當不錯，不過實際上對於 Network 來說它造成了怎樣的影響呢？\",\"作者認為過去大模型之所以會 overfit datatset 是因為不同的 hidden units 會互相影響，也許某一個 unit 的\\\"錯誤\\\"會被其他的 unit 修正，以至於實際上每個 unit 的效益參差不齊，但是在訓練上的效果看起來還不賴。這個問題被稱為 co-adaptations。\",\"作者認為 Dropout 能夠透過在訓練當中忽略其他的 hidden unit 進而避免 co-adaptation 的問題。\",\"Info\",\"這就像是平常訓練的時候你有隊友可以 cover 你的失誤，所以作為一個團隊來說，你們整體看起來的表現會很不錯。\",\"不過當你們現在處在一個未知的環境當中，反而彼此會難以配合，畢竟都還在熟悉新的環境。\",\"然而 Dropout 就像是有時候你的隊友會請假，但是那些事情還是要處理，所以漸漸地每個人都會有一定的能力水平。現在即便丟到未知的環境當中，每個人也都還是能夠有一定的能力去解決。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"作者使用一個簡單的 Autoencoder 做在 MNIST dataset 上，取出第一層 Layer 擷取出的特徵圖如上。雖然就結果而言他們的 test error 是類似的，但是從擷取出的特徵當中可以很明顯的看出搭配 Dropout 後的 Network 是真的有在學習特徵。\",\"作者也發現到當使用 Dropout 之後 hidden units 的 activation 都會變得比較分散，如下圖所示。而且可以發現到幾乎所有的 activation 都會在 0 附近。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\"]},\"120\":{\"h\":\"Hyperparameter\",\"t\":[\"Dropout 當中有 dropout rate 可以設定，作者設計了兩種狀況來看 p 在不同選擇下的結果。\",\"固定 Layer 當中 hidden unit n\",\"固定 pn\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"可以發現到在左邊的圖中選擇 0.4≤p≤0.8 之間會比較平坦，大致上也會是比較好的選擇點。\",\"而在右邊的圖當中可以看到如果是固定 pn 的話大致上會在 0.4≤p≤0.7 之間會是比較好的選擇。\",\"因此作者認為選擇 p=0.5 (或是到 0.6) 會是比較好的選擇。\"]},\"121\":{\"h\":\"Effect of Data Size\",\"t\":[\"Data size 通常會對模型的結果有不小的影響。在同樣的網路架構下，如果在很少量資料的狀況下，模型通常會傾向 overfitting。作者比較了同樣架構下不同 data size，dropout 使用的有無造成的結果。\",\"Image from Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, et.al (2014)\",\"可以發現到在資料量小的狀況下使用 dropout 對於模型的訓練沒有什麼幫助。但隨著資料量越來越大，dropout 給出的幫助開始出現，但又會逐漸趨緩。\"]},\"122\":{\"h\":\"Contribution\",\"t\":[\"提出一個簡單避免 Overfitting 的方法\",\"探討 Dropout 在各種狀況下給出的效益\"]},\"123\":{\"h\":\"值得一看的文章們\",\"t\":[\"AutoEncoder (一)-認識與理解\"]},\"124\":{\"c\":[\"Note\"]},\"125\":{\"c\":[\"Paper Read\",\"Regularization\",\"JMLR\"]},\"126\":{\"h\":\"HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation\"},\"127\":{\"h\":\"Basic Information\",\"t\":[\"Lukas Hoyer, Dengxin Dai, Luc Van Gool @ ETH Zurich & MPI for Informatics\",\"2022 ECCV\"]},\"128\":{\"h\":\"問題描述\",\"t\":[\"這篇 paper 如同 DAFormer 關注在 UDA for semantic segmentation 。\",\"在過去的方法上由於 UDA 會需要儲存許多不同 Domain 的資訊、額外的 network 架構(如 Knowledge Distilation 時的 Student 與 Teacher)，以及各種新定義的 Loss 資訊，導致 GPU 用量普遍很高，也因此在圖片的輸入上通常會刻意先將輸入圖片的解析度降低。\",\"像是 Cityscapes 的圖片解析度有 2048x1024，但是實作時卻使用 1024x512 的大小。\",\"這種 Low-Resolution(LR) 的方法對於辨識細節相當不利，因為無法好好擷取特徵。然而若直接使用高解析度的圖片，即便 GPU 的記憶體空間足夠，也會因為擷取到過於細節的特徵導致大的物件無法好好辨認。\",\"Info\",\"這就像是不同地方的人行道也許會有不同的地磚設計，但我們都會認為那就是人行道。如果我們看過於細節，導致我們對於不同的設計誤以為是不同類型的物件，那就會在辨識上出現問題。\",\"HRDA 的目標是希望結合 Low-Resolution(LR) 能辨識大範圍特徵的優點，以及 High-Resolution(HR) 能辨識細節的優點，創造出在兩個狀況下都能夠順利辨認的方法。\",\"最後，他們提出的 HRDA 方法甚至大幅超越了過去他們提出的 DAFormer。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\"]},\"129\":{\"h\":\"Related Works\",\"t\":[\"Sematic Image Segmentation\",\"Unsupervised Domain Adaptation (UDA)\",\"Self-training\",\"DAFormer\"]},\"130\":{\"h\":\"Methodology\"},\"131\":{\"h\":\"Preliminary\",\"t\":[\"UDA 以 self-training 方法處理的大架構如下圖所示，由於過去的研究當中發現 Knowledge-Distillation 會對於 UDA 有正向的幫助，因此我們會訓練出一個 Student Network，依照 Source Label 以及 Target Pseudo-Label 去訓練，而 Teacher Network 則透過 Exponential Moving Average(EMA) 去更新。\",\"Image from Lukas Hoyer's YouTube Channel\",\"這裡先定義一下接下來會用到的基本 Notation。\",\"fθ​ 表示要訓練的模型\",\"NS​ 表示 Source Domain 的資料數量\",\"NT​ 表示 Target Domain 的資料數量\",\"XS={xHRS,m​}m=1NS​​ 表示 Source Domain 的資料\",\"XT={xHRT,m​}m=1NT​​ 表示 Target Domain 的資料\",\"YS={yHRS,m​}m=1NS​​ 表示 Source Domain 對應的 labels\",\"yHRS,m​∈{0,1}HS​×WS​×C\",\"xHRS,m​∈RHS​×WS​×3\",\"xHRT,m​∈RHT​×WT​×3\",\"過去的 UDA 方法通常會將輸入的圖片經過一個 bilinearly downsampling function ζ。例如說 xLRT​=ζ(xHRT​,1/sT​)∈RsT​HT​​×sT​WT​​×3。\",\"經過 fθ​ 可以預測出 source label y^​LRS​=fθ​(xLRS​)，也就可以用 Categorical Cross-Entropy (CCE) 去計算出 Loss。\",\"LSLce​(y^​,y,q)​=Lce​(y^​LRS​,yLRS​,1)=−i=1∑H(y)​j=1∑W(y)​c=1∑C​qij​yijc​logζ(y^​,H(y^​)H(y)​)ijc​​\",\"不過只運用這樣的 Loss 去訓練基本上不會得到太好的結果，過去的經驗上都還需要額外設計一些 Loss function LT 去幫助訓練。HRDA 採用了 DAFormer 的方法。\",\"關於 DAFormer 可以參考過去的文章\",\"DAFormer 透過 teacher model gϕ​ 去 adapt target domain，透過 gϕ​ 可以得到每個 label 預測的機率，我們會從其中取最大的當成是最終的 label pLRT​。\",\"pLR,ijcT​=[c=argmaxc′​gϕ​(xLRT​)ijc′​]\",\"Info\",\"這裡的 [⋅] 是 Iverson Bracket，只是單純符合條件給 1，否則給 0 的符號。\",\"[P]={10​ifPistrueotherwise​\",\"而 student network fθ​ 就可以透過 pseudo label 去學習 target domain 的資訊。\",\"LT=Lce​(y^​LRT​,pLRT​,qLRT​)\",\"這裡的 qLRT​ 是 DAFormer 當中的信心水平。\",\"如同 DAFormer，在實作上會包含：\",\"Transformer Network\",\"Rare Class Sampling\",\"Thing-Class ImageNet Feature Distance\",\"Learning Rate Warmup for UDA\"]},\"132\":{\"h\":\"Overview\",\"t\":[\"HRDA 做的事情簡單來說就是「透過同時考慮 Low-Resolution 以及 High-Resolution 提升模型的預測能力，且提出技巧避免 GPU 記憶體過量」。整體的架構如下圖。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"作者在 YouTube 上的簡短說明當中給出的架構圖也許會有更加直觀的理解。\",\"將圖片切成 Low-Resolution(LR) 與 High-Resolution(HR) 兩個部分\",\"透過 Segmentation Head 得出 LR 與 HR 的預測結果\",\"透過 Attention 混合 LR 與 HR 的優勢，並預測出結果\",\"Fuse 以上的結果，變成最後的預測\",\"Image from Lukas Hoyer's YouTube Channel\"]},\"133\":{\"h\":\"Context and Detail Crop\",\"t\":[\"為了同時保有 Low-Resolution(LR) 能辨識大範圍特徵的優點，以及 High-Resolution(HR) 能辨識細節的優點，HRDA 使用 large LR context crop 以及 small HR detail crop 來分別處理這兩個部分。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"大概念上，對於原本 H×W 的圖片，我們想要從中切出一塊 Low-Resolution 的大圖片(Context Crop)、一塊 High-Resolution 的小圖片(Detail Crop)。\",\"HR 會從 LR 當中切出來。更細節來說，LR 的邊長都會是 HR 的兩倍，hc​=hd​,wc​=wd​,s=2。\",\"large LR context crop\",\"首先將圖片經過裁切出一部分 xc,HR​\",\"xc,HR​=xHR​[bc,1​:bc,2​,bc,3​:bc,4​]\",\"接著透過 ζ 將裁切後的圖片 downsample 得到 Low-Resolution Context Crop xc​。\",\"xc​=ζ(xc,HR​,1/s)\",\"這裡的 bc​ 用來描述裁切的 bounding box，在選擇上會是在圖片範圍當中的 normal distribution，並且選擇了一個 k=s⋅o (o 表示 output stride) 保證選出來的座標可以被 k 整除，藉此確保後續的運作正常。\",\"​bc,1​∼U{0,(H−shc​)/k}⋅k,bc,2​=bc,1​+shc​bc,3​∼U{0,(W−swc​)/k}⋅k,bc,4​=bc,3​+swc​​\",\"small HR detail crop\",\"將 Low-Resolution Context Crop xc​ 再進一步裁切得到 High-Resolution Detail Crop xd​。\",\"xd​=xc,HR​[bd,1​:bd,2​,bd,3​:bd,4​]\",\"其中\",\"​bd,1​∼U{0,(shc​−hd​)/k}⋅k,bd,2​=bd,1​+hd​bd,3​∼U{0,(swc​−hw​)/k}⋅k,bd,4​=bd,3​+wd​​\",\"Get Semantic Segmentation\",\"接下來透過 Feature Encoder Network fE 以及 Semantic Decoder Network fS 就可以分別得到 Context Semantic Segmentation 以及 Detail Semantic Segmentation。\",\"Context Semantic Segmentation y^​c​\",\"y^​c​=fS(fE(xc​))∈Rohc​​×owc​​×C\",\"Detail Semantic Segmentation y^​d​\",\"y^​d​=fS(fE(xd​))∈Rohd​​×owd​​×C\"]},\"134\":{\"h\":\"Multi-Resolution Fusion\",\"t\":[\"如同前面所描述，我們知道 LR 與 HR 各有其好處，作者除了分開兩個部分去產出 LR 與 HR 的結果以外，也希望能夠有一個部分能夠讓模型能同時考慮 LR 與 HR 兩者去作出判斷。\",\"從 Low-Resolution 得到大範圍的資訊、從 High-Resolution 得到細節的資訊。再結合 Scale Attention，透過 Attention 機制去判斷現在應該要注重 LR 還是 HR，又分別要給多少的注意力。\",\"於是我們再加上一個 Scale Attention Decoder fA 去預測 attention scale ac​ 來表示一個點要偏向用 LR 或是 HR。\",\"ac​=σ(fA(fE(xc​)))∈[0,1]ohc​​×owc​​×C\",\"其中 σ 會限制輸出在 [0,1] 之間，越靠近 0 則越相信 LR，反之則越相信 HR。\",\"接下來，由於 Detail Crop(HR) 只有 Context Crop(LR) 的一部分，所以沒有 HR 的地方只能參考 LR，Attention 設為 0。\",\"Image from YuKai Chen @ Medium\",\"就像是這邊的 Attention Map。在 Detail Crop 以外的範圍都是黑的，也就是 Attention 為 0，中間的部分才會有 Detail 與 Context 的交互考量。\",\"寫成數學式也許比較難理解，但他只是希望把多的地方設為 0 而已。\",\"ac′​∈Rohc​​×owc​​,ac′​(i,j)={ac​(i,j)0​ifs⋅obd,1​​≤i<s⋅obd,2​​∧s⋅obd,3​​≤j<s⋅obd,4​​otherwise​\",\"最後要把結果 fuse 在一起，所以要讓大小都相等，對 Detail Crop(HR) 外面補 0，變成跟 Context Crop(LR) 相同。\",\"y^​d′​(i,j)={y^​d​(i−obi,1​​,j−obi,3​​)0​ifobd,1​​≤i<obd,2​​∧obd,3​​≤j<obd,4​​otherwise​\",\"Attention 可以告訴我們要考慮多少的 Context 以及多少的 Detail，於是把每個點跟 Attention 的值相乘，得到 Context 與 Detail 的結果後相加就會是 Fused 之後的預測結果。\",\"y^​c,F​=ζ((1−ac′​)⊙y^​c​,s)+ζ(ac′​,s)⊙y^​d′​\",\"別忘了 Attention 越靠近 0 則越相信 Context，反之則越相信 Detail\",\"訓練時就會使用 Fused 的版本去訓練，Source 採用的 Loss 是單純的 Cross Entropy 如下。額外考慮 Detail Crop y^​dS​,ydS​ 可以讓模型學習到更好的高解析度特徵。\",\"LHRDAS​=(1−λd​)Lce​(y^​c,FS​,yc,HRS​,1)+λd​Lce​(y^​dS​,ydS​,1)\",\"Target Domain 亦然。不過使用的目標是 pseudo labels，且包含了 quality 的項。\",\"LHRDAT​=(1−λd​)Lce​(y^​c,FT​,pc,FT​,qc,FT​)+λd​Lce​(y^​dT​,pdT​,qdT​)\"]},\"135\":{\"h\":\"Pseudo-Label Generation with Overlapping Sliding Window\",\"t\":[\"預測的結果並不是每一個點都會考慮到 Detail Crop 的資訊，然而我們會希望每一個地方都能夠精確地考慮，因此會希望能夠取得所有點的 Detail Crop 資訊。\",\"直覺上會覺得，既然我們想要得到全部 Detail Crop 的資訊，那就直接把原圖交給 Network 去處理即可，尤其預測的過程當中並不包含 backpropagation，也就不會有需要儲存過多資訊導致的記憶體不足問題。\",\"然而由於 HRDA 是 based on DAFormer 做出來的，當中包含了 Transformer 架構，會隱晦地在訓練過程中學習到 positional embedding 資訊，因此當訓練時與預測時採用的解析度大小相同會是最理想的。\",\"因此作者採用相同解析度大小，再使用 Sliding Window 去看過整體的圖片，重疊的部分以平均作為結果，給出更穩健的 pseudo label 預測。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"Image from Lukas Hoyer's YouTube Channel\"]},\"136\":{\"h\":\"Results\"},\"137\":{\"h\":\"實驗設定\",\"t\":[\"一如既往，這裡使用的 Datasets 包含了 Cityscapes, GTA5, SYNTHIA。\",\"在 Network 的架構上採用 DAFormer，當中包含了 MiT-B5 encoder 以及 DAFormer 中提出的 decoder。在 Scale Attention 的部份使用的是 SegFormer MLP decoder。\",\"在部分與其他 Model 比較的部份為了公平有時會將 DAFormer 的 backbone 換成 ResNet101 或是 DeepLabV2，底下使用不同架構時會再特別提醒。\"]},\"138\":{\"h\":\"Overview\",\"t\":[\"與過去的 SOTA 比較的結果如下圖所示。\",\"在 GTA5 → Cityscapes 當中全部的 Class 都獲得比過去 SOTA 更好的結果，最後的 mIoU 與 DAFormer 相比多了 5.5 的成長。\",\"在 Synthia → Cityscapes 當中則對大多數的 Class 而言都獲得更好的結果，並且 mIoU 與 DAFormer 相比多了 4.9 的成長。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"從圖片上來觀察，可以發現到 HRDA 在細節上的處理比起過去的 SOTA 有更好的處理（如：紅綠燈、騎士），並且對於大的物件也同樣保有更好的結果（如：巴士、汽車）。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"並且如果將 HRDA 應用在過去各種 UDA 的方法上，也可以看出 HRDA 都能夠有效地提升最後的 performance。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\"]},\"139\":{\"h\":\"Influence of Resolution and Crop Size on UDA\",\"t\":[\"HRDA 最一開始的假設是認為小的解析度會不好預測小物體，反之大的解析度會不好預測大物件，因此這一部分就是要驗這這個假設。\",\"他們比較了 DAFormer 以及 HRDA 在 GTA5 → Cityscapes 中調整 Crop Size 帶來的影響後發現，Crop Size 對於 UDA 與 Supervised Learning 相比更加重要。可以看到左邊的部分，當我們把 Crop Size 調低 4 倍，出來的結果會比沒有調整還要降低約 30 mIoU。\",\"同時，這裡也可以觀察到當我們把解析度提升，無論是對 UDA 或是 Supervised Learning 都會有正面的影響。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"進一步去看每個 Class 的影響，可以觀察到：\",\"比對 Row 1 與 Row 3，可以得到 Crop Size 越大，對所有 Class 的預測都有提升\",\"比對 Row 1 與 Row 2，可以得到 Resolution 越大，小物件的預測更加精確，大物件則有降低的趨勢\",\"將兩者結合後，在所有的 Class 的預測可以有更進一步的提升\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"因此驗證了假設是正確的。\"]},\"140\":{\"h\":\"Crop Size Selection\",\"t\":[\"針對 Context Crop Size 與 Detail Crop Size 做了詳細的比較如下表。\",\"可以發現到：\",\"Context Crop Size 越大，得到的結果會更好\",\"Detail Crop Size 越大，得到的結果會更好\",\"同樣的 Crop Size 下，再多搭配另一個 Resolution 都可以獲得更好的結果 \",\"不過相較之下，Low-Resolution 的有無對於最終結果的影響會是更大的\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"此外，HRDA 在 LR Crop 與 HR Crop 使用不同的 Resolution 是相當重要的。固定使用的 Context Crop 的情況下，如果採用低解析度的圖片，或是從低解析度 Upsample 到高解析度，得出來的效果都比起直接使用高解析度圖片還要差。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\"]},\"141\":{\"h\":\"Memory Usage Comparison\",\"t\":[\"HRDA 最一開始提到的問題就是使用 High Resolution 作為輸入會太過於耗費記憶體，因此這一部分會針對記憶體用量的部分去做討論。\",\"註：這裡之所以是拿 HR0.75​ 作為 baseline 是因為 HR1​ 已經放不進 24GB 的顯卡了，並且 HR0.75​ 已經有比 DAFormer 更好的 performance。\",\"比較之後可以發現到 HRDA 比起單純的 HR 可以有 3.8 mIoU 的成長，並且如果進一步把 Crop Size 縮小，即便只用了 HR 60% 的記憶體，但卻還是有 1.3 mIoU 的成長。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\"]},\"142\":{\"h\":\"Ablation Study\",\"t\":[\"在這一篇 paper 當中整體包含了幾個方法\",\"混合 Context 與 Detail Crop\",\"使用 Scale Attention 進一步產出合併結果\",\"Overlapping Detail 處理預測階段的 Detail Crop\",\"加上 Detail Loss 進一步提升效能\",\"針對這幾個部分同樣去研究對應的影響。\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"從上表可以觀察到\",\"Context 與 Detail 相較之下有更大的影響 (Row 1 & Row 2)\",\"單純 Average Context 與 Detail 並不會有更好的 performance (Row 2 & Row 3)\",\"採用 learned scale attention 對於效能提升大約 3.0 mIoU\",\"採用 Overlapping Detail 可以進一步提升 0.9 mIoU\",\"加上 Detail Loss 可以進一步提升 1.4 mIoU\",\"Image from Lukas Hoyer, Dengxin Dai, Luc Van Gool (2022)\",\"從圖片當中可以再次觀察到 LR 的結果在細節上會有缺失，HR 當中則對於大物件有較差的表現。\"]},\"143\":{\"h\":\"Contribution\",\"t\":[\"研究了 Crop Size 對於 UDA performance 的影響\",\"採用 multi-resolution fusion 搭配 scale attention 同時將 LR 與 HR 的優點帶走\",\"在相同量級或甚至更小量級的記憶體使用量下，得到比 SOTA 更好的結果\",\"提出一個能夠搭配許多 UDA Framework 的方法\",\"提出新的 SOTA model\"]},\"144\":{\"h\":\"值得一看的文章們\",\"t\":[\"[論文筆記] HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation\",\"[ECCV22] HRDA: Context-Aware High-Resolution Domain-Adaptive Semantic Segmentation\",\"Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L. Yuille (2016)\"]},\"145\":{\"c\":[\"Note\"]},\"146\":{\"c\":[\"Paper Read\",\"Domain Adaptation\",\"Computer Vision\",\"ECCV\"]},\"147\":{\"h\":\"Noisy Networks for Exploration\"},\"148\":{\"h\":\"Basic Information\",\"t\":[\"2018 ICLR\",\"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, et al. @ Google Deepmind\"]},\"149\":{\"h\":\"問題描述\",\"t\":[\"在過去的 RL 當中我們往往仰賴對 agent 的 policy 增加 randomness 去增加 exploration，例如 ϵ-greedy 和 entropy regularization 等。不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索，然而在現實狀況下往往並不會如此簡單，而這種探索的困難度甚至是指數性地成長。\",\"例如在 Alpha Go 當中就至少探索了上千億甚至到幾兆個模擬的遊戲狀態\",\"Info\",\"論文中有提及在 Matthieu Geist, Olivier Pietquin (2014) 有提及一個使用 Neural Network 的方法，不過並沒有保證收斂，因此仍然沒有解決問題。\",\"因此這一篇論文提出一個方法試圖去消除 exploration 效率與品質的問題。\"]},\"150\":{\"h\":\"Related Works\",\"t\":[\"ϵ-greedy、Entropy Regularization\",\"Parameter Space Noise for Exploration\",\"用來加上 Noisy-Net 的各種 RL 架構 \",\"DQN\",\"Double-DQN\",\"Dueling DQN\",\"A3C\"]},\"151\":{\"h\":\"Parameter Space Noise for Exploration\",\"t\":[\"2017 年由 OpenAI 發表在 ICLR 的 paper。其方法與這一篇可說是大同小異。\",\"在過往的研究可以發現到說往往我們在設計讓 agent 有更多的 exploration 的時候都是透過增加 noise 來達成。\",\"Info\",\"舉例而言，ϵ-greedy 就是在 action space 上增加了 noise，讓選擇更多樣，以達成 exploration。\",\"而在 A3C 當中加上 Entropy Regularization，是在 Loss 上鼓勵 policy 的亂度越高越好，達到鼓勵 exploration 的效果。\",\"Image from OpenAI - Better exploration with parameter noise\",\"核心的概念很簡單，過去增加探索的方法大多都是在 action space 上增加 noise，而這裡則選擇在 parameter space 上增加 noise，並且達到了很棒的效果。\",\"Tips\",\"ϵ-greedy 就像是獵人裡面的凱特，行動之前需要先看運氣抽接下來使用的武器，即便自己知道當下用哪一個 action 比較好，卻會受到 ϵ 的限制。\",\"而在 parameter space 加上 noise 就像是可以換個角度去想其他人會怎麼做，試著用那一個人的做法走過一次，得到不同的經驗。\",\"相較之下，action space 加 noise 就比較像是在亂試，反之在 parameter space 上加 noise 就比較有系統性一些。\",\"具體來說就是他們試圖在 parameter 上加上 Gaussian Noise。\",\"θ~=θ+N(0,σ2I)\",\"Action Space Noise\",\"Parameter Space Noise\",\"Videos from OpenAI - Better exploration with parameter noise\",\"Warning\",\"底下的內容只是單純的 Review\"]},\"152\":{\"h\":\"DQN\",\"t\":[\"透過 Neural Network 去學習 optimal action value function Q∗。 Action 的決定上採用了 ϵ-greedy。\",\"Image from Ziyu Wang et al. (2015)\",\"於是他們定義了底下的 Loss function 去試圖得到 Q∗。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γb∈Amax​Q(y,b;θ−)−Q(s,a;θ))2]\",\"D 是上一個 replay buffer 的 transition distribution \",\"state s\",\"action a\",\"reward r=R(s,a)\",\"probability y∼P(⋅∣s,a)\",\"θ− 是被固定的參數\",\"Tips\",\"DQN 帶來了幾個好處\",\"Experience Replay 透過儲存 Experience，更新參數是從 replay buffer 中隨意挑一筆，降低了資料之間的相關性，讓 Neural Network 訓練避免偏差。\",\"Target Network 避免了訓練目標經常地變動造成訓練效果差\",\"使用 Neural Network 替代 action value function 避免了 Q-Learning 的 table 維度過大訓練困難的問題\"]},\"153\":{\"h\":\"Double-DQN\",\"t\":[\"在 DQN 當中我們需要同時訓練兩個 model，也就是 θ 與 θ−。然而 DQN 的設定上對於目標被發現存在高估的問題，因此 Double-DQN 提出了解決這個高估問題的方法。\",\"原始 DQN 目標\",\"Double-DQN 目標\",\"r+γmaxb∈A​Q(y,b;θ−)\",\"r+γQ(y,maxb∈A​Q(y,b;θ);θ−)\",\"高估的狀況如底下的綠線。紫線是目標函數，橘線是綠線與紫線的誤差，不難發現到確實都存在高估的狀況。\",\"然而使用了 Double-DQN 之後的誤差(藍線)就小到幾乎不存在了。\",\"Image from Hado van Hasselt, Arthur Guez, David Silver (2015)\",\"Tips\",\"Double-DQN 帶來的幾個好處\",\"讓 DQN 高估的問題消失，有更好的效果\"]},\"154\":{\"h\":\"Dueling DQN\",\"t\":[\"Dueling DQN 的概念仍然是透過 Neural Network 去學習 optimal action value function Q∗。 Action 的決定上採用了 ϵ-greedy。\",\"Image from Ziyu Wang et al. (2015)\",\"與 DQN 不同的地方在於他並不是直接去學習 Q∗，而是另外定義了一個 Advantage functionA。\",\"A(s,a)=Q(s,a)−V(s)\",\"V(s) 就像是 baseline，表示著在當前這個 state s 底下你預期可以拿到多好的 return，所以 A(s,a) 意義上就是在看每個 action 有多好多壞。\",\"透過 V 和 A 的總和就能夠得到 Q。上圖就是在最後分開成兩個輸出結果 V 和 A，最後合併成 Q。\",\"Q(s,a;θ)=V(s;θV​)+A(s,a;θA​)\",\"Info\",\"在實務上為了避免像是 V(s) 都是 0，實際上跟 DQN 沒有差異的問題，因此細節上是還會對 A(s,a) 加上總和為 0 的限制。\",\"Q(s,a,θ,α,β)=V(a,θ,α)+​A(s,a,θ,β)−∣A∣1​a′∈∣A∣∑​A(s,a′,θ,β)​\",\"α,β 只是調整 V 和 A 兩部分影響程度的參數。\",\"把 Dueling DQN 搭配 Double DQN 之後可以得到底下的 Loss。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γQ(y,argb∈Amax​Q(y,b;θ);θ−)−Q(s,a;θ))2]\",\"Tips\",\"Dueling DQN 帶來了幾個好處\",\"使用 Advantage function 增加模型的更新與 exploration。\"]},\"155\":{\"h\":\"A3C\",\"t\":[\"在 DQN 當中使用了 Experience Reply 去避免訓練資料上的強關聯性，然而存在幾個缺點\",\"需要額外的 memory 去儲存 replay buffer\",\"需要 off-policy alogorithm，對於 online RL 來說可能導致收斂不穩定以及緩慢等問題\",\"A3C 的概念就如同火影忍者的影分身之術，讓每個分身在各自的環境當中訓練，訓練成效也就翻倍。\",\"Image from Arthur Juliani@Medium\",\"A3C 是使用 advantage actor-critic 的方式，會直接去學 policy 以及 value function。因此在參數上也就包含了兩項 θπ​ 以及 θv​ 分別表示 policy 以及 value function 的參數。考慮在時間 t 往後看 k 步的更新，參照 A3C 的論文，兩個參數的 Loss 計算分別如下。\",\"∇θπ​​Lπ(θπ​)LV(θv​)​=−Eπ[i=0∑k​∇θπ​​log(π(at+i​∣st+i​;θπ​))A(st+i​,at+i​;θv​)+βi=0∑k​∇θπ​​H(π(⋅∣st+i​;θπ​))]=i=0∑k​Eπ[(Qi​−V(st+i​;θv​))2∣st+i​]​\",\"A：Advantage function 也就是 A3C 當中的 R−V\",\"H：Entropy function 根據 A3C 的論文，加上這一項能夠促使模型更好 exploration\",\"β：調整 A 和 H 的影響程度\",\"Qi​：在時間 i 時於 state st+i​ 執行 policy π 的 return\",\"Note\",\"紅色的部分也就是前面提及的 entropy regularization\",\"最後整體的 Loss 也就如下\",\"L(θ)=Lπ(θπ​)+λLV(θv​)\",\"λ 用來調整兩個 Loss 的影響力\",\"Info\",\"Note：原始 A3C 論文中更新一步，並且沒有使用 entropy 的算式\",\"θ′θv′​​:dθ←dθ+∇θ′​logπ(ai​∣si​;θ′)(R−V(si​;θv′​)):dθv​←dθv​+∂(R−V(si​;θv′​))2/∂θv′​​\",\"Tips\",\"A3C 帶來了幾個好處\",\"降低訓練資料之間的關聯性 畢竟每個 agent 訓練的環境都不同，得到的資料也就不同\",\"能夠使用 on-policy 或是 off-policy，增加通用性\",\"可以平行化加速訓練\",\"穩定地訓練\"]},\"156\":{\"h\":\"Methodology\"},\"157\":{\"h\":\"基本想法\",\"t\":[\"Noisy-Net 的想法跟 Parameter Space Noise for Exploration 的想法基本上是相同方向，都是要對 parameter space 去加上 noise。\",\"作法上，對於每個可訓練的參數拆解成 ζ=(μ,σ)，然後再透過 zero-mean 的 ϵ 增加 noise。也就是說對於一個參數 θ 我們會寫成：\",\"θ=defμ+σ⊙ϵ\",\"所以對於一個 Linear Layer 來說\",\"y=wx+b⇒y=(μw+σw⊙ϵw)x+(μb+σb⊙ϵb)\",\"就只是這樣而已，不要想太多！\",\"Tips\",\"刻意挑 zero-mean 的 noise 是為了採用底下的特性方便後續 Loss 的計算。\",\"Lˉ(ζ)=E[L(θ)]\",\"因此\",\"∇Lˉ(ζ)=∇E[L(θ)]=E[∇μ,Σ​L(μ+Σ⊙ϵ)]\",\"Σ 包含了所有 σ\",\"加上了 Monte-Carlo approximation 之後，可以用單一的 sample ξ 去近似\",\"∇Lˉ(ζ)≈∇μ,Σ​L(μ+Σ⊙ξ)\",\"跟 OpenAI 提出的方法略為不同的地方在於他並不是直接對 network 的參數加上 Gaussian Noise，而是給了參數 ϵw,ϵb 去決定要加怎樣的 noise。\",\"在每一個 episode 開始之前先把參數加上 noise，接下來這一整個 episode 就都是用這個 network 去訓練，意即在過程中不會對 noise 做調整。\"]},\"158\":{\"h\":\"減少產 random number 時間\",\"t\":[\"這樣的做法下每一個 episode 都需要 random noise 在 weight 和 bias 上。假如 w∈Rq×p,b∈Rq，那麼 ϵw∈Rq×p,ϵb∈Rq，也就意味著需要 random 出 pq+q 個數值。\",\"上面基本的做法作者稱他為 Independent Gaussian noise，而接下來作者給出一個 Factorised Gaussian noise 的做法。\",\"基本上就是將 random number 拆分\",\"ϵi,jw​ϵjb​​=f(ϵi​)f(ϵj​)=f(ϵj​)​\",\"其中 f(x)=sgn(x)∣x∣​,ϵi​∈Rq,ϵj​∈Rp。\",\"如此一來，只需要產出 p+q 個 random number 也可以達到類似的效果。\"]},\"159\":{\"h\":\"DQN & Dueling DQN\",\"t\":[\"由於 DQN 和 Dueling DQN 是在 single-thread 上訓練，因此上述的 Random Overhead 會比較大，在這裡採用 Factorised Gaussian noise。\",\"現在 Network 的部分改用 Noisy Network，也就不需要再使用 ϵ-greedy 了。\",\"原本的 DQN 對 Loss 的定義如下。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γb∈Amax​Q(y,b;θ−)−Q(s,a;θ))2]\",\"現在替換成 Noisy Net 的形式\",\"Lˉ(ζ)=E[E(s,a,r,y)∼D​[r+γb∈Amax​Q(y,b,ϵ′;ζ−)−Q(s,a,ϵ;ζ)]2]\",\"最外層的期望值是對 ϵ 和 ϵ′\",\"同樣地也可以對 Dueling DQN 做修改。原本的定義為\",\"L(θ)=E(s,a,r,y)∼D​[(r+γQ(y,argb∈Amax​Q(y,b;θ);θ−)−Q(s,a;θ))2]\",\"現在替換成 Noisy Net 的形式\",\"Lˉ(ζ)=E[E(s,a,r,y)∼D​[r+γQ(y,argb∈Amax​Q(y,b,ϵ′′;ζ),ϵ′;ζ−)−Q(s,a,ϵ;ζ)]2]\"]},\"160\":{\"h\":\"Distributed A3C\",\"t\":[\"由於 A3C 是在 multi-thread 上訓練，因此不太需要考慮上述的 Random Overhead，在這裡採用 Independent Gaussian noise 即可。\",\"現在 Network 的部分改用 Noisy Network，也就不需要再使用 Entropy function 了。\",\"原本的 A3C\",\"∇θπ​​Lπ(θπ​)LV(θv​)​=−Eπ[i=0∑k​∇θπ​​log(π(at+i​∣st+i​;θπ​))A(st+i​,at+i​;θv​)+βi=0∑k​∇θπ​​H(π(⋅∣st+i​;θπ​))]=i=0∑k​Eπ[(Qi​−V(st+i​;θv​))2∣st+i​]​\",\"NoisyNet-A3C\",\"∇ζπ​​Lπ(ζπ​)LV(ζv​)​=−E[Eπ[i=0∑k​∇ζπ​​log(π(at+i​∣st+i​;ζπ​,ϵ))A(st+i​,at+i​;ζv​,ϵ)]]=E[i=0∑k​Eπ[(Qi​−V(st+i​;ζv​,ϵ))∣st+i​]]2​\",\"Noise initialize details\",\"Independent Gaussian noise\",\"μi,j​∼U[−p3​​,+p3​​]\",\"p 是 input 的數量\",\"σi,j​=0.017\",\"Factorised Gaussian noise\",\"μi,j​∼U[−p1​​,+p1​​]\",\"p 是 input 的數量\",\"σi,j​=p​0.5​\"]},\"161\":{\"h\":\"Results\"},\"162\":{\"h\":\"Experiments\",\"t\":[\"實驗是做在 57 Atari games 上。每 1M 個 frames 評估一次，episode 每 108K frames 會 truncate 一次。將沒有做任何修正的 DQN、Dueling DQN、A3C 作為 Baseline。\",\"首先把 Baseline 以及加上 NoisyNet 的模型都跟 Human 比較，底下是用來評估優劣的評分方式。\",\"100×ScoreHuman​−ScoreRandom​Scoreagent​−ScoreRandom​​\",\"Note\",\"最後得出的結果為 0：跟 Random 一樣糟 最後得出的結果為 100：跟 Human 一樣好\",\"可以從分數上明顯看出來加上了 NoisyNet 後對於 Mean 以及 Median 都有正面的影響。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\",\"接下來評估加上 NoisyNet 帶來的影響力，評分方式會也跟 Baseline 比較。\",\"100×max(ScoreHuman​,ScoreBaseline​)−ScoreRandom​Scoreagent​−ScoreBaseline​​\",\"可以看到在大多數的遊戲加上了 NoisyNet 之後的結果都有些進步。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\",\"不過進步主要在 DQN 以及 Dueling 上較為顯著。A3C 的部分在退步也是有幾項退步蠻多，也並不是每次加上 NoisyNet 都會帶來 improvement。\",\"從訓練中的曲線也可以明顯看到 NoisyNet 可以帶來很不錯的 improvement。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\"]},\"163\":{\"h\":\"Analysis\",\"t\":[\"為了進一步去釐清這樣的做法為什麼是可行、合理的，作者進一步去研究。\",\"回顧一下我們加上 Noise 的方法，是把一個可訓練參數拆成 ζ=(μ,σ)，再額外多一個 Noise ϵ。\",\"θ=defμ+σ⊙ϵ\",\"理想上，我們最後的 Loss 應該要能夠好好收斂，也就是說最後的 solution 應該要是 deterministic。那麼這裡加上的 ϵ 就應該隨著訓練慢慢被忽視，作用只在於訓練的前中期提供 exploration。因此，我們也就會期待 σ 這個參數會漸漸趨近於 0 了！\",\"定義底下的平均\",\"Σˉ=Nweights​1​i∑​∣σiw​∣\",\"作者發現在每一個遊戲當中最後一個 Layer 的 Σˉ 都是會逐漸趨近於 0 的，然而若觀察倒數第二個 Layer 卻並不一定了，有些甚至是遞增的。也就是說，其實 NoisyNet 並不會都得出 deterministic solution。\",\"此外，透過 Σˉ 的曲線也可以發現到在不同的遊戲當中他們的更新曲線相當地不同，也說明了實際上這樣的更新方式是能夠依照不同的遊戲去適應的。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\"]},\"164\":{\"h\":\"Contribution\",\"t\":[\"提供一個簡單又有效的 Exploration 方式\",\"能夠在 on-policy 以及 off-policy 上適用\",\"能夠輕易地套用在所有的 RL 算法當中\"]},\"165\":{\"h\":\"值得一看的文章們\",\"t\":[\"强化学习中on-policy 与off-policy有什么区别？\",\"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)\",\"Asynchronous Methods for Deep Reinforcement Learning\",\"Deep Exploration via Randomized Value Functions\",\"Kalman Temporal Differences\",\"VIME: Variational Information Maximizing Exploration\",\"Parameter Space Noise for Exploration\",\"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\",\"Better exploration with parameter noise\",\"强化学习中的探索与利用（count-based)\"]},\"166\":{\"c\":[\"Note\"]},\"167\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"ICLR\"]},\"168\":{\"h\":\"PiPa: Pixel- and Patch-wise Self-supervised Learning for Domain Adaptative Semantic Segmentation\"},\"169\":{\"h\":\"Basic Information\",\"t\":[\"Mu Chen, Zhedong Zheng, Yi Yang, Tat-Seng Chua\",\"2022 ACM Multimedia\"]},\"170\":{\"h\":\"問題描述\",\"t\":[\"這一篇與過去看過的 DACS, ProDA, DAFormer, HRDA 同樣都是以 Unsupervised 的方式解決 Semantic Segmentationb 的 Domain Adaptation問題。\",\"也就是說，我們會在一個 Source Domain 上具有標記過的資料，但是 Target Domain 上則缺乏標記。我們的目標是透過這些資料去學習，讓這個模型有辦法對 Target Domain 上的資料順利地給予正確的 Label。\",\"過去的這些 Works 普遍關注於如何在不同 Domain 之間築起橋梁，讓他們的 Domain Gaps 減少。包含了在 pixel level, feature level 以及 prediction level 之間的 Domain Gaps。\",\"然而，這卻忽略了在同一個 Domain 當中的特徵。\",\"Info\",\"這就好比我們學會對應 Source Domain 當中的\\\"汽車\\\"如何對應到 Target Domain，卻不太了解同樣 Source Domain 當中同樣是\\\"汽車\\\"的部份有怎樣的關聯。\",\"作者提出 PiPa，一個 Pixel-wise 以及 Patch-wise 的 self-supervised 的架構，能夠應用在過去的各種 UDA for semantic segmentation 問題上，並超越過去的 SOTA。\"]},\"171\":{\"h\":\"Related Works\",\"t\":[\"Unsupervised Domain Adaptation (UDA)\",\"Contrastive Learning\"]},\"172\":{\"h\":\"Methodology\",\"t\":[\"大致分成了三個主軸\",\"基本的 UDA Loss 設定\",\"Pixel-wise Contrastive Learning\",\"Patch-wise Contrastive Learning\",\"最後再將這三個部份結合起來。\"]},\"173\":{\"h\":\"基本的 UDA Loss 設定\",\"t\":[\"與過往我們讀過的幾篇論文一樣，我們會先設定好最基本的兩組 Loss。分別會希望我們的模型在 Source Domain 以及 Target Domain 的預測結果要與對應的 Label 相同。\",\"對於 Target Domain，因為缺少了 Label 標記，因此會使用 Pseudo Label。\",\"LceS​LceT​​=E[−puS​loghcls​(gθ​(xuS​))]=E[−pv​ˉ​Tloghcls​(gθ​(xvT​))]​\",\"也與過去的做法相同，會加上知識蒸餾(Knowledge Distillation, KD)，因此會包含了 Student Network 以及 Teacher Network。\",\"其中的一些 Notations：\",\"S 表示 Source Domain，包含了 U 個資料。\",\"T 表示 Target Domain，包含了 V 個資料。\",\"xuS​ 表示在 Source Domain 的第 u 個資料。而 yuS​ 是對應的 Label。\",\"xvT​ 表示在 Target Domain 的第 v 個資料。\",\"puS​ 是 yuS​ 轉換成 one-hot 的形式。\",\"pv​ˉ​T 是預測的 pseudo label yv​ˉ​T 轉換成 one-hot 的形式。\",\"gθ​ 是我們的模型 backbone。對應的 teacher network 為 gθˉ​。\",\"hcls​ 是最終給出 label 的 network。\",\"針對 Target Domain 的 Loss，作者提及因為 Domain Gap 的存在，導致 pseudo label 必然會存在 noise。也就意味著並不是所有的 pseudo label 都值得信任。\",\"因此 LceT​ 計算上只會考慮 yv​ˉ​T 大於某個 threshold 的部份。意即只有那些足以信任的預測結果才會被考慮進去。\",\"此外，也如同 DAFormer 與 DACS，他們會對 Source Domain 與 Target Domain 的圖片去做混合，得到相對應的圖片 xvMix​ 與 Pseudo Label yv​ˉ​Mix，當然也有對應的 one-hot vector pv​ˉ​Mix。因此 LceT​ 被改寫如下。\",\"LceT​=E[−pv​ˉ​Mixloghcls​(gθ​(xvMix​))]\"]},\"174\":{\"h\":\"Pixel-wise Contrastive Learning\",\"t\":[\"Image from Mu Chen, Zhedong Zheng, Yi Yang, Tat-Seng Chua (2022)\",\"Pixel-wise 的想法就是希望具有相同 Class 的 Pixel 要有類似的 Feature，反之則要有較為不同的 Feature。如上圖，同樣是圓圈的部份會被拉進，不同的則被推遠。\",\"實際上的做法是把 gθ​ 產出的 feature 經過 Projection Head hpixel​ 得到對應的 Feature Map e。接著透過 Contrastive Learning 去把應該要相近的 feature 拉近，反之推遠。\",\"定義 Loss Lpixel​ 如下。\",\"Lpixel​=−C(i)=C(j)∑​log∑k=1Npixel​​(r(ei​,ej​))r(ei​,ej​)​\",\"其中\",\"ei​ 表示 Feature Map e 的第 i 個特徵。也就是 pixel i 對應的特徵。\",\"C(i) 表示 pixel i 對應的 class。\",\"r(ei​,ej​) 用來描述 ei​ 與 ej​ 的相似性。採用的是 Exponential Cosine Similarity。\",\"r(ei​,ej​)=exp(s(ei​,ej​)/τ) 其中 s 表示 cosine similarity。\",\"由於這裡需要 class 的資訊，因此只會使用到 Source Domain 的資料。\"]},\"175\":{\"h\":\"Patch-wise Contrastive Learning\",\"t\":[\"Image from Mu Chen, Zhedong Zheng, Yi Yang, Tat-Seng Chua (2022)\",\"Patch-wise 的想法是今天同一個 crop 無論是出現在 patch M1​ 的右上角還是 patch M2​ 的左下角，因為同樣都是對應到相同的 crop，所以特徵也必須要相同。\",\"作法上會把 Mixed Image 經過 Projection Head hpatch​ 後，從當中切出兩個大小相同的 Patch M1​,M2​，並且他們之間有一個重疊的區塊 O1​,O2​。目標是要讓同樣都在 O1​,O2​ 的特徵拉近，否則推遠。\",\"定義 Loss Lpatch​ 如下。\",\"Lpatch​=−O1​(i)=O2​(j)∑​log∑k=1Npatch​​r(fi​,fk​)r(fi​,fj​)​\",\"其中\",\"fi​ 表示 pixel i 經過 hpatch​ 得到對應的特徵。\",\"r 與 Pixel-wise 一樣表示 Exponential Cosine Similarity。\"]},\"176\":{\"h\":\"結合\",\"t\":[\"最後結合上面三個部分，可以得到最終的 Loss 如下。\",\"Ltotal​=LceS​+LceT​+αLpixel​+βLpatch​\",\"整體的架構可以用底下這張圖來簡單了解。\",\"image\",\"Image from Mu Chen, Zhedong Zheng, Yi Yang, Tat-Seng Chua (2022)\",\"由於 Pixel-wise Consistency 與 Patch-wise Consistency 都是在幫助訓練，因此在測試階段的時候這兩個部分是不會參與的。上圖當中的藍色區塊都是只在訓練階段包含的架構。\"]},\"177\":{\"h\":\"Results\"},\"178\":{\"h\":\"實驗設定\",\"t\":[\"與過去相同，我們一樣會有 GTA5, Cityscapes, SYNTHIA 這三個 datasets，測試 (1) GTA5 → Cityscapes 以及 (2) SYNTHIA → Cityscapes 的結果。\",\"實作上採用了常見的 mmsegmentation framework，Network 的架構由於 PiPa 的通用性，作者有在 DAFormer 以及 HRDA 分別搭配 PiPa 去做實驗。\"]},\"179\":{\"h\":\"Quantitative Comparison\",\"t\":[\"首先看到 GTA5 → Cityscapes 的結果。\",\"image\",\"Image from Mu Chen, Zhedong Zheng, Yi Yang, Tat-Seng Chua (2022)\",\"可以看到無論是把 PiPa 搭配 DAFormer 或是 HRDA 都可以進一步得到更好的最終結果。而對於細部每一個預測的 Class 則可以看到在絕大多數的類別都有了提升。\",\"接下來看到 SYNTHIA → Cityscapes 的結果。\",\"image\",\"Image from Mu Chen, Zhedong Zheng, Yi Yang, Tat-Seng Chua (2022)\",\"同樣也可以看到與 GTA5 → Cityscapes 相同的結果。\",\"Tips\",\"與 HRDA + MIC 相較之下，HRDA + PiPa 在 GTA5 → Cityscapes 小輸 0.3 mIoU，而 SYNTHIA → Cityscapes 則大贏 7.5 mIoU。\",\"Info\",\"SYNTHIA 這個 Dataset 因為部分 paper 採用 16 個 classes，部分則是 13 個 classes 的資料去訓練，因此在數據上 mIoU 有兩列分別表示 16 個平均跟 13 個的平均。\"]},\"180\":{\"h\":\"Qualitative Results\",\"t\":[\"image\",\"Image from Mu Chen, Zhedong Zheng, Yi Yang, Tat-Seng Chua (2022)\",\"從圖片上的結果可以看到結果有了一些提升，主要是在細節的呈現上更精準了。\"]},\"181\":{\"h\":\"Ablation Studies\",\"t\":[\"Image from Mu Chen, Zhedong Zheng, Yi Yang, Tat-Seng Chua (2022)\",\"針對 GTA5 → Cisyscapes 的部份作者嘗試了解 Patch Contrast 與 Pixel Contrast 分別帶來的效益。可以看到兩者分別使最後結果提升了 1.4 mIoU 與 2.3 mIoU，並且兩者結合後可以再帶來更高的 3.3 mIoU 的提升。\",\"Image from Mu Chen, Zhedong Zheng, Yi Yang, Tat-Seng Chua (2022)\",\"針對 α 與 β 的選擇上，作者認為 PiPa 比較不會對 hyperparameter 敏感。在實驗的幾組 α,β 都會帶來相近的結果，並且都比 baseline DAFormer 的 68.4 來得高。\",\"Image from Mu Chen, Zhedong Zheng, Yi Yang, Tat-Seng Chua (2022)\",\"最後，針對 Patch-wise 得 Crop Size，作者認為普遍來說 Crop Size 是越高越好，不過實驗中 720x720 的大小會是最恰當的。當 Crop Size 過大時，會時常導致 Overlapped 的區域過大，因此也不太適合將 Crop Size 社得太大。\"]},\"182\":{\"h\":\"Contribution\",\"t\":[\"針對 intra-domain knowledge 去改善 UDA 的成果\",\"設計出一個通用的架構\",\"在 GTA5 → Cityscapes 與 SYNTHIA → Cityscapes 與 HRDA 結合後分別得到 75.6 與 68.2 mIoU\"]},\"183\":{\"c\":[\"Note\"]},\"184\":{\"c\":[\"Paper Read\",\"Domain Adaptation\",\"Computer Vision\",\"ACM Multimedia\"]},\"185\":{\"h\":\"Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation\"},\"186\":{\"h\":\"Basic Information\",\"t\":[\"Pan Zhang1, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen @ University of Science and Technology of China, Microsoft Research Asia\",\"2021 CVPR\"]},\"187\":{\"h\":\"問題描述\",\"t\":[\"如同前面看過的 DACS，這一篇 paper 也是想要解決 semantic segmentation 當中 UDA(Unsupervised Domain Adaption) 的問題。\",\"近年來流行 self-training 方法，透過 Pseudo Labelling 的方式來處理。也就是說會在訓練的過程當中透過當下的預測給這些 training data 一個假的 label，然後再拿去訓練。雖然這種做法開始能夠讓 source domain 適應 target domain 了，但比起 supervised learning 與 semi-supervised learning 的 performance 還是相差許多。\",\"作者認為目前的做法存在兩個問題\",\"只選擇信度高於某個閥值的預測作為 pseudo label，但結果不一定正確，會使模型被誤導 例如下圖，圈起來的 - 就被錯誤分類。\",\"Image modified from Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen (2021)\",\"由於 Domain Gap 很大，Network 傾向在 target domain 上產生比較雜亂的特徵 就像是看到有殼的動物就當成是昆蟲，把四隻腳站立的動物都當成草食類動物一樣。認知跟實際充滿巨大的 Gap 導致對結果的特徵很雜亂。\",\"針對上述兩個問題，作者分別使用 Online pseudo labels denoising 以及 learning a compact target structure 來解決。\"]},\"188\":{\"h\":\"Related Works\",\"t\":[\"UDA\",\"Unsupervised representation learning\",\"Learning from noisy labels\",\"Self-training\"]},\"189\":{\"h\":\"Methodology\"},\"190\":{\"h\":\"Preliminary\",\"t\":[\"這裡先定義一下接下來會用到的基本 Notation。\",\"ns​,nt​ 分別表示 source 和 target dataset 的大小\",\"Xs​={xs​}j=1ns​​,Xt​={xt​}j=1nt​​ 分別表示 source 和 target dataset\",\"Ys​={ys​}j=1ns​​,Yt​={yt​}j=1nt​​ 分別表示 source 和 target dataset 對應的 segmentation labels\",\"y^​t​ 表示 pseudo label\",\"Ys​ 和 Yt​ 都有 K 個共通的 classes\"]},\"191\":{\"h\":\"Target\",\"t\":[\"semantic segmentation 當中的 UDA 問題，目標在於給定 Xs​,Xt​,Ys​，不知道 Yt​ 的前提下，去預測 target dataset 的 semantic segmentation。\",\"其中一種方法是採用 Pseudo Labels，透過如 Cross Entropy Loss 來調整模型的機率分布。\",\"lcet​=−i=1∑H×W​k=1∑K​y^​t(i,k)​log(pt(i,k)​)\",\"其中 pt(i,k)​ 是一個 softmax probability 表示 pixel xt(i)​ 是 class k 的機率。 至於 y^​t(i,k)​ 則會直接表示屬於哪一個 class，也就是 hard labels。也額外定義 ξ 來轉換 soft 與 hard labels。\",\"y^​t(i,k)​={1,0,​if k=argmaxk′​pt(i,k′)​otherwise​,yt​^​=ξ(pt​)\",\"而一個 network h 也可以拆成 feature extractor f 以及 classifier g 兩個部分，用 h=f∘g 來表示。\"]},\"192\":{\"h\":\"Prototypical pseudo label denoising\",\"t\":[\"作者認為每經過一個 training stage 才去更新 pseudo label 會太慢，在一個 training stage 當中 network 可能已經 overfit 在那些充滿噪點的 labels，被錯誤的資訊誤導了。\",\"很直覺地，會想要讓 network 的參數更新、pseudo label 的更新 兩個可以同時處理。\",\"然而，若直接同時更新的話，network 會很容易忽略了細部的特徵，進而傾向 overfit 在 source domain，只在 source domain 獲得高的分數。\",\"因此作者提出的方法是將 soft pseudo labels 固定住，對於每個 class k 選擇一個 prototype η(k) 以及一個對應的 weight wt(i,k)​。訓練過程中根據與 prototype 之間的距離去調整 weight，進而影響預測的 pseudo label。\",\"y^​t(i,k)​=ξ(wt(i,k)​pt,0(i,k)​)\",\"wt(i,k)​ 就是上述的 weight\",\"pt,0(i,k)​ 與過去的 soft pseudo label pt,(i,k)​ 稍有不同，整個訓練過程中都會固定住\",\"Tips\",\"跟 Clustering 頗類似，每個 cluster 的中心點就如同這裡的 prototype，距離 cluster A 中心點越近，模型就越相信他是屬於 cluster A。\",\"我們會隨著訓練過程慢慢調整 prototype，讓他越來越貼合真實的狀況。\",\"Image from Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen (2021)\",\"當然，這裡的距離是投射到高維空間之後 feature 之間的距離！\"]},\"193\":{\"h\":\"權重計算\",\"t\":[\"權重的計算方式如下\",\"wt(i,k)​=∑k′​exp(−∥f~​(xt​)(i)−η(k′)∥/τ)exp(−∥f~​(xt​)(i)−η(k)∥/τ)​\",\"f(xt​)(i) 表示第 i 筆 target data 的 feature\",\"f~​(xt​)(i) 是 momentum encoder，可以看成是更新較慢的 encoder\",\"η(k) 表示 class k 的特徵中心點\",\"τ 表示 softmax temperature，這裡設為 1\",\"權重的計算方式本質上就是 softmax function。計算的是 feature 跟每個 class 的中心點 η(k) 距離遠近。\",\"當距離很大時，產出的權重就會很小\",\"當距離很小時，產出的權重就會很大\"]},\"194\":{\"h\":\"prototype 計算\",\"t\":[\"而 prototype 的計算方式如下\",\"η(k)=∑xt​∈Xt​​∑i​1(y^​t(i,k)​==1)∑xt​∈Xt​​∑i​f(xt​)(i)∗1(y^​t(i,k)​==1)​\",\"Prototype 的計算本質上就是找中心點。把所有對應到 class k 的 feature 加總後平均。\",\"然而這種做法每次要更新 prototype 就需要看過整個 target dataset 的所有 features，計算上負擔過大。因此作者改用一個 mini-batch 當中中心點的 moving average 來估計 (Exponential Moving Average, EMA)。\",\"η(k)←λη(k)+(1−λ)η′(k)\",\"η′(k) 表示當前 mini-batch 當中 class k 的 feature 平均\",\"λ=0.9999\"]},\"195\":{\"h\":\"Loss 計算\",\"t\":[\"至此我們有了新的方法取得 pseudo labels (也就是 y^​t(i,k)​=ξ(wt(i,k)​pt,0(i,k)​))，最後就剩下更新的 Loss 如何計算。\",\"與傳統的 Cross Entropy(CE) 不同，這裡作者採用 Symmetric Cross Entropy(SCE) 試圖增加對 lebel noise 的容忍程度。\",\"lscet​=αlce​(pt​,y^​t​)+βlce​(y^​t​,pt​)\",\"α=0.1\",\"β=1\",\"Tips\",\"改成透過 prototype 去調整 pseudo label 能夠帶來許多的好處\",\"對 outlier 比較不敏感\",\"每個 class 都是平等的，較不會因為 class 的不平衡導致預測錯誤 \",\"在 semantic segmentation 中這一點尤其重要，因為 class 的分布往往分散\",\"實際上對於 hard class 的預測有改善\",\"對於一開始預測錯誤的 pseudo labels 能夠漸漸改正\"]},\"196\":{\"h\":\"Structure learning by enforcing consistency\",\"t\":[\"理想上，只要我們的 feature extractor 能好好表示出 feature，即便在 target domain 上也能好好地區分不同的 class，那麼 pseudo label 也就可以更好地減輕 noise 的影響。\",\"然而因為 Domain Adaption 尤其 UDA 對於 target domain 的認識嚴重缺乏，encode 出來的 features 往往會很分散。\",\"作者透過對所擁有的 target domain 知識增強來改善，並且分成了弱增強 T(xt​) 以及 強增強 T′(xt​)。實際上，弱增強只是給原圖，強增強只是加上 data augumentation。\",\"這裡使用的 Data Augumentation 包含了旋轉、明暗調整、彩度調整等\",\"我們的目標是要讓 T(xt​) 與 T′(xt​) 對應的 feature 可以比較接近。作者分別去計算兩者的 weight zT​ 與 zT′​ (稱為 Soft prototypical assignment)，試圖讓他們產出的分布要越接近越好。因此透過 KL divergence 去計算 loss lklt​。\",\"lklt​=KL(zT​∥zT′​)\",\"其中\",\"zT(i,k)​=∑k′​exp(−∥f~​(T(xt​))(i)−η(k′)∥/τ)exp(−∥f~​(T(xt​))(i)−η(k)∥/τ)​,zT′(i,k)​=∑k′​exp(−∥f(T(xt​))(i)−η(k′)∥/τ)exp(−∥f(T(xt​))(i)−η(k)∥/τ)​\",\"兩者差異只在於使用的 encoder 分別是 f 和 f~​。因為 zT​ 是由弱增強得到，受到的干擾較少，因此適合用來教 encoder 經過強增強的 prototype assignment 應該與經過弱增強的相同。\",\"ProDA\",\"Tips\",\"如此一來，就能夠迫使模型對於這些略有不同的 feature 具有相同的 pseudo label，使得 target domain features 更加密集。\",\"最後，為了避免所謂的 degeneration issue，也就是有些 cluster 是空的狀態，作者進一步設計一個 loss lregt​ 鼓勵類別盡量地平均。\",\"lregt​=−i=1∑H×W​j=1∑K​logpt(i,k)​\",\"將上述的種種 loss 結合，合併成底下的 loss ltotal​。\",\"ltotal​=lces​+lscet​+γ1​lklt​+γ2​lregt​\",\"γ1​=10\",\"γ2​=0.1\"]},\"197\":{\"h\":\"Distillation to self-supervised model\",\"t\":[\"最後，作者進一步加上了 Knowledge Distillation，使最終的結果進一步提昇。\",\"透過前面的步驟得到的模型稱為 h，會做為 KD 當中的 Teacher Model。\",\"要訓練的 Student Model 稱為 h†。會採用 SimCLRv2 的 pretrain weights 開始訓練。\",\"為了避免 student model 忘記 source domain 的知識，所以也會把 source domain 的資料拿進來使用。整體的 Loss lKD​ 計算方式如下。\",\"lKD​=lces​(ps​,py​)+lcet​(pt†​,ξ(pt​))+βKL(pt​∥pt†​)\"]},\"198\":{\"h\":\"整體流程\",\"t\":[\"整體流程被分成三個階段。\",\"第一階段包含了 Prototypical Pseudo Label Denoising 以及 Target Structure Learning。 目的是要先訓練出一個 Teacher Model。意即讓 ltotoal​ 收斂。\",\"ProDA_Losses\",\"第二與第三階段都是 Knowledge Distillation。 目的是要訓練出一個 Student Model。意即讓 lKD​ 收斂。\",\"ProDA_Losses2\",\"Caution\",\"根據 Appendix 附上的 algorithm，實際上他所謂的 避免模型忘記 source domain 只不過是把 source data 扔進 \\\"teacher model\\\"，取得 lces​(ps​,py​)，然後拿去 tune \\\"student model\\\"。\",\"但是從頭到尾都沒丟給 student model，為甚麼可以直接拿去 tune，並且預期能夠讓 student model \\\"學會 source domain\\\" 的資料?\"]},\"199\":{\"h\":\"Results\"},\"200\":{\"h\":\"實驗設定\",\"t\":[\"Segmentation 模型採用 DeepLabv2 搭配 ResNet-101 Backbone。\",\"訓練前首先透過 AdaptSegNet 搭配對抗式學習對 segmentation 模型 warmup。\",\"Knowledge Distillation 的部分採用了 pretrained SimCLRv2 搭配 ResNet-101 backbone。\",\"Dataset 的部分一如既往採用 GTA5、SYNTHIA、Cityscapes 這三個 datasets。之前在 DACS 也有介紹過同樣的 datasets。\",\"接下來直接看 ProDA 在兩個 benchmarks 上面的表現。\"]},\"201\":{\"h\":\"GTA5 → \\\\rightarrow → Cityscapes\",\"t\":[\"在 GTA5 → Cityscapes 的部分明顯可以看到最後的 mIOU 比起過去的 SOTA models 好許多。在絕大多數的類別當中也是比起過去的做法還要強。\",\"這樣的進步有蠻多部分是來自於對較難分類的類別的提升。因為我們現在對每一個 classes 都是平等對待所導致。\",\"image\",\"Image from Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen (2021)\",\"上半部分是用 domain alignment 的解決方案，下面則是 self-training。\"]},\"202\":{\"h\":\"SYNTHIA → \\\\rightarrow → Cityscapes\",\"t\":[\"在 SYNTHIA → Cityscapes 的部分如下，同樣可以看到 mIoU 比起過去的 SOTA 有不少的提升。\",\"image\"]},\"203\":{\"h\":\"Contribution\",\"t\":[\"提出一個可以即時修正 psuedo label 的方法(prototypes)\",\"展示知識蒸餾(Knowledge Distillation)在 UDA 上同樣可以獲得更多改善\",\"提出一個新的 UDA for semantic segmentation 的 SOTA model ProDA\"]},\"204\":{\"h\":\"值得一看的文章們\",\"t\":[\"［論文筆記］Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation\",\"UDA 論文 HackMD 筆記\",\"剖析深度學習 (2)：你知道Cross Entropy和KL Divergence代表什麼意義嗎？談機器學習裡的資訊理論\",\"【读】领域自适应语义分割 - ProDA\",\"論文筆記 — Momentum Contrast for Unsupervised Visual Representation Learning(MOCO)\",\"Momentum Contrast for Unsupervised Visual Representation Learning\",\"Symmetric Cross Entropy\"]},\"205\":{\"c\":[\"Note\"]},\"206\":{\"c\":[\"Paper Read\",\"Domain Adaptation\",\"Computer Vision\",\"CVPR\"]},\"207\":{\"h\":\"Attention is all you need\"},\"208\":{\"h\":\"Basic Information\",\"t\":[\"NIPS 2017 (former NeuralPS)\",\"Ashish Vaswani, Noam Shazeer, Niki Parmar et al. from Google Brain and Google Research\"]},\"209\":{\"h\":\"問題描述\"},\"210\":{\"h\":\"RNN\",\"t\":[\"近年來自然語言處理(Natural Language Processing, NLP)與機器翻譯等任務上時常使用 Recurrent Neural Network(RNN), Long Short-Term Memory(LSTM), Gated Recurrent Neural Network 等模型架構，我們也看到使用 Recurrent 模型以及 Encoder-Decoder 架構蔚為流行。\",\"Recurrent Model 雖然強大，卻有兩個很大的缺點。\",\"由於每個 state ht​ 依賴於上一個 state ht−1​，使 RNN 平行度極差\",\"前面序列的資訊隨著長度越長會逐漸被稀釋\",\"雖然後續有一些研究試圖在 RNN 的基礎上去改善上述兩個缺點，但是這些問題仍然存在。\"]},\"211\":{\"h\":\"CNN\",\"t\":[\"也有一些相異於 RNN 的做法，使基於 Convolution Neural Network(CNN) 處理，如 Extended Neural GPU, ByteNet 以及 ConvS2S。透過 CNN 這種單純的矩陣運算可以提高運算的平行度，解決 RNN 平行度差的問題，不過也會使得 input sequence 當中相距較遠的元素，需要花費更多次運算得到彼此的關係。\"]},\"212\":{\"h\":\"Self-Attention\",\"t\":[\"Self-Attention 這個技術能夠將一個 sequence 中每個位置的元素依據與其他元素之間的關係，得出一個對應的 representation。這樣的機制在許多的任務當中都看到了不錯的結果。\",\"基於上述的問題以及過去的研究，作者提出了 Transformer 架構，單純依賴 Attention 機制，並且將 Encoder-Decoder 架構替換成 Multi-headed Self-Attention。除了能夠有高度的平行度，也能夠將過去的資訊好好地保留。在翻譯的任務上也打破 WMT 2014 English-to-German 與 WMT 2014 English-to-French 的紀錄成為新的 SOTA。\"]},\"213\":{\"h\":\"Related Works\",\"t\":[\"Recurrent Neural Network(RNN)\",\"Seq2Seq\",\"Attention\"]},\"214\":{\"h\":\"Recurrent Neural Network(RNN)\",\"t\":[\"Image from LeeMeng - 進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南\",\"像是文意理解或是語言翻譯這種任務，每個詞語的意義都會與前面的內容相關。RNN 就像是模擬了人在閱讀文章的狀態，由左至右地逐漸理解文意。\",\"RNN 會將每個序列中的每個元素 xt​ 依序放入一個網路中，除了得到一個對應的 Representation ht​ 以外，也會將這個 ht​ 會與 xt+1​ 作為下一個網路的輸入。如此一來就可以將前面的資訊傳遞下去。\",\"Image from LeeMeng - 進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南\",\"不過正因為這樣的架構設計，使得 RNN 的每個狀態 ht​ 都依賴於前一個狀態 ht−1​，難以透過平行化加速運算。同時，隨著 sequence 長度越長，前面的狀態在傳遞過程中也會不斷被稀釋。\"]},\"215\":{\"h\":\"Seq2Seq\",\"t\":[\"Seq2Seq 是 Sequence to Sequence 的意思，如同字面意義能夠將一個 sequence 轉換成另一個 sequence，常見在翻譯任務上。\",\"Image from Frederick Lee - Attention in Text：注意力機制\",\"Seq2Seq 將 RNN 分別製作成 Encoder 與 Decoder。Encoder 負責將原本的文字變成一個 context vector，交給 Decoder 產生出對應的 sequence。\",\"前面所提及的 Encoder-Decoder 架構就是指這個部分。\"]},\"216\":{\"h\":\"Attention\",\"t\":[\"在 Seq2Seq 當中我們會透過 Encoder 將 input sequence 壓成單一的 context vector，不過 RNN 在傳遞過程當中可能導致訊息的流失，且單一 vector 的表達能力也有所侷限。Attention 機制是其中一個解方。\",\"Attention 簡單來說，就是能夠告訴我們需要注意 sequence 當中的哪些元素。\",\"Tips\",\"這裡十分推薦可以去看看 3Blue1Brown 的解說影片，相信可以給你更多的啟發，這裡我會簡單地說明 Attention 的機制。\",\"對於每個 sequence 中的元素，Attention 會先將他們各自透過一個矩陣得到對應的 Query VectorQi​​ 以及 Key VectorKi​​。\",\"值得一提的是，除了元素本身之外，元素所在的位置(position)也會被納入考量產生 Qi​​,Ki​​。\",\"當 Query Vector Qi​​ 與 Key Vector Kj​​ 很像的時候，我們會說元素 i 需要多注意元素 j。\",\"Image from 3Blue1Brown - Attention in transformers, visually explained | Chapter 6, Deep Learning\",\"對於每個 Query 與 Key 都去計算他們的相似程度，我們就可以得到一個 Attention Matrix，告訴我們每個元素需要關注其他元素多少程度。\",\"Image from 3Blue1Brown - Attention in transformers, visually explained | Chapter 6, Deep Learning\",\"例如從上面可以知道 creature 需要多注意 fluffy 和 blue。\",\"不過這樣的數值範圍可以是 (−∞,∞)，我們其實更想知道的只是數值之間的大小差異，讓我們知道要著重於哪些部分，因此會再進一步加上 SoftMax function，讓他看起來就像是機率一樣，數值範圍介於 [0,1]。\",\"Image from 3Blue1Brown - Attention in transformers, visually explained | Chapter 6, Deep Learning\",\"Info\",\"計算相似度的方法有許多種，上面呈現的是使用 dot product 的版本。透過 dot product，你可以知道兩個向量在方向上的相似性。\",\"最後我們還有一個向量 vi​​，這個向量就像是在描述一個元素的特性。以下圖為例，creature 根據剛剛計算出來的結果，我們需要多注意 fluffy 以及 blue，也就是說，根據前後文，creature 實際上是指 fluffy blue creature。\",\"我們期待代表 creature 的向量 E4​​ 加上了一些些的 v2​​ (fluffy) 與一些些的 v3​​ (blue) 後，可以變成我們期待的 fluffy blue creature。\",\"Image from 3Blue1Brown - Attention in transformers, visually explained | Chapter 6, Deep Learning\",\"綜合上面的幾個操作，將許多的向量以矩陣改寫後，我們得到簡單的 Attention 數學描述。\",\"Attention(Q,K,V)=softmax(QKT)V\"]},\"217\":{\"h\":\"Methodology\",\"t\":[\"Image from Ashish Vaswani, Noam Shazeer, Niki Parmar et al. (2017)\",\"Transformer 的架構圖如上所示，其中左邊是 Encoder，右邊則是 Decoder。接下來我們就仔細看看 Transformer 每個部分的設計。\"]},\"218\":{\"h\":\"Scaled Dot-Product Attention\",\"t\":[\"原本的 Attention 如同上面的描述如下。\",\"Attention(Q,K,V)=softmax(QKT)V\",\"把 Dimention 也標記後如下圖所示。\",\"dk​: Sequence 大小\",\"n: Sequence 數量\",\"m: Key value 數量\",\"dv​: Output Dimension\",\"在語言模型當中 sequence 大小 dk​ 可能會很大。當兩個很大的向量座內積的時候可能會得到過大或是過小的數值。這會導致 Softmax 出來的結果可能極端地靠近 1 或是 0。\",\"當 Softmax 結果有些趨近於 1 有些趨近於 0 時，意味著我們認為我們已經成功地讓模型知道 Query 與 Key 之間的關聯，換言之，我們認為模型已經訓練得差不多了，也就使梯度收斂。\",\"因此，前面提到 dk​ 過大就可能使我們誤判現在已經訓練差不多，梯度就收斂了。\",\"為了避免這個情況，他們將原本的 Attention 除上 dk​​，得到了 Scaled 的版本。\",\"Attention(Q,K,V)=softmax(dk​​QKT​)V\"]},\"219\":{\"h\":\"Masking\",\"t\":[\"在 Attention 當中，我們會把每個詞跟前後文之間的關係都直接計算出來。不過實際上我們在輸出的過程當中，是不會知道未來才會輸出的內容的。\",\"具體來說，如果今天要將 Hello World 翻譯成 你好，世界，那麼在處理輸出 好 的當下，你是不會知道你現在要輸出 好，也不知道後面會輸出 ，世界 的。\",\"以數學來描述，也就是說在時間 t，你只能考慮時間 [0,t−1] 之間的資料。\",\"但 Attention 顯然會將未來的資料也考量進去。為了避免這個問題，我們會在 Attention 的計算中間加上 Masking，把時間 [t,n] 之間的部分都乘上一個很大的負數，就能使 softmax 的計算結果趨近於 0，而達到忽略未來的效果。\",\"Image from Ashish Vaswani, Noam Shazeer, Niki Parmar et al. (2017)\"]},\"220\":{\"h\":\"Multi-head Attention\",\"t\":[\"到目前為止我們看到的 Attention 都可以被稱為 Single-head Attention。仔細看 Attention 當中的 learnable parameters 就只有投影到 Q,K,V 的矩陣 WQ,WK,WV。或許在表達能力上有所不足。\",\"在 Attention 當中，一個單詞決定要去看哪些 sequence 中的元素，取決於對應到的投影矩陣 WQ,WK,WV。\",\"如果我們把 Single-head Attention 看作是一種解讀事物的視角，那麼有更多的的視角去理解一個事情，直覺上能帶來對事物更加深刻與全面的理解。\",\"換言之，如果我們有多組的 WQ,WK,WV，那麼不同 Head 能代表不同的表達，綜合起來可以達到更加全面的描述及理解。這就是 Multi-head Attention 想做到的事情。\",\"Image from Ashish Vaswani, Noam Shazeer, Niki Parmar et al. (2017)\",\"我們設定有 h 種不同的視角，將 V,K,Q 分別經過 h 個 Linear Layer，再接回前面的 Scaled Dot-Product Attention。最後再 Concat 再一起，經過一個 Linear Layer 作為最後的輸出，如上圖所示。\",\"於是我們的 Multi-head Attention 可以寫成底下的數學表達。\",\"MultiHead(Q,K,V)headi​​=Concat(head1​,…,headh​)WO=Attention(QWiQ​,KWiK​,VWiV​)​\",\"這裡的 Concat 就是單純把結果串接在一起而已。其中各個矩陣的大小如下\",\"WiQ​∈Rdm​×dk​\",\"WiK​∈Rdm​×dk​\",\"WiV​∈Rdm​×dv​\",\"WiO​∈Rhdv​×dm​\",\"也就是說，最後得到的結果會是一個大小為 Rdm​×hdv​×Rhdv​×dm​=Rdm​×dm​ 的結果。\",\"再 Transformer 當中設定的參數如下。\",\"h=8\",\"dk​=dv​=dm​/h=64\",\"dm​=512\",\"原本 Single-head 的大小是 dm​，現在在 Multi-head 我們希望產出相同大小的結果，因此會讓每個 head 的大小平分，變成了上面的 dm​/h。\",\"如此一來，在 Multi-head 的設定上每個 head 的大小會比 Single-head 小，而總共需要的計算量與 Single-head 會相似。\"]},\"221\":{\"h\":\"Encoder and Decoder Stacks\",\"t\":[\"接下來可以詳細看 Transformer 的架構了！\",\"Encoder\",\"Image from Ashish Vaswani, Noam Shazeer, Niki Parmar et al. (2017)\",\"在 Encoder 的部份，首先將 Input Embedding 加上 Position 的資訊。\",\"如同前面 Attention 的部份所提及。 除了元素本身之外，元素所在的位置(position)也會被納入考量產生 Qi​​,Ki​​。 這就是在這邊加上的。\",\"接下來可以看到連接了一個 Multi-Head Attention，而串接的 V,K,Q 都是同樣的輸入，也就是 Self-Attention。\",\"將輸入與 Attention 的結果相加，再經過 Layer Normalization，接著會經過 Feed Forward Layer，然後再次相加、經過 Layer Normalization。\",\"這樣的 block 在 Transformer 的 Encoder 當中會重複 N=6 次。\",\"Tips\",\"Multi-Head Attention 可以把 Query 跟 Key 去比較相似度。當 Query 與 Key 來源相同，也就意味著擷取自己跟大家的相似度有多少。\",\"可以簡單理解成 Encoder 會將 Input Sequence 的特徵擷取起來，變成一個 Embedding。\",\"Decoder\",\"Image from Ashish Vaswani, Noam Shazeer, Niki Parmar et al. (2017)\",\"Decoder 的部分與 Encoder 設計部分類似，不過在 Output Embedding 會先經過 Masked Multi-Head Attention。原因與前述相同，是為了把未來的資訊 Mask 掉，避免去考慮到後面的內容。\",\"接下來，Encoder 的輸出作為 Multi-Head Attention 的 Value 與 Key 輸入，而 Output 的部份作為 Query 輸入。後續部分與 Encoder 相同。\",\"這樣的 block 在 Transformer 的設計中會重複 N=6 次。\",\"Tips\",\"這意味著輸出的內容會根據 Input 擷取出來的 Embedding 來決定輸出特徵。\",\"最後經過 Linear Layer 與 SoftMax，就得到 Transformer 的最後輸出了！\"]},\"222\":{\"h\":\"Position-wise Feed-Forward Networks\",\"t\":[\"在 Encoder 與 Decoder 都有 FFN，會將 Attention 擷取特徵後的結果再做一些加工。實際上 FFN 的設計如下。\",\"FFN(x)=max(0,xW1​+b1​)W2​+b2​\",\"這裡的兩個矩陣大小分別是 W1​∈Rdm​×fff​,W2​∈Rdff​×dm​。其中 dm​=512,fff​=2048。\",\"也就是說，經過 FFN 之後他的大小仍然會跟原本是相同的。\"]},\"223\":{\"h\":\"Embeddings and Softmax\",\"t\":[\"無論是 Input 或是 Output 原本都是一個單純的詞語，會需要轉換成向量才能計算，這也就是所謂的 Embedding。\",\"論文當中並沒有特別說明 Embedding 的設計方式，不過有特別提到每一個需要 Embedding 的地方都會是用相同的權重，並且這個權重會依照現在選用的 dm​ 去乘上 dm​​ 的大小。\",\"可以理解成當 Embedding 的維度越大時，學出來的權重就可能越小，因此乘上一個 dm​​ 會讓他比較好學。\"]},\"224\":{\"h\":\"Positional Encoding\",\"t\":[\"如同前面我們提到，實際上每個元素都會再加上位置的資訊。這是因為原本的 Attention 無論現在的 sequence 順序為何，都不會影響到最後的結果。然而很直覺地，一個句子當中的內容如果調換，對於語句的理解也會有不同。\",\"因此在這裡我們會再加上位置的資訊，就可以避免這個問題。\",\"Info\",\"這也是 Attention 跟 RNN 不同的地方。RNN 會依序接收上一個時間點的訊息，因此本身就已經包含順序的特徵。但 Attention 需要我們自己告訴他。\",\"在 Transformer 當中使用的 Positional Encoding 如下。\",\"PE(pos,2i)​=sin(pos/100002i/dm​)PE(pos,2i+1)​=cos(pos/100002i/dm​)​\"]},\"225\":{\"h\":\"Results\"},\"226\":{\"h\":\"Why Self-Attention?\",\"t\":[\"為什麼我們要選擇 Transformer 這種 Self-Attention 的架構，而不要用 RNN 與 CNN 的做法呢？\",\"在計算的複雜度上比較一下。\",\"Image from Ashish Vaswani, Noam Shazeer, Niki Parmar et al. (2017)\",\"給定輸入序列數量 n，序列長度 d，比較 Self-Attention 與 RNN。\",\"首先在 Complexity per Layer 的部份一個是 O(n2⋅d)，一個是 O(n⋅d2)。在現在我們的序列數量基本上跟序列長度的數量級是差不多的狀況下，兩者沒有太大的差異。\",\"當然，實際上你還是可以依照狀況決定要哪一種架構，但 Sequential Operations 所需的時間仍然是 Attention 較佳。\",\"而 Sequential Operations 的部份由於 RNN 需要等待前面的輸入，因此會是 O(n)，而 Self-Attention 只需要 O(1)。\",\"最後的 Maximum Path Length 可以看成是資訊流失的程度。在 Attention 當中我們可以直接獲得訊息，不需要等待傳遞，也不需要擔心中間的流失。而 RNN 就會需要擔心了。\",\"Info\",\"CNN 的部份雖然我並沒有深入去理解，不過大致上他的做法是會經過幾個 kernel 運算，因此 Complexity 的部份會多上一個 k。\",\"而訊息流失則是看 kernel 需要跑過幾次，因此會是 O(logk​(n))。\",\"可以看到 Attention 確實能夠避免最初提及 RNN 並行度差的狀況，以及資訊流失的狀況。\"]},\"227\":{\"h\":\"實驗設定\",\"t\":[\"Datasets\",\"在實驗上使用的 Dataset 為 WMT 2014 English-German dataset。當中包含了 4.5 million sentence pairs。\",\"這些 sentence 會透過 byte-pair encoding 被 encode 成一個 token。他們的 token 總共有 37000 種。\",\"對於英文轉法文的部分使用的是 WMT 2014 English-French dataset。當中包含了 36 million sentence pairs。而 token 則有 32000 種。\",\"Hardware\",\"硬體上使用的是 8 張 NVIDIA P100 GPU。\",\"Base Model 大約訓練了 12 小時，而大的模型則訓練約 3.5 天。\",\"Optimizer\",\"使用了 Adam Optimizer，細節設定如下。\",\"β1​=0.9\",\"β2​=0.98\",\"ϵ=10−9\",\"warmup_steps=4000\",\"lrate=dm−0.5​⋅min(step_num−0.5,step_num⋅warmup_steps−1.5)\",\"Regularization\",\"首先是對每個 sub-layer 在經過 add&normalize 之前會經過 Dropout。Base Model 使用 Pdrop​=0.1。\",\"訓練期間他們採用了 Label Smoothing。輸出是會經過一個 Softmax 函數去決定每個 token 被輸出的機率。一般來說我們會期待正確的輸出要越接近 1 越好，但是這需要結果趨近於無限大才可能發生。\",\"據說一般會將目標改成越接近 0.9 越好，讓模型比較好學，不過 Transformer 這邊選擇只要接近 0.1 就好。\",\"雖然這樣的做法會讓模型的困惑度(perplexity)增加，但他們發現這樣會在最後得到更好的準確度與 BLEU score。\"]},\"228\":{\"h\":\"Model Variations 實驗結果\",\"t\":[\"Image from Ashish Vaswani, Noam Shazeer, Niki Parmar et al. (2017)\",\"BLEU 是一種評價機器翻譯品質的方法，數字是越大越好。而 PPL 是困惑度，也是評量一個語言模型優劣的方法，數字是越小越好。\"]},\"229\":{\"h\":\"English Constituency Parsing 實驗結果\",\"t\":[\"Image from Ashish Vaswani, Noam Shazeer, Niki Parmar et al. (2017)\",\"在同樣的 Training Data 下，Transformer 都可以得到更好的結果。\"]},\"230\":{\"h\":\"Contribution\",\"t\":[\"總結來說，Transformer 是一個跨時代的傑作，順利解決掉 RNN 的兩個重大缺點：極低的平行度以及資訊的流失。\",\"雖然 Transformer 起初只針對機器翻譯領域做研究，不過他的靈活性在當前許多的領域都可以看到 Transformer 的身影。包含了 LLM、Diffusion Model、Object Detection、Domain Adaptation 等，都可以看到替換成 Transformer base 後帶來的優勢。是一篇相當值得深讀的論文。\"]},\"231\":{\"h\":\"值得一看的文章們\",\"t\":[\"Mu Li - Transformer论文逐段精读\",\"LeeMeng - 進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南\",\"Frederick Lee - Attention in Text：注意力機制\",\"3Blue1Brown - Attention in transformers, visually explained | Chapter 6, Deep Learning\",\"Sharon Peng - BLEU評估方法\",\"CHEN TSU PEI - Perplexity（困惑度）是什麼？\"]},\"232\":{\"c\":[\"Note\"]},\"233\":{\"c\":[\"Paper Read\",\"NLP\",\"Computer Vision\",\"NeurIPS\"]},\"234\":{\"h\":\"筑波大學交換週記 Week 01\",\"t\":[\"感謝一路上教導我日語的阿普魯老師，陪伴我持續練習日文的冠霆、致越、陳曦、6uc，日文課程當中不厭其煩地協助我的川越老師、勇氣、小川、北之間、關口、雅之、鉦洋、冠文、阿郡助教們，大力推薦我的濬屹教授，支持我的家人們，以及我的摯愛心瑤\"]},\"235\":{\"h\":\"契機\",\"t\":[\"大一剛入學的我因為好奇而選了一門日文課，沒想到因此開啟了前往筑波大學交換的契機。\",\"當時遇到的阿普魯老師用他的熱情感染了每一個人。在學習日文以外，包含我在內的不少人也開始萌生想要用學習到的日語能力到日本交換的念頭。\",\"經過許多的努力，最終我在兩年內取得 N2，並且隨後取得了 N1 合格，也取得了筑波大學交換一學年的機會。\"]},\"236\":{\"h\":\"啟程\",\"t\":[\"在 2024 年的 9 月 18 日，在家人和朋友的目送下離開了熟悉的台灣，來到了陌生的日本。\",\"不過幸好也許是因為緣分，一起練習日文的致越也申請到了筑波的交換，讓我在來到這個陌生的地方時並不是獨自面對這些未知的環境。\",\"在起飛的前一晚確認過無數次行李，事情告一段落後就立馬休息，但還是翻來覆去睡不著。心裡不斷思考隔天將會面對的事件，心中模擬著跟海關要說些什麼、跟 Tutor 怎麼聊天、是不是東西都帶好了等等。\",\"雖然並不是第一次搭飛機，不過窗外的景色相當地美麗，忍不住就想拿出手機多拍幾張照片。特地挑了窗邊的座位真的太好了。\",\"在空中看到的可愛雲朵\",\"已經看得到日本了！\",\"抵達了機場後有些緊張，不斷在心中提醒自己要有在留卡和資格外活動證明，也不斷在猶豫到底要不要嘗試用日文溝通。\",\"首先我們在某個機器上掃描了指紋，也讓工作人員看了一下護照。試著用簡單的日文跟工作人員說話後得到了日文的回覆讓我感到很開心。\",\"接著到了海關，遞出護照、CoE 之後，對方用日文問了我有沒有打算要打工，也回覆了簡單的 はい 之後讓我填寫了文件，接著就順利取得了在留卡以及資格外活動證明的印章。也許過程比想像中簡單許多，也很開心能聽得懂對方的日文，還有順利地溝通。\",\"為了搭筑波大學的專車回去，我跟致越向櫃台詢問了相關的資訊，搭乘接駁車來到了成田機場的第二航廈。再次找到對應的櫃台後，得知下一班已經客滿，比預期晚了一些時間才順利抵達筑波大學。\",\"在第二航廈等待了一些時間\",\"抵達學校後就是一陣兵荒馬亂。原本我們以為要自己先去處理宿舍入住手續之後再跟 Tutor 會合，後來才發現這些流程還會需要 Tutor 一起跟著跑。興許是學校跟 Tutor 的溝通也出了問題，Tutor 本人也不知道其實他需要在，所以比較晚才和我們會合。\",\"填寫文件、取得鑰匙後，宿舍人員體醒我們接下來要去領寢具，但他只開放到晚上 6 點，要我們馬上過去排隊。\",\"領寢具的地方在一個被樹林包圍的角落，剛好那天下著雨，並且只有一位工作人員，隊伍排得長長的，前進的速度也相當緩慢。在潮濕、悶熱與勞累下排了不少的時間，終於順利領到了寢具。\",\"在來筑波之前還以為需要自己準備床墊、棉被、枕頭等等，沒想到居然有一個專門提供的單位，甚至還有完整的 schedule 告訴你每個月的哪幾天可以過去更換乾淨的寢具，這一點其實很貼心。\",\"宿舍雖然小小的，但基本上什麼都有。左邊還有個廁所和淋浴間沒有拍到。\",\"晚上 Tutor 帶我們去附近的松屋吃晚餐。還沒有腳踏車的我們只能步行緩緩前進。\",\"筑波基本上就是一個森林，也不知道為什麼在学園東大通り這種車流量大的路段還會有一整路沒有路燈，黑黑的一片如果是自己一個人也許會有些不安。\",\"在走了一段全黑的道路後，終於看到了有亮光的地方\"]},\"237\":{\"h\":\"生活安頓\",\"t\":[\"在剛進到學校會有許多的任務需要處理，像是需要去市役所辦理的転入届、国民健康保険還有国民年金，或像是校方的入住相關資料、宿舍繳費方式登記、腳踏車登記，還有基本生活所需的各種用品補齊等等。\",\"從房間陽台往外看，是在校內的兵太郎池\",\"為了買各種生活用品，我們在前一天列好了購買清單，Tutor 帶我們到学園都市附近的 Mall 和超市去找便宜實惠的東西。\",\"之後也許會開始嘗試自己料理，所以買了一些碗盤、菜瓜布和保鮮盒。也買了垃圾袋、掛勾、毛絮黏把、盥洗用品等等，總之就是一頓大買特買。\",\"我在來筑波之前有跟學長買好了腳踏車，不過致越目前還沒有，所以就一起去了腳踏車店找二手腳踏車。\",\"老闆相當平易近人，很熱情地邀請致越去試試各種腳踏車，也稍微聊了一下。\",\"走了一堆路，腳快要斷了。不知道為什麼筑波大學附近常常看到這種青蛙雕像，Tutor 似乎也不知道為什麼。\",\"跟學長買的腳踏車，有 7 檔變速器的公路車，但椅子坐久了著實不大舒服w\",\"晚餐的清六家拉麵我對最重要的拉麵沒什麼感想，反倒對有氣泡水的可爾必思念念不忘\",\"話說，日本喝水道水的習慣一開始真的不大適應，認真考慮很久要不要買小飲水機來燒水。直到我遇到了一日分の野菜，那味道讓我無法接受，以至於要多喝水消除那個味道。突然覺得水道水很好喝，於是就接受了喝水道水。\",\"如果你不能接受喝水道水，也許你只是缺一罐一日分の野菜(X)\"]},\"238\":{\"h\":\"持續安頓生活和探索\",\"t\":[\"校內的 Orientation 領到了更多的文件要閱讀，也終於領到了學生證。雖然有許多手續要接著做，不過就持續推進進度就沒有問題了！\",\"學生證上寫的情報科学類讓不少人以為我是來交換學做 CIA 的(X) 只能太有趣\",\"筑波大學的吉祥物 コズミくん\",\"陸陸續續處理好了市役所的各個手續，也持續購買各種生活物品，慢慢適應新的生活環境。\",\"不過在各種購物以外，接下來對於校園還有附近環境的熟悉也許也是頗重要。現在有了腳踏車也終於能夠比較輕鬆地前往各種地方。\",\"校園一隅\",\"之前在清大的時候偶爾會自己騎著機車隨意的兜風，看看在前進之後能到怎樣的地方看到哪些有趣的風景。現在換成了腳踏車，也許距離稍稍受限，不過與機車不同，有一種踏實感。偶爾也會迷路，但只要還不是晚上我想應該都還行。夜晚的筑波是真的有點可怕w\",\"迷路走到某個鄉間小路，但其實這條是人行道。其中幾段植物過於茂密，前進有些不舒服。\",\"到二手用品店找便宜電器。意外遇到美麗的餘暉。\",\"去宿舍中心交資料，熱情的阿姨給了他自己手做的葡萄XD\"]},\"239\":{\"h\":\"在不安與焦躁當中的放鬆\",\"t\":[\"大四這個時間點交換有好多事情會打在一起。到了新學校的環境適應、手續處理是本來就需要做的，不過大四學生的我又再加上專題、研究所推甄。為了研究所推甄也是煞費苦心。\",\"詳細地跟學校確認能不能準時畢業、要件又是什麼，同時也要確認想要選哪些課程、能不能被認抵。一方面又因為大學成績普通以至於需要考慮更多校系，卻又擔心花太多錢在報名費跟回台灣面試的來回機票之類的成本過高。\",\"冠霆也申請到了東工大的交換，趁我們都還沒開學之前空出了幾天出去筑波和秋葉原晃晃。雖然去了不少地方，不過想到還有沒順利推進的專題進度、尚未完成的推甄資料、還在確認的畢業條件、接下來要選擇的課程等等，很難真的放鬆下來享受這趟旅程。\",\"再次來到成田機場，準備接機\",\"在中央公園裡面的火箭，經過他很多次，終於拍到全貌\",\"覺得很好吃的力うどん\",\"某個遊戲場內的交流ノート\",\"JAXA 裡面的火箭。還順道買了紀念品和太空食物。\",\"つくば駅附近的 Mall\",\"秋葉原的某一條河流\"]},\"240\":{\"h\":\"後記\",\"t\":[\"接下來空閒的時間需要先好好準備推甄的申請，學校這邊的手許也終於快告一個段落了。\",\"在一個人走一片漆黑的路回宿舍真的有些嚇人，但似乎又勇敢了點。\",\"日本人好像沒有像過去印象中總是視客人為上帝的感覺了。在抵達日本的第一天就遇到直接在面前嘖嘴嘆氣的工作人員著實嚇到我了。\",\"日本的交通違規還是有，也不是稀有到一個禮拜都遇不到。不過頻率真的比台灣低太多太多了。尤其對於行人、自行車的停讓看起來已經是一種習慣了。不過腳踏車騎在人行道上的時候到底要往哪邊讓對向的行人/腳踏車真的還是抓不準，原以為都維持靠左邊就無敵了，但似乎不是這麼一回事。\",\"普遍遇到的日本人都很友善，能用日文溝通真的很開心。\",\"辦電信的時候店員覺得我們不太會講日文，找了會講英文的工作人員來幫忙，然後突然發現我講不出英文，腦袋都是日文跟中文了w\",\"不過後續其實是在線上辦 Rokuten 的 eSIM\",\"希望接下來可以有更多有趣的事件跟個人的成長。\"]},\"241\":{\"c\":[\"Diary\"]},\"242\":{\"c\":[\"Diary\",\"University of Tsukuba\",\"Exchange Program\"]},\"243\":{\"h\":\"Agent Planning with World Knowledge Model\"},\"244\":{\"h\":\"Basic Information\",\"t\":[\"2024/05/13 發布 (尚未正式於 Conf. 發表)\",\"Shuofei Qiao, Runnan Fang, Ningyu Zhang et al. @ Zhejiang University, National University of Singapore, Alibaba Group\"]},\"245\":{\"h\":\"問題描述\",\"t\":[\"近年來大型語言模型(LLM)在許多自然語言處理的問題有很快速的成長，而近期開始出現一些使用 LLM 作為 agent model 來處理物理環境中的規劃問題。然而由於當前 SOTA 的 LLM 幾乎都是 autoregressive model，模型實際上會做的事情是去預測下一個 output token 要是什麼，實際上他們對於物理環境是沒有任何理解的。\",\"這導致過去使用 LLM 作為 agent model 時常出現兩個問題。\",\"時常會產生怪異的行為\",\"出現一些無腦的 trial-and-error\",\"如果跟人類的決策方式去比較，中間的差異在於人類會具有跟真實世界相關的知識，因此會在心中先預想解決問題的方式。而實際在執行的過程當中，人類也會依據現有的資訊持續解決當前任務。\",\"我們稱呼這個事先預想的知識為global task knowledge。 而在執行 task 過程中的現有資訊則被稱為local state knowledge。\",\"Info\",\"舉一個簡單的例子來說明。\",\"想像你現在在廚房當中，你的目標是要把一個乾淨的雞蛋放進微波爐當中。\",\"global task knowledge 首先，雞蛋很可能出現在冰箱裡面，而要洗乾淨最常去洗手檯。這些預先知道的知識讓你決定接下來解決的步驟大概會像\",\"去找到冰箱，並找到雞蛋\",\"去洗手檯把雞蛋洗乾淨\",\"拿到微波爐裡\",\"local state knowledge 當規劃出整體的步驟後，接著就要去實踐。實踐的過程當中我們會需要理解當前所處的狀態。我要完成什麼步驟、完成的狀況如何、下一步該做什麼。\",\"例如現在你要完成第一個步驟：找到冰箱、並找到雞蛋。那麼你會需要知道\",\"我現在找到冰箱了嗎\",\"如果找到冰箱了，那我找到雞蛋了嗎\",\"諸如此類的問題\",\"透過 global task knowledge 與 local state knowledge，我們可以事先理解許多事情，避免執行怪異的行為以及無腦的嘗試。像是去烤箱裡面找雞蛋，或是嘗試在書房完成整個任務等等。\",\"因此作者依照上面的想法，提出了 World Knowledge Model(WKM) 去提供 agent global task knowledge 與 local state knowledge。\",\"透過與當前 SOTA LLM model 如 Mistral-7B, Gemma-7B, Llama-3-8B 結合，在 ALFWorld, WebShop, ScienceWorld 獲得了極佳的成果。\"]},\"246\":{\"h\":\"Related Works\"},\"247\":{\"h\":\"Knowledge Augumented Agent Planning\",\"t\":[\"在過去也有一些透過 LLM 輔助 agent 做決策的研究。底下以發表於 ICML 2022 的 Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents 為例來說明。\",\"他們讓 LLM 先輸出一個步驟，接下來為了要讓輸出的動作變成環境能夠接收的詞句，會再去找到所有 agent 能執行的步驟當中語意最接近的 action 輸出。\",\"Image from Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\",\"底下是 Browse Internet 任務的結果。\",\"Image from Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\",\"雖然在部分的例子當中可以看到這種相關的做法還不錯，但他們往往需要手動設定 prompt template 以及 task procedures。因此這類的結果很難直接應用到其他的環境當中。\",\"其他的研究方向則是單純考慮到整體的工作流程，或是只考慮到局部的 action，並沒有兩方面都顧慮到。作者提出的 WKM 則有辦法解決上述的三個痛點。\"]},\"248\":{\"h\":\"Methodology\"},\"249\":{\"h\":\"Preliminaries\",\"t\":[\"他們將整個環境以 Partially Observable Markov Decision Process(POMDP): (U,S,A,O,T) 來描述。\",\"U 表示 insturction space，定義了有哪些 task 以及各自的規範\",\"S 表示 state space\",\"A 表示 action space\",\"O 表示 observation space\",\"T:S×A→S 表示 transition function\",\"此外，我們也定義 historical trajectory ht​ 包含了這次目標的 task u∈U、執行的 actions a∈A 以及接收到的 observations o∈O。\",\"ht​=(u,a0​,o0​,a1​,o1​,…,at​,ot​)\",\"我們的 policy model πθ​ 在這邊是以 LLM 來扮演，也就是說 LLM 會去決定下一個 action。這裡的 θ 描述 LLM 的參數。\",\"at+1​∼πθ​(⋅∣ht​)\",\"不過其中比較特別的是最初的 action a0​ 是根據 task 去決定的。\",\"a0​∼πθ​(⋅∣u)\",\"一個 trajectory τ 結束的時機分成 已經完成 task 以及 超過執行時間上限 兩個狀況。\",\"那麼一個 task 會執行 trajectory τ 的機率也就可以描述成\",\"π(τ∣u)=t=0∏n​πθ​(at+1​∣ht​)πθ​(a0​∣u)\",\"最終，一個 trajectory 得到的 reward r(i,τ) 會是一個介於 [0,1] 之間的數值，用來描述 task u 完成率有多少。\",\"作者提出的 WKM 當中的 world 意指模擬 task 所使用的環境。而 world knowledge 則包含了 真實世界的知識(prior global knowledge) 以及 模擬環境的知識(dynamic local knowledge)。\"]},\"250\":{\"h\":\"整體流程\",\"t\":[\"Image from Shuofei Qiao, Runnan Fang, Ningyu Zhang et al. (2024)\"]},\"251\":{\"h\":\"Task Knowledge Synthesis\",\"t\":[\"這個步驟的目標是產出 global task knowledge。藉此讓 agent 可以學習規劃整體的解決步驟，也避免不必要的 trial-and-error。這裡分成了兩個步驟。\",\"Experienced Agent Exploration\",\"要訓練模型產出 global task knowledge，我們會需要知道正確的步驟會是什麼，也就是知道 Expert Trajectory。\",\"不過作者認為如果我們只看 Expert Trajectory，或許會使模型缺乏一些比較高層次的認知與知識。我們雖然知道跟隨一個步驟可以完成任務，但卻不知道每個步驟安排的意義是什麼。\",\"因此作者希望產出另一組 Trajectory，讓模型可以參考兩個做法去推論。不過如果這個產出的 Trajectory 太差，那也沒有太大的參考價值。\",\"因此作者首先透過 Expert Trajectory 訓練了一個 Experienced Agent。接下來再讓這個 agent 去產出 Rejected Trajectory。也就是說，我們會預設產出來的 Trajectory 比起 Expert Trajectory 是包含一些需要避免的操作。\",\"Self Knowledge Synthesis\",\"有了 Expert Trajectory 以及 Rejected Trajectory，接下來就可以把它們作為作為參考資訊，去產出 global task knowledgeκ。\",\"κ∼πθ​(⋅∣ρTaskKnow​,u,τw​,τl​)\",\"κ∈K 表示 task knowledge\",\"ρTaskKnow​ 是讓 LLM model 產出 task knowledge 的 prompt\",\"u 表示 task\",\"τw​ 表示 Expert Trajectory\",\"τl​ 表示 Rejected Trajectory\"]},\"252\":{\"h\":\"State Knowledge Summarization\",\"t\":[\"這個步驟的目標是要產出 local state knowledge。藉此讓 agent 可以釐清當前的狀況，並給出接下來目標方向。\",\"做法是製作 prompt 讓模型去產出當前步驟的總結。為了確保最終輸出結果的品質，這裡會只考慮 Expert Trajectory。\",\"假如我們在這裡也考慮了 Rejected Trajectory，那麼也許會出現前後動作缺乏連貫性，或是與最終目標不符等問題。這會導致無法好好判斷當前的狀態。\",\"st​∼πθ​(⋅∣ρStateKnow​,ht​)\",\"st​∈S 表示 state knowledge，是 state space 的一部分\",\"ρStateKnow​ 表示讓模型產出總結的 prompt\",\"ht​ 表示 historical trajectory\",\"在 inference 階段，直覺上直接把產出的 state knowledge 接在 agent 的輸入當中就可以作為參考輸出下一個 action。不過作者發現過多的資訊反倒會導致模型會感到困惑。\",\"Tips\",\"在 inference 階段，我們在意的是現在執行 action apre​ 並總結出 state knowledge s 後，接下來我該做什麼。\",\"Summarize 後的 state knowledge s 沒必要再丟給 agent，我們只要知道 (s,apre​) 會對應哪個 anext​ 即可。\",\"因此作者製作了一個 state knowledge base B 去紀錄 (s,apre​,anext​)。\",\"B={(s,apre​,anext​)(i)}i=1∣B∣​\",\"s 表示 state knowledge\",\"apre​=at​\",\"anext​=at+1​\",\"∣B∣ 表示 state knowledge base 的大小\",\"如此一來，我們就可以更好地運用 state knowledge 了！\"]},\"253\":{\"h\":\"Model Training\",\"t\":[\"我們總共會有兩個模型需要訓練，分別是用來決策的 Agent Model 以及用來提供 global task knowledge 與 local state knowledge 的 World Knowledge Model。接著會詳細說明兩個模型的訓練。\",\"不過值得一提的是，作者在訓練兩個模型都是用相同的 Backbone，並且都是使用 LoRA 而非 Full Training。\",\"Agent Model Training\",\"給定 Expert Trajectories Dataset D\",\"D={(u,κ,τw​)(i)}i=1∣D∣​\",\"u 表示 task\",\"κ 表示前面產出的 global task knowledge\",\"τw​ 表示 Expert Trajectory\",\"我們可以定義 Loss Lagent​(πθ​) 如下\",\"Lagent​(πθ​)=−Eτw​∼D​[πθ​(τw​∣u,κ)]\",\"Tips\",\"也就是說，我們期待根據 task u 與 global task knowledge κ 去決策出來的結果，要越接近目標的 Expert Trajectory τw​ 越好。\",\"每個 Expert Trajectory 實質上都是一連串的 token，描述 Agent 的行為、觀察到的結果、得到的 Reward。令 X=(x1​,x2​.…,x∣X∣​) 表示 τw​ 的 token sequence。那麼我們定義 πθ​(τw​∣u,κ) 如下。\",\"πθ​(τw​∣u,κ)=−j=1∑∣X∣​(1(xj​∈A)×logπθ​(xj​∣u,κ,x<j​))\",\"1(xj​∈A) 會把跟 action 無關的 token 篩除\",\"Tips\",\"也就是說，如果 token xj​ 是合法的 token，就去看現在他被輸出的機率是多少。\",\"而整體而言，πθ​(τw​∣u,κ) 就在描述在當前的 task u 與 global task knowledge κ 底下，得到輸出 τw​ 的機率有多高。\",\"World Knowledge Model Training\",\"與 Agent Model 不一樣的地方是，World Knowledge Model 還需要考慮 local state knowledge。\",\"因此定義新的 Dataset D′ 如下\",\"D′={(u,κ,τw′​)(i)}i=1∣D′∣​\",\"τw′​=(a0​,o0​,s0​,…,an​,on​,sn​)\",\"也就是多考慮了 local state knowledge s\",\"那麼定義 World Knowledge Model πϕ​ 對應的 Loss Lknow​\",\"Lknow​=−Eκ,τw′​∼D′​[πϕ​(κ∣u)πϕ​(τw′​∣u,κ)]\",\"與前面相同，Trajectories 實際上都是 token 組成。\",\"令 X′=(x1′​,x2′​,…,x∣X′∣​) 表示 τw′​ 的 token sequence\",\"令 Y=(y1​,y2​,…,y∣Y∣​) 表示 global task knowledge κ 的 token sequence\",\"我們可以定義πϕ​(κ∣u) 與 πϕ​(τw′​∣u,κ) 如下。\",\"πϕ​(κ∣u)πϕ​(τw′​∣u,κ)​=−i=1∑∣Y∣​logπϕ​(yi​∣u,y<i​)=−j=1∑∣X′∣​(1(xj′​∈S)×logπϕ​(xj′​∣u,κ,x<j′​))​\",\"1(xj′​∈S) 會把跟 state 無關的 token 篩除\",\"Tips\",\"也就是說，我們期待給定 task u 時產出 global task knowledge κ 的機率要越大越好。同時也要讓這個狀況下產出的 trajectory 跟目標 τw′​ 越像越好。\"]},\"254\":{\"h\":\"Agent Planning with World Knowledge Model\",\"t\":[\"在 Inference 階段，agent 會搭配 World Knowledge Model(WKM) 去做決策。整體流程可以簡單描述成 7 個步驟。\",\"給定目標 task u\",\"World Knowledge Model(WKM) 產出 global task knowledge κ\",\"World Knowledge Model(WKM) 產出 local state knowledge st​\",\"根據 st​ 從 state knowledge base B 決定出 next action 的機率分布\",\"Agent 也產出 next action 的機率分布\",\"考慮兩者的機率分布給出最終的 next action\",\"重複步驟 3~7 直到結束\",\"Step 1, 2：給定 u, WKM 產出 κ\",\"給定一個 task u，WKM 首先會產出 global task knowledge κ。\",\"κ∼πϕ​(⋅∣u)\",\"Step 3：WKM 產出 st​\",\"接下來 agent 就會開始規劃。令 task u 當中所有合法的 action Au​⊆A 為 (αu(1)​,αu(2)​,…,αu(∣Au​∣)​)。在時間 t≥0 時考慮下一個 action 之前我們需要去考慮 local state knowledge st​。\",\"st​ht​​∼πϕ​(⋅∣ht​)=(u,κ,a0​,o0​,a1​,o1​,…,at​,ot​)​\",\"注意，這邊的 ht​ 定義有加上了 global task knowledge κ。\",\"Step 4：根據 st​ 從 B 決定出第一個 next action 機率分布\",\"當取得 local state knowledge st​ 後，如同前面 State Knowledge Summarization 當中所提及，我們會去 state knowledge base B 當中尋找下一個 action anext​。\",\"這裡的作法是以 st​ 作為 key，找到 apre​=at​ 當中語意最接近的前 N 個 next actions。依據這 N 個 next actions 去決定最終的機率分布。如果一個 next action 被決定的次數越高，我們就給他更高的機率。\",\"Pknow​(Au​)pknow​(αu(i)​)​=(pknow​(αu(1)​),pknow​(αu(2)​),…,pknow​(αu(∣Au​∣)​)),i=1∑∣Au​∣​pknow​(αu(i)​)=1=NNi​​​\",\"Ni​ 表示 action i 被 sample 的次數\",\"pknow​(αu(i)​) 表示 action i 被 sample 到的機率\",\"Pknow​(Au​) 表示 task u 下，選擇 next action 的機率分布\",\"Step 5：Agent 決定出第二個 next action 機率分布\",\"Agent 訓練過程中不會考慮到 local state knowledge，給出 next action 的機率分布。底下列出的數學式只是在說明模型給出的機率分布總和是 1。方法是經過一個 SoftMax。\",\"Pmodel​(Au​)=(pagent​(αu(1)​),pagent​(αu(2)​),…,pagent​(αu(∣Au​∣)​)),i=1∑∣Au​∣​pagent​(αu(i)​)=1\",\"Step 6：考慮兩個機率分布決定最終 next action\",\"最後，我們會把兩者都考慮進來，並選擇其中機率最高的作為最後的 next action。\",\"at+1​=αu(i)​∈Au​,1≤i≤∣Au​∣argmax​(γ⋅pagent​(αu(i)​)+(1−γ)⋅pknow​(αu(i)​))\",\"γ 是一個調整兩者重要性的 hyperparameter\"]},\"255\":{\"h\":\"Results\"},\"256\":{\"h\":\"實驗設定\"},\"257\":{\"h\":\"Dataset 與環境\",\"t\":[\"實驗做在三個模擬真實世界環境的 datasets 上，分別是 ALFWorld, WebShop 以及 ScienceWorld。\",\"ALFWorld\",\"一個在房子當中的環境，大致的目標是要在房子當中移動，並且與各式物件互動。\",\"Image from ALFWorld: Aligning Text and Embodied Environments for Interactive Learning\",\"以上面的圖片為例，prompt 內容如下。\",\"開啟 prompt 內容\",\"You are in the middle of a room. Looking quickly around you, you see a drawer 2, a shelf 5, a drawer 1, a shelf 4, a sidetable 1, a drawer 5, a shelf 6, a shelf 1, a shelf 9, a cabinet 2, a sofa 1, a cabinet 1, a shelf 3, a cabinet 3, a drawer 3, a shelf 11, a shelf 2, a shelf 10, a dresser 1, a shelf 12, a garbagecan 1, a armchair 1, a cabinet 4, a shelf 7, a shelf 8, a safe 1, and a drawer 4. Your task is to: put some vase in safe. > go to shelf 6 You arrive at loc 4. On the shelf 6, you see a vase 2. > take vase 2 from shelf 6 You pick up the vase 2 from the shelf 6. > go to safe 1 You arrive at loc 3. The safe 1 is closed. > open safe 1 You open the safe 1. The safe 1 is open. In it, you see a keychain 3. > put vase 2 in/on safe 1 You won! \",\"ALFWorld 當中包含了 unseen tasks，可以用來測試模型的一般性。此外，他的 Reward 設定上只包含 0 和 1，表示是否完成 task。\",\"WebShop\",\"一個模擬在網購的網頁虛擬環境。任務大致是要在網頁當中購買特定的物品，而 agent 需要透過查詢、點擊按鈕等操作去達成任務。\",\"Image from WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents\",\"Reward 設定上會是 [0,1] 之間的數值，表示任務完成率。\",\"ScienceWorld\",\"一個 2D 的模擬環境，包含了許多的空間。目標是要在這個空間當中達成指定的任務。例如要燒開水、找到動物、種植水果、觀察物體熔點等等。\",\"Image from ScienceWorld: Is your Agent Smarter than a 5th Grader?\",\"ScienceWorld 當中包含了 unseen tasks，用來測試模型的一般性。而在 Reward 的設定上會是 [0,1] 之間的數字表示完成率。\"]},\"258\":{\"h\":\"模型架構與 Baseline\",\"t\":[\"LLM 採用的模型有三種，分別是 Mistral-7B-Instruct-v0.2、Gemma-1.1-7B-it 以及 Llama-3-8B-Instruct。\",\"Baseline 比較上分成三個部分，分別是 prompt-based、with rejected-trajectories 以及 knowledge-augumented planning method。\",\"prompt-based\",\"這個部分的做法是透過 prompt engineering 去提升 LLM 輸出結果的品質，包含了 REACT 與 Reflection。\",\"Info\",\"REACT 是將 Chain-of-Thought(CoT) 引入 prompt 當中，讓 agent 在規劃的過程當中會歷經 思考(Thought)、行動(Action)、觀察(Observation) 三個步驟。此篇 paper 為了比較的公平性採用 one-shot prompting。\",\"Reflection 則是會讓 agent 規劃後當中根據過去的經驗給予 feedback，並且依據 feedback 重新規劃出更好的方案。此篇 paper 為了比較的公平性採用 one-shot prompting。\",\"由於這部分是針對 prompt 去做設計，因此也延伸出如 REACT-style prompt format。此篇 paper 採用的設計也幾乎基於這個。\",\"with rejected-trajectories\",\"訓練中包含 rejected trajectories 並不是這篇論文的首創，其他如 NAT 以及 ETO 也都有這樣的設計。\",\"Info\",\"NAT 是在 fine-tune 的過程當中引入不同的 prompt 設計，使模型產出 rejected-trajectories。此篇 paper 當中使用 LoRA 方式去 fine-tune。\",\"ETO 會在訓練過程中先訓練在 expert trajectory，接下來把訓練中失敗的 trajectories 當作 rejected-trajectories 進一步訓練。訓練方式採用 DPO。\",\"knowledge-augumented planning method\",\"這種類型的模型也同樣會在模型決策時提供其他的知識。這裡選擇了 KnowAgent。\",\"Info\",\"KnowAgent 在決策的過程當中會先將 action 的知識提取出來，包含 action 的定義與使用方式等，接下來構建出 planning path。執行的過程當中會透過 prompt 等方式指引 agent 持續待在正確的 planning path 上。\"]},\"259\":{\"h\":\"Training and Inference Setups\",\"t\":[\"訓練當中使用 LlamaFactory 去訓練模型，都是採用 LoRA。訓練使用了 8 張 NVIDIA V100 32G GPUs 訓練 12 小時。更詳細的參數設定請參考論文。\",\"部分參數細節\",\"Learning Rate: 1e-4\",\"Sequence Length: 2048\",\"Training Epoch: 3\",\"Batch Size: 32\",\"Optimizer: AnamW with consine learning scheduler\",\"N: 3000\",\"γ={0.4,0.5,0.7}\"]},\"260\":{\"h\":\"實驗結果\",\"t\":[\"Image from Shuofei Qiao, Runnan Fang, Ningyu Zhang et al. (2024)\",\"首先可以看到無論採用的 Backbone 為何，WKM 都可以得到更出色的結果。這樣的結果無論是在哪個 dataset 當中，或是有沒有看過的 task 都是更好的。\",\"此外，除了 GPT4 以外，普遍而言單純用 prompt-based 方法都會得到與其他方法相距甚遠的結果。\",\"同樣是給予額外知識的 KnowAgent，可以觀察到 KnowAgent 面對沒看過的 task 上得到的結果比起看過的 task 都會相差蠻多。從這裡可以看出 WKM 是具有足夠的一般性。\",\"Image from Shuofei Qiao, Runnan Fang, Ningyu Zhang et al. (2024)\",\"接著是 Ablation Study，比較 Global Task Knowledge 與 Local State Knowledge 分別帶來的影響。這裡以 Mistral-7B 作為 Backbone 實驗。\",\"可以觀察到無論是加上 state knowledge 或是 task knowledge 都可以對結果帶來好的影響，而兩者結合也可以帶來更好的結果。其中又以 task knowledge 帶來的正面效益為最大。無論在哪個 dataset 當中都可以觀察到相同的趨勢。\",\"如果更進一步觀察可以發現到 state knowledge 在 seen data 上面會有更多的正面影響。而 unseen data 則比較沒有顯著的差異。作者認為這樣的結果是因為 state knowledge 是從 training 階段製作的 knowledge base 中擷取的，這也許導致了 state knowledge 減少了一般性。\",\"Image from Shuofei Qiao, Runnan Fang, Ningyu Zhang et al. (2024)\",\"接下來作者想確認 WKM 是否真的能避免無謂的 trial-and-error，因此計算了每個方法在各個環境當中執行的 step 數量。其中 ALFWorld 的 step 上限為 40，WebShop 為 10，而 ScienceWorld 根據 task 不同有不同上限，但平均落在 40 上下。\",\"可以看到無論在哪個 dataset 底下，無論是不是有看過的 task，WKM 都可以用更少的平均 steps 達成。可以認為 WKM 確實可以避免無謂的 trail-and-error。\",\"Image from Shuofei Qiao, Runnan Fang, Ningyu Zhang et al. (2024)\",\"而由於 ALFWorld 可以接收錯誤的 action，因此這裡也統計了一個 trajectory 平均出現怪異 action 的頻率。\",\"可以發現到無論是哪個環境當中 WKM 都有更低的 Hallucinatory Action Rates。尤其在 Unseen task 當中可以降低許多。可以推斷 WKM 可以降低模型產生怪異 action 的發生機率，並且可以很好適應到 unseen data 上。\",\"Image from Shuofei Qiao, Runnan Fang, Ningyu Zhang et al. (2024)\",\"在前面的實驗當中，agent 跟 world knowledge model 都是使用相同的 Backbone，作者嘗試使用不同 Backbone 去觀察。這裡固定 knowledge model 使用 Mistral-7B，得到的結果如上。\",\"可以觀察到當 agent model 比 knowledge model 弱時，結果會很差。但是反過來當 agent 比起 knowledge model 還強時，得到的結果有大幅度的成長。\",\"作者猜想這是因為比較弱的模型包含了強的模型當中缺乏的資訊所帶來的結果。\",\"Image from Shuofei Qiao, Runnan Fang, Ningyu Zhang et al. (2024)\",\"接著，作者嘗試混合各個 datasets 的資訊，然後只訓練一個 WKM，搭配各個 agent model。作者發現到這樣的作法除了能大幅超越其他的方法外，甚至可以比沒有混合的 WKM 還要更亮眼。\",\"這個結果展現了單一模型可以帶來相當強大的廣泛性。現在我們的實驗只做在單一的 WKM 上，作者也大膽預測，如果連 agent model 也是單一的，可以帶來更強大的效益，這也許會是前往 AGI 的關鍵。不過此篇 paper 中並沒有呈現這樣的結果。\",\"Image from Shuofei Qiao, Runnan Fang, Ningyu Zhang et al. (2024)\",\"最後，在前面我們設計 local state knowledge 時決定製作一個 knowledge base 而非直接加入 prompt 的理由是因為會使模型感到困惑。這裡做了相關的實驗。\",\"如果我們直接把 state knowledge 加入 prompt 當中，也就是圖中的 Explicit State，可以發現到結果甚至會比起沒有 local state knowledge 更差。也因此確認了上面的結論。以較隱晦的方式傳遞 local state knowledge 會是更好的做法。\"]},\"261\":{\"h\":\"Contribution\",\"t\":[\"提出 WKM，結合了 LLM 與 agent model planning\",\"降低無腦 trial-and-error 的發生\",\"避免錯誤的 action\",\"面對 unseen task 有更好的一般性\",\"發現弱模型可以引導強模型有更好的效益\",\"發現混合 dataset 製作的 WKM 模型搭配其他 agent 可以達到更加效益\",\"不過同時也存在幾個 limitation。\",\"他們仍然無法說明 LLM 實際上學到了怎樣的 world knowledge\",\"目前只能在文字訊息上處理，還未能做到 multi-modal\",\"WKM 無法及時針對環境與 agent 給出的 feedback 做調整 \",\"local state knowledge 仰賴 training 階段的知識\",\"global task knowledge 在 inference 階段也是固定的\",\"多了 WKM 或產生額外的 overhead，大約會比沒使用 WKM 的方法多 2.5 倍的時間花費\"]},\"262\":{\"c\":[\"Note\"]},\"263\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"LLM\"]},\"264\":{\"h\":\"Posts\"}},\"dirtCount\":0,\"index\":[[\"仰賴\",{\"1\":{\"261\":1}}],[\"製作的\",{\"1\":{\"261\":1}}],[\"面對\",{\"1\":{\"261\":1}}],[\"面對沒看過的\",{\"1\":{\"260\":1}}],[\"弱時\",{\"1\":{\"260\":1}}],[\"弱增強只是給原圖\",{\"1\":{\"196\":1}}],[\"達成\",{\"1\":{\"260\":1}}],[\"達到鼓勵\",{\"1\":{\"151\":1}}],[\"持續待在正確的\",{\"1\":{\"258\":1}}],[\"持續安頓生活和探索\",{\"0\":{\"238\":1}}],[\"規劃後當中根據過去的經驗給予\",{\"1\":{\"258\":1}}],[\"此篇\",{\"1\":{\"258\":4}}],[\"此外\",{\"1\":{\"7\":1,\"10\":1,\"26\":1,\"34\":1,\"35\":1,\"37\":1,\"73\":2,\"74\":1,\"83\":2,\"140\":1,\"163\":1,\"173\":1,\"249\":1,\"257\":1,\"260\":1}}],[\"三個步驟\",{\"1\":{\"258\":1}}],[\"思考\",{\"1\":{\"258\":1}}],[\"引入\",{\"1\":{\"258\":1}}],[\"觀察\",{\"1\":{\"258\":1}}],[\"觀察物體熔點等等\",{\"1\":{\"257\":1}}],[\"觀察到的結果\",{\"1\":{\"253\":1}}],[\"任務大致是要在網頁當中購買特定的物品\",{\"1\":{\"257\":1}}],[\"任務的結果\",{\"1\":{\"247\":1}}],[\"內容\",{\"1\":{\"257\":1}}],[\"內容如下\",{\"1\":{\"257\":1}}],[\"開啟\",{\"1\":{\"257\":1}}],[\"開始訓練\",{\"1\":{\"197\":1}}],[\"開始前\",{\"1\":{\"32\":1}}],[\"開始之前先把參數加上\",{\"1\":{\"157\":1}}],[\"開始之前他們把一個\",{\"1\":{\"35\":1}}],[\"開始之前\",{\"1\":{\"26\":1}}],[\"決定出第二個\",{\"1\":{\"254\":1}}],[\"決定出第一個\",{\"1\":{\"254\":1}}],[\"決定出\",{\"1\":{\"254\":1}}],[\"×logπϕ​\",{\"1\":{\"253\":1}}],[\"×logπθ​\",{\"1\":{\"253\":1}}],[\"組成\",{\"1\":{\"253\":1}}],[\"組成的模型就像是有\",{\"1\":{\"111\":1}}],[\"篩除\",{\"1\":{\"253\":2}}],[\"令\",{\"1\":{\"253\":3,\"254\":1}}],[\"階段也是固定的\",{\"1\":{\"261\":1}}],[\"階段的知識\",{\"1\":{\"261\":1}}],[\"階段製作的\",{\"1\":{\"260\":1}}],[\"階段\",{\"1\":{\"252\":2,\"254\":1}}],[\"階段能夠動態地依據當下的輸入資料的相似性來產生對應的affinity\",{\"1\":{\"74\":1}}],[\"ρstateknow​\",{\"1\":{\"252\":1}}],[\"ρtaskknow​\",{\"1\":{\"251\":1}}],[\"κ∼πϕ​\",{\"1\":{\"254\":1}}],[\"κ∼πθ​\",{\"1\":{\"251\":1}}],[\"κ∣u\",{\"1\":{\"253\":3}}],[\"κ\",{\"1\":{\"253\":16,\"254\":5}}],[\"κ∈k\",{\"1\":{\"251\":1}}],[\"太差\",{\"1\":{\"251\":1}}],[\"太強\",{\"1\":{\"83\":1}}],[\"模擬環境的知識\",{\"1\":{\"249\":1}}],[\"模型搭配其他\",{\"1\":{\"261\":1}}],[\"模型架構與\",{\"0\":{\"258\":1}}],[\"模型實際上會做的事情是去預測下一個\",{\"1\":{\"245\":1}}],[\"模型以及\",{\"1\":{\"210\":1}}],[\"模型採用\",{\"1\":{\"200\":1}}],[\"模型就越相信他是屬於\",{\"1\":{\"192\":1}}],[\"模型通常會傾向\",{\"1\":{\"121\":1}}],[\"模型能夠得到的\",{\"1\":{\"100\":1}}],[\"模型很可能已經被\",{\"1\":{\"75\":1}}],[\"模型還是能夠順利學習\",{\"1\":{\"37\":1}}],[\"模型已經能夠在大多的\",{\"1\":{\"16\":1}}],[\"模型會傾向回答\",{\"1\":{\"7\":1}}],[\"模型\",{\"1\":{\"6\":1,\"16\":1,\"200\":1}}],[\"真實世界的知識\",{\"1\":{\"249\":1}}],[\"真的很糟\",{\"1\":{\"61\":1}}],[\"完成率有多少\",{\"1\":{\"249\":1}}],[\"完成的狀況如何\",{\"1\":{\"245\":1}}],[\"描述\",{\"1\":{\"249\":1,\"253\":1}}],[\"輔助\",{\"1\":{\"247\":1}}],[\"諸如此類的問題\",{\"1\":{\"245\":1}}],[\"雞蛋很可能出現在冰箱裡面\",{\"1\":{\"245\":1}}],[\"幾乎都是\",{\"1\":{\"245\":1}}],[\"幾乎都是訓練在\",{\"1\":{\"76\":1}}],[\"尚未正式於\",{\"1\":{\"244\":1}}],[\"尚未完成的推甄資料\",{\"1\":{\"239\":1}}],[\"腦袋都是日文跟中文了w\",{\"1\":{\"240\":1}}],[\"找到動物\",{\"1\":{\"257\":1}}],[\"找到\",{\"1\":{\"254\":1}}],[\"找到冰箱\",{\"1\":{\"245\":1}}],[\"找到寶藏可以加分\",{\"1\":{\"16\":1}}],[\"找了會講英文的工作人員來幫忙\",{\"1\":{\"240\":1}}],[\"辦電信的時候店員覺得我們不太會講日文\",{\"1\":{\"240\":1}}],[\"普遍而言單純用\",{\"1\":{\"260\":1}}],[\"普遍遇到的日本人都很友善\",{\"1\":{\"240\":1}}],[\"普遍關注於如何在不同\",{\"1\":{\"170\":1}}],[\"秋葉原的某一條河流\",{\"1\":{\"239\":1}}],[\"つくば駅附近的\",{\"1\":{\"239\":1}}],[\"裡面的火箭\",{\"1\":{\"239\":1}}],[\"某個遊戲場內的交流ノート\",{\"1\":{\"239\":1}}],[\"覺得很好吃的力うどん\",{\"1\":{\"239\":1}}],[\"覺得很開心\",{\"1\":{\"6\":1}}],[\"準備接機\",{\"1\":{\"239\":1}}],[\"趁我們都還沒開學之前空出了幾天出去筑波和秋葉原晃晃\",{\"1\":{\"239\":1}}],[\"冠霆也申請到了東工大的交換\",{\"1\":{\"239\":1}}],[\"冠文\",{\"1\":{\"234\":1}}],[\"詳細地跟學校確認能不能準時畢業\",{\"1\":{\"239\":1}}],[\"詳細的步驟具體來說\",{\"1\":{\"54\":1}}],[\"手續處理是本來就需要做的\",{\"1\":{\"239\":1}}],[\"熱情的阿姨給了他自己手做的葡萄xd\",{\"1\":{\"238\":1}}],[\"迷路走到某個鄉間小路\",{\"1\":{\"238\":1}}],[\"夜晚的筑波是真的有點可怕w\",{\"1\":{\"238\":1}}],[\"偶爾也會迷路\",{\"1\":{\"238\":1}}],[\"校園一隅\",{\"1\":{\"238\":1}}],[\"校內的\",{\"1\":{\"238\":1}}],[\"慢慢適應新的生活環境\",{\"1\":{\"238\":1}}],[\"陸陸續續處理好了市役所的各個手續\",{\"1\":{\"238\":1}}],[\"コズミくん\",{\"1\":{\"238\":1}}],[\"突然覺得水道水很好喝\",{\"1\":{\"237\":1}}],[\"話說\",{\"1\":{\"237\":1}}],[\"晚餐的清六家拉麵我對最重要的拉麵沒什麼感想\",{\"1\":{\"237\":1}}],[\"晚上\",{\"1\":{\"236\":1}}],[\"檔變速器的公路車\",{\"1\":{\"237\":1}}],[\"似乎也不知道為什麼\",{\"1\":{\"237\":1}}],[\"腳踏車真的還是抓不準\",{\"1\":{\"240\":1}}],[\"腳踏車登記\",{\"1\":{\"237\":1}}],[\"腳快要斷了\",{\"1\":{\"237\":1}}],[\"走了一堆路\",{\"1\":{\"237\":1}}],[\"走到\",{\"1\":{\"35\":1}}],[\"老闆相當平易近人\",{\"1\":{\"237\":1}}],[\"盥洗用品等等\",{\"1\":{\"237\":1}}],[\"毛絮黏把\",{\"1\":{\"237\":1}}],[\"掛勾\",{\"1\":{\"237\":1}}],[\"菜瓜布和保鮮盒\",{\"1\":{\"237\":1}}],[\"国民健康保険還有国民年金\",{\"1\":{\"237\":1}}],[\"生活安頓\",{\"0\":{\"237\":1}}],[\"終於拍到全貌\",{\"1\":{\"239\":1}}],[\"終於看到了有亮光的地方\",{\"1\":{\"236\":1}}],[\"終於順利領到了寢具\",{\"1\":{\"236\":1}}],[\"黑黑的一片如果是自己一個人也許會有些不安\",{\"1\":{\"236\":1}}],[\"筑波大學的吉祥物\",{\"1\":{\"238\":1}}],[\"筑波大學交換週記\",{\"0\":{\"234\":1}}],[\"筑波基本上就是一個森林\",{\"1\":{\"236\":1}}],[\"帶我們到学園都市附近的\",{\"1\":{\"237\":1}}],[\"帶我們去附近的松屋吃晚餐\",{\"1\":{\"236\":1}}],[\"帶來了幾個好處\",{\"1\":{\"152\":1,\"154\":1,\"155\":1}}],[\"帶來的正面效益為最大\",{\"1\":{\"260\":1}}],[\"帶來的幾個好處\",{\"1\":{\"153\":1}}],[\"帶來的影響力\",{\"1\":{\"162\":1}}],[\"帶來的影響後發現\",{\"1\":{\"139\":1}}],[\"帶來的影響耶\",{\"1\":{\"116\":1}}],[\"帶來的好處\",{\"1\":{\"116\":1}}],[\"帶來的效益\",{\"1\":{\"38\":1}}],[\"宿舍繳費方式登記\",{\"1\":{\"237\":1}}],[\"宿舍雖然小小的\",{\"1\":{\"236\":1}}],[\"宿舍人員體醒我們接下來要去領寢具\",{\"1\":{\"236\":1}}],[\"告訴你每個月的哪幾天可以過去更換乾淨的寢具\",{\"1\":{\"236\":1}}],[\"告訴我們每個元素需要關注其他元素多少程度\",{\"1\":{\"216\":1}}],[\"枕頭等等\",{\"1\":{\"236\":1}}],[\"棉被\",{\"1\":{\"236\":1}}],[\"悶熱與勞累下排了不少的時間\",{\"1\":{\"236\":1}}],[\"隊伍排得長長的\",{\"1\":{\"236\":1}}],[\"剛好那天下著雨\",{\"1\":{\"236\":1}}],[\"領到了更多的文件要閱讀\",{\"1\":{\"238\":1}}],[\"領寢具的地方在一個被樹林包圍的角落\",{\"1\":{\"236\":1}}],[\"領域當中把一些\",{\"1\":{\"91\":1}}],[\"領域當中做出了劃時代的貢獻\",{\"1\":{\"69\":1}}],[\"領域\",{\"1\":{\"85\":1}}],[\"領域都已經是過時的產物\",{\"1\":{\"70\":1}}],[\"領域的各種知識\",{\"1\":{\"0\":1}}],[\"領域發展\",{\"1\":{\"0\":1}}],[\"填寫文件\",{\"1\":{\"236\":1}}],[\"興許是學校跟\",{\"1\":{\"236\":1}}],[\"抵達學校後就是一陣兵荒馬亂\",{\"1\":{\"236\":1}}],[\"抵達了機場後有些緊張\",{\"1\":{\"236\":1}}],[\"搭乘接駁車來到了成田機場的第二航廈\",{\"1\":{\"236\":1}}],[\"搭配各個\",{\"1\":{\"260\":1}}],[\"搭配對抗式學習對\",{\"1\":{\"200\":1}}],[\"搭配\",{\"1\":{\"30\":2,\"56\":1,\"70\":1,\"84\":1,\"95\":1,\"143\":1,\"154\":1,\"179\":1,\"200\":2}}],[\"はい\",{\"1\":{\"236\":1}}],[\"遞出護照\",{\"1\":{\"236\":1}}],[\"特地挑了窗邊的座位真的太好了\",{\"1\":{\"236\":1}}],[\"忍不住就想拿出手機多拍幾張照片\",{\"1\":{\"236\":1}}],[\"怎麼聊天\",{\"1\":{\"236\":1}}],[\"心中模擬著跟海關要說些什麼\",{\"1\":{\"236\":1}}],[\"心裡不斷思考隔天將會面對的事件\",{\"1\":{\"236\":1}}],[\"心得\",{\"0\":{\"4\":1},\"1\":{\"2\":2,\"3\":1}}],[\"事情告一段落後就立馬休息\",{\"1\":{\"236\":1}}],[\"事先處理好了\",{\"1\":{\"101\":1}}],[\"事先標記好\",{\"1\":{\"101\":1}}],[\"月\",{\"1\":{\"236\":1}}],[\"年的\",{\"1\":{\"236\":1}}],[\"年由\",{\"1\":{\"151\":1}}],[\"啟程\",{\"0\":{\"236\":1}}],[\"包含\",{\"1\":{\"258\":1}}],[\"包含我在內的不少人也開始萌生想要用學習到的日語能力到日本交換的念頭\",{\"1\":{\"235\":1}}],[\"包含了這次目標的\",{\"1\":{\"249\":1}}],[\"包含了旋轉\",{\"1\":{\"196\":1}}],[\"包含了在\",{\"1\":{\"170\":1}}],[\"包含了所有\",{\"1\":{\"157\":1}}],[\"包含了許多的空間\",{\"1\":{\"257\":1}}],[\"包含了許多的高解析度彩色圖片\",{\"1\":{\"116\":1}}],[\"包含了許多\",{\"1\":{\"116\":3}}],[\"包含了\",{\"1\":{\"73\":1,\"97\":1,\"137\":1,\"173\":2,\"230\":1,\"258\":1}}],[\"包含了幾個重要的部分\",{\"1\":{\"18\":1}}],[\"沒必要再丟給\",{\"1\":{\"252\":1}}],[\"沒想到居然有一個專門提供的單位\",{\"1\":{\"236\":1}}],[\"沒想到因此開啟了前往筑波大學交換的契機\",{\"1\":{\"235\":1}}],[\"沒有差異的問題\",{\"1\":{\"154\":1}}],[\"契機\",{\"0\":{\"235\":1}}],[\"支持我的家人們\",{\"1\":{\"234\":1}}],[\"阿郡助教們\",{\"1\":{\"234\":1}}],[\"鉦洋\",{\"1\":{\"234\":1}}],[\"雅之\",{\"1\":{\"234\":1}}],[\"北之間\",{\"1\":{\"234\":1}}],[\"勇氣\",{\"1\":{\"234\":1}}],[\"陳曦\",{\"1\":{\"234\":1}}],[\"致越\",{\"1\":{\"234\":1}}],[\"陪伴我持續練習日文的冠霆\",{\"1\":{\"234\":1}}],[\"感謝一路上教導我日語的阿普魯老師\",{\"1\":{\"234\":1}}],[\"感到很開心\",{\"1\":{\"11\":1}}],[\"困惑度\",{\"1\":{\"231\":1}}],[\"起初只針對機器翻譯領域做研究\",{\"1\":{\"230\":1}}],[\"極低的平行度以及資訊的流失\",{\"1\":{\"230\":1}}],[\"據說一般會將目標改成越接近\",{\"1\":{\"227\":1}}],[\"函數去決定每個\",{\"1\":{\"227\":1}}],[\"輸出結果的品質\",{\"1\":{\"258\":1}}],[\"輸出\",{\"1\":{\"247\":1}}],[\"輸出是會經過一個\",{\"1\":{\"227\":1}}],[\"輸入\",{\"1\":{\"221\":2}}],[\"輸入進去\",{\"1\":{\"25\":1}}],[\"細節設定如下\",{\"1\":{\"227\":1}}],[\"細節上\",{\"1\":{\"25\":1,\"26\":1,\"35\":1}}],[\"張\",{\"1\":{\"227\":1,\"259\":1}}],[\"硬體上使用的是\",{\"1\":{\"227\":1}}],[\"運算\",{\"1\":{\"226\":1}}],[\"運算及儲存資源\",{\"1\":{\"5\":1}}],[\"較佳\",{\"1\":{\"226\":1}}],[\"較不會因為\",{\"1\":{\"195\":1}}],[\"較不具有彈性\",{\"1\":{\"35\":1}}],[\"序列長度\",{\"1\":{\"226\":1}}],[\"順利解決掉\",{\"1\":{\"230\":1}}],[\"順利避免了最初提及的兩個問題\",{\"1\":{\"97\":1}}],[\"順序為何\",{\"1\":{\"224\":1}}],[\"擷取特徵後的結果再做一些加工\",{\"1\":{\"222\":1}}],[\"擷取出來的\",{\"1\":{\"221\":1}}],[\"擷取出的特徵圖如上\",{\"1\":{\"119\":1}}],[\"掉\",{\"1\":{\"221\":1}}],[\"次\",{\"1\":{\"221\":2}}],[\"想像你現在在廚房當中\",{\"1\":{\"245\":1}}],[\"想做到的事情\",{\"1\":{\"220\":1}}],[\"想解決的就是盡可能地將\",{\"1\":{\"46\":1}}],[\"綜合起來可以達到更加全面的描述及理解\",{\"1\":{\"220\":1}}],[\"綜合上面的幾個操作\",{\"1\":{\"216\":1}}],[\"或產生額外的\",{\"1\":{\"261\":1}}],[\"或許會使模型缺乏一些比較高層次的認知與知識\",{\"1\":{\"251\":1}}],[\"或許在表達能力上有所不足\",{\"1\":{\"220\":1}}],[\"或像是校方的入住相關資料\",{\"1\":{\"237\":1}}],[\"或是有沒有看過的\",{\"1\":{\"260\":1}}],[\"或是與最終目標不符等問題\",{\"1\":{\"252\":1}}],[\"或是只考慮到局部的\",{\"1\":{\"247\":1}}],[\"或是嘗試在書房完成整個任務等等\",{\"1\":{\"245\":1}}],[\"或是從低解析度\",{\"1\":{\"140\":1}}],[\"或是到\",{\"1\":{\"120\":1}}],[\"或是再額外加上其他優化的技巧都可以得到更好的結果\",{\"1\":{\"118\":1}}],[\"或是使用其他的優化手段都有辦法獲得更好的\",{\"1\":{\"117\":1}}],[\"或是汽車比卡車更常見\",{\"1\":{\"50\":1}}],[\"或是\",{\"1\":{\"16\":1,\"27\":1,\"70\":2,\"77\":1,\"81\":1,\"95\":1,\"134\":1,\"137\":1,\"139\":1,\"155\":1,\"179\":1,\"218\":1,\"223\":1,\"260\":1}}],[\"或是看起來安全帽是後製貼上去的\",{\"1\":{\"7\":1}}],[\"仔細看\",{\"1\":{\"220\":1}}],[\"顯然會將未來的資料也考量進去\",{\"1\":{\"219\":1}}],[\"顯示了\",{\"1\":{\"38\":1}}],[\"翻譯成\",{\"1\":{\"219\":1}}],[\"除上\",{\"1\":{\"218\":1}}],[\"除了元素本身之外\",{\"1\":{\"216\":1,\"221\":1}}],[\"除了得到一個對應的\",{\"1\":{\"214\":1}}],[\"除了能夠有高度的平行度\",{\"1\":{\"212\":1}}],[\"除了越準確越好\",{\"1\":{\"73\":1}}],[\"除了\",{\"1\":{\"61\":1,\"62\":1,\"99\":1,\"260\":1}}],[\"除了展現驚人的成果以外\",{\"1\":{\"38\":1}}],[\"梯度就收斂了\",{\"1\":{\"218\":1}}],[\"梯度會是\",{\"1\":{\"113\":1}}],[\"數字是越小越好\",{\"1\":{\"228\":1}}],[\"數字是越大越好\",{\"1\":{\"228\":1}}],[\"數學描述\",{\"1\":{\"216\":1}}],[\"數值範圍介於\",{\"1\":{\"216\":1}}],[\"數量\",{\"1\":{\"112\":1,\"218\":2,\"260\":1}}],[\"數量以及大小\",{\"1\":{\"74\":1}}],[\"數量相同\",{\"1\":{\"23\":1,\"26\":1,\"29\":1}}],[\"∞\",{\"1\":{\"216\":1}}],[\"|\",{\"1\":{\"216\":4,\"231\":1}}],[\"元素所在的位置\",{\"1\":{\"216\":1,\"221\":1}}],[\"壓成單一的\",{\"1\":{\"216\":1}}],[\"負責將原本的文字變成一個\",{\"1\":{\"215\":1}}],[\"注意\",{\"1\":{\"254\":1}}],[\"注意力機制\",{\"1\":{\"215\":1,\"231\":1}}],[\"注意並不是\",{\"1\":{\"35\":1}}],[\"常見在翻譯任務上\",{\"1\":{\"215\":1}}],[\"長度越長\",{\"1\":{\"214\":1}}],[\"隨著\",{\"1\":{\"214\":1}}],[\"隨機地放在地圖上的任意格子\",{\"1\":{\"35\":1}}],[\"難以透過平行化加速運算\",{\"1\":{\"214\":1}}],[\"難以收斂的問題\",{\"1\":{\"25\":1,\"40\":1}}],[\"難以收斂\",{\"1\":{\"23\":1}}],[\"世界\",{\"1\":{\"219\":2}}],[\"世界的最佳橋樑\",{\"1\":{\"214\":2,\"231\":1}}],[\"世界第三名\",{\"1\":{\"2\":1}}],[\"基於上述的問題以及過去的研究\",{\"1\":{\"212\":1}}],[\"基本的\",{\"0\":{\"173\":1},\"1\":{\"172\":1}}],[\"基本的想法是對每個不同的選擇去評估每個決策的信賴區間的上界\",{\"1\":{\"27\":1}}],[\"基本想法\",{\"0\":{\"157\":1}}],[\"基本概念\",{\"1\":{\"41\":1}}],[\"基本上就是將\",{\"1\":{\"158\":1}}],[\"基本上就是從\",{\"1\":{\"51\":1}}],[\"基本上就是會有一些工地的照片\",{\"1\":{\"5\":1}}],[\"基本上都相當接近\",{\"1\":{\"35\":1}}],[\"基本上使用了\",{\"1\":{\"22\":1}}],[\"基本上他們期待我們會運用\",{\"1\":{\"5\":1}}],[\"平行度差的問題\",{\"1\":{\"211\":1}}],[\"平行度極差\",{\"1\":{\"210\":1}}],[\"平均出現怪異\",{\"1\":{\"260\":1}}],[\"平均去評估\",{\"1\":{\"100\":1}}],[\"平均\",{\"1\":{\"27\":5,\"28\":1,\"194\":1}}],[\"依據這\",{\"1\":{\"254\":1}}],[\"依序放入一個網路中\",{\"1\":{\"214\":1}}],[\"依賴於上一個\",{\"1\":{\"210\":1}}],[\"依照\",{\"1\":{\"49\":1,\"131\":1}}],[\"依照過去\",{\"1\":{\"19\":1}}],[\"领域自适应语义分割\",{\"1\":{\"204\":1}}],[\"读\",{\"1\":{\"204\":1}}],[\"談機器學習裡的資訊理論\",{\"1\":{\"204\":1}}],[\"剖析深度學習\",{\"1\":{\"204\":1}}],[\"展示知識蒸餾\",{\"1\":{\"203\":1}}],[\"學校這邊的手許也終於快告一個段落了\",{\"1\":{\"240\":1}}],[\"學生證上寫的情報科学類讓不少人以為我是來交換學做\",{\"1\":{\"238\":1}}],[\"學出來的權重就可能越小\",{\"1\":{\"223\":1}}],[\"學會\",{\"1\":{\"198\":1}}],[\"學習\",{\"1\":{\"32\":1}}],[\"扔進\",{\"1\":{\"198\":1}}],[\"附上的\",{\"1\":{\"198\":1}}],[\"附近\",{\"1\":{\"119\":1}}],[\"收斂\",{\"1\":{\"198\":2}}],[\"收斂性相關研究\",{\"0\":{\"95\":1},\"1\":{\"92\":1}}],[\"鼓勵類別盡量地平均\",{\"1\":{\"196\":1}}],[\"鼓勵去探索那些在\",{\"1\":{\"19\":1}}],[\"受到的干擾較少\",{\"1\":{\"196\":1}}],[\"稱為\",{\"1\":{\"196\":1,\"197\":1}}],[\"彩度調整等\",{\"1\":{\"196\":1}}],[\"彩度跟亮度跟環境有些落差\",{\"1\":{\"7\":1}}],[\"明暗調整\",{\"1\":{\"196\":1}}],[\"強增強只是加上\",{\"1\":{\"196\":1}}],[\"強增強\",{\"1\":{\"196\":1}}],[\"知識增強來改善\",{\"1\":{\"196\":1}}],[\"←λη\",{\"1\":{\"194\":1}}],[\"∗1\",{\"1\":{\"194\":1}}],[\"∗y\",{\"1\":{\"112\":1}}],[\"∥\",{\"1\":{\"193\":2,\"196\":4}}],[\"∥2​\",{\"1\":{\"76\":1}}],[\"權重的計算方式本質上就是\",{\"1\":{\"193\":1}}],[\"權重的計算方式如下\",{\"1\":{\"193\":1}}],[\"權重計算\",{\"0\":{\"193\":1}}],[\"距離遠近\",{\"1\":{\"193\":1}}],[\"距離\",{\"1\":{\"192\":1}}],[\"頗類似\",{\"1\":{\"192\":1}}],[\"頗偏\",{\"1\":{\"62\":1}}],[\"稍有不同\",{\"1\":{\"192\":1}}],[\"稍微修改了這個做法\",{\"1\":{\"97\":1}}],[\"稍微翻了一下\",{\"1\":{\"16\":1}}],[\"η\",{\"1\":{\"192\":1,\"193\":2,\"194\":4}}],[\"ηt​=ηbase​⋅t\",{\"1\":{\"77\":1}}],[\"獲得了極佳的成果\",{\"1\":{\"245\":1}}],[\"獲得高的分數\",{\"1\":{\"192\":1}}],[\"獲得的\",{\"1\":{\"100\":1}}],[\"若直接同時更新的話\",{\"1\":{\"192\":1}}],[\"若已經又經過\",{\"1\":{\"32\":1}}],[\"才去更新\",{\"1\":{\"192\":1}}],[\"才會去擷取畫面\",{\"1\":{\"99\":1}}],[\"認真考慮很久要不要買小飲水機來燒水\",{\"1\":{\"237\":1}}],[\"認知跟實際充滿巨大的\",{\"1\":{\"187\":1}}],[\"認識與理解\",{\"1\":{\"123\":1}}],[\"認識到其他組都用了怎樣的方法去解決\",{\"1\":{\"10\":1}}],[\"圈起來的\",{\"1\":{\"187\":1}}],[\"適應\",{\"1\":{\"187\":1}}],[\"適應不同的環境\",{\"1\":{\"37\":1}}],[\"社得太大\",{\"1\":{\"181\":1}}],[\"敏感\",{\"1\":{\"181\":1}}],[\"主要是在細節的呈現上更精準了\",{\"1\":{\"180\":1}}],[\"部分參數細節\",{\"1\":{\"259\":1}}],[\"部分則是\",{\"1\":{\"179\":1}}],[\"部分的影響程度\",{\"1\":{\"54\":1}}],[\"測試\",{\"1\":{\"178\":1}}],[\"測試過程中讓每個\",{\"1\":{\"111\":1}}],[\"否則推遠\",{\"1\":{\"175\":1}}],[\"否則給\",{\"1\":{\"73\":1,\"131\":1}}],[\"∑xt​∈xt​​∑i​f\",{\"1\":{\"194\":1}}],[\"∑​log∑k=1npatch​​r\",{\"1\":{\"175\":1}}],[\"∑​log∑k=1npixel​​\",{\"1\":{\"174\":1}}],[\"∑k−1​rk​\",{\"1\":{\"28\":1}}],[\"∑k−1​1\",{\"1\":{\"28\":1}}],[\"拉近\",{\"1\":{\"174\":1}}],[\"產出\",{\"1\":{\"251\":1,\"254\":4}}],[\"產出的權重就會很大\",{\"1\":{\"193\":1}}],[\"產出的權重就會很小\",{\"1\":{\"193\":1}}],[\"產出的\",{\"1\":{\"174\":1}}],[\"產生出對應的\",{\"1\":{\"215\":1}}],[\"產生出選擇\",{\"1\":{\"26\":1}}],[\"產生一個新的\",{\"1\":{\"51\":1}}],[\"必然會存在\",{\"1\":{\"173\":1}}],[\"轉換成另一個\",{\"1\":{\"215\":1}}],[\"轉換成\",{\"1\":{\"173\":2}}],[\"轉變到\",{\"1\":{\"47\":1}}],[\"標記\",{\"1\":{\"173\":1}}],[\"減少了一般性\",{\"1\":{\"260\":1}}],[\"減少\",{\"1\":{\"170\":1}}],[\"減少產\",{\"0\":{\"158\":1}}],[\"强化学习中的探索与利用\",{\"1\":{\"165\":1}}],[\"强化学习中on\",{\"1\":{\"165\":1}}],[\"与off\",{\"1\":{\"165\":1}}],[\"方式去\",{\"1\":{\"258\":1}}],[\"方式\",{\"1\":{\"164\":1}}],[\"方法都會得到與其他方法相距甚遠的結果\",{\"1\":{\"260\":1}}],[\"方法是經過一個\",{\"1\":{\"254\":1}}],[\"方法通常會將輸入的圖片經過一個\",{\"1\":{\"131\":1}}],[\"方法處理的大架構如下圖所示\",{\"1\":{\"131\":1}}],[\"方法甚至大幅超越了過去他們提出的\",{\"1\":{\"128\":1}}],[\"方法如\",{\"1\":{\"113\":1}}],[\"方法\",{\"1\":{\"51\":1,\"74\":1,\"187\":1}}],[\"方法也是屬於\",{\"1\":{\"51\":1}}],[\"回顧一下我們加上\",{\"1\":{\"163\":1}}],[\"合格\",{\"1\":{\"235\":1}}],[\"合併成底下的\",{\"1\":{\"196\":1}}],[\"合理的\",{\"1\":{\"163\":1}}],[\"合成出\",{\"1\":{\"51\":2}}],[\"評分方式會也跟\",{\"1\":{\"162\":1}}],[\"評估一次\",{\"1\":{\"162\":1}}],[\"評估方式\",{\"0\":{\"100\":1}}],[\"拆分\",{\"1\":{\"158\":1}}],[\"拆開來解決\",{\"1\":{\"35\":1}}],[\"拆開來\",{\"1\":{\"25\":1}}],[\"假如我們在這裡也考慮了\",{\"1\":{\"252\":1}}],[\"假如\",{\"1\":{\"158\":1}}],[\"意指模擬\",{\"1\":{\"249\":1}}],[\"意外遇到美麗的餘暉\",{\"1\":{\"238\":1}}],[\"意味著我們認為我們已經成功地讓模型知道\",{\"1\":{\"218\":1}}],[\"意即讓\",{\"1\":{\"198\":2}}],[\"意即只有那些足以信任的預測結果才會被考慮進去\",{\"1\":{\"173\":1}}],[\"意即在過程中不會對\",{\"1\":{\"157\":1}}],[\"意義上就是在看每個\",{\"1\":{\"154\":1}}],[\"≈∇μ\",{\"1\":{\"157\":1}}],[\"ξ\",{\"1\":{\"157\":1,\"191\":1,\"197\":1}}],[\"∇ζπ​​lπ\",{\"1\":{\"160\":1}}],[\"∇μ\",{\"1\":{\"157\":1}}],[\"∇lˉ\",{\"1\":{\"157\":2}}],[\"∇θπ​​lπ\",{\"1\":{\"155\":1,\"160\":1}}],[\"刻意挑\",{\"1\":{\"157\":1}}],[\"穩定地訓練\",{\"1\":{\"155\":1}}],[\"∂θv\",{\"1\":{\"155\":1}}],[\"紅色的部分也就是前面提及的\",{\"1\":{\"155\":1}}],[\"紅綠燈\",{\"1\":{\"138\":1}}],[\"執行的過程當中會透過\",{\"1\":{\"258\":1}}],[\"執行的\",{\"1\":{\"249\":1}}],[\"執行\",{\"1\":{\"155\":1}}],[\"參照\",{\"1\":{\"155\":1}}],[\"參數越多就有越強大的表達能力\",{\"1\":{\"107\":1}}],[\"步的更新\",{\"1\":{\"155\":1}}],[\"往後看\",{\"1\":{\"155\":1}}],[\"往往會很分散\",{\"1\":{\"196\":1}}],[\"往往會有許多我們沒有的\",{\"1\":{\"46\":1}}],[\"往往假設資料的分布會維持住\",{\"1\":{\"91\":1}}],[\"往往在邊界上會有許多誤判的\",{\"1\":{\"53\":1}}],[\"往往先從貼標籤開始\",{\"1\":{\"50\":1}}],[\"往往就會\",{\"1\":{\"47\":1}}],[\"藍線\",{\"1\":{\"153\":1}}],[\"藍色線\",{\"1\":{\"82\":1}}],[\"橘線是綠線與紫線的誤差\",{\"1\":{\"153\":1}}],[\"橘色線\",{\"1\":{\"82\":1}}],[\"紫線是目標函數\",{\"1\":{\"153\":1}}],[\"紫色圓點表示\",{\"1\":{\"35\":1}}],[\"原以為都維持靠左邊就無敵了\",{\"1\":{\"240\":1}}],[\"原因與前述相同\",{\"1\":{\"221\":1}}],[\"原本我們以為要自己先去處理宿舍入住手續之後再跟\",{\"1\":{\"236\":1}}],[\"原本都是一個單純的詞語\",{\"1\":{\"223\":1}}],[\"原本\",{\"1\":{\"220\":1}}],[\"原本的定義為\",{\"1\":{\"159\":1}}],[\"原本的\",{\"1\":{\"159\":1,\"160\":1,\"218\":1}}],[\"原始\",{\"1\":{\"153\":1,\"155\":1}}],[\"原先\",{\"1\":{\"30\":1}}],[\"維度過大訓練困難的問題\",{\"1\":{\"152\":1}}],[\"替代\",{\"1\":{\"152\":1}}],[\"∼d​\",{\"1\":{\"152\":1,\"154\":1,\"159\":4}}],[\"∼ε​\",{\"1\":{\"93\":1}}],[\"底下列出的數學式只是在說明模型給出的機率分布總和是\",{\"1\":{\"254\":1}}],[\"底下\",{\"1\":{\"253\":1,\"260\":1}}],[\"底下是\",{\"1\":{\"247\":1}}],[\"底下是用來評估優劣的評分方式\",{\"1\":{\"162\":1}}],[\"底下以發表於\",{\"1\":{\"247\":1}}],[\"底下你預期可以拿到多好的\",{\"1\":{\"154\":1}}],[\"底下的內容只是單純的\",{\"1\":{\"151\":1}}],[\"底下使用不同架構時會再特別提醒\",{\"1\":{\"137\":1}}],[\"試著用簡單的日文跟工作人員說話後得到了日文的回覆讓我感到很開心\",{\"1\":{\"236\":1}}],[\"試著用那一個人的做法走過一次\",{\"1\":{\"151\":1}}],[\"試圖增加對\",{\"1\":{\"195\":1}}],[\"試圖讓他們產出的分布要越接近越好\",{\"1\":{\"196\":1}}],[\"試圖讓\",{\"1\":{\"7\":1}}],[\"核心的概念很簡單\",{\"1\":{\"151\":1}}],[\"效率與品質的問題\",{\"1\":{\"149\":1}}],[\"論文當中並沒有特別說明\",{\"1\":{\"223\":1}}],[\"論文\",{\"1\":{\"204\":1}}],[\"論文中更新一步\",{\"1\":{\"155\":1}}],[\"論文中有提及在\",{\"1\":{\"149\":1}}],[\"論文筆記\",{\"1\":{\"74\":1,\"86\":1,\"144\":1,\"204\":2}}],[\"研究所推甄\",{\"1\":{\"239\":1}}],[\"研究了\",{\"1\":{\"143\":1}}],[\"研究不同的\",{\"1\":{\"85\":1}}],[\"縮小\",{\"1\":{\"141\":1}}],[\"註\",{\"1\":{\"141\":1}}],[\"倍的時間花費\",{\"1\":{\"261\":1}}],[\"倍\",{\"1\":{\"139\":1}}],[\"調低\",{\"1\":{\"139\":1}}],[\"調整\",{\"1\":{\"54\":1,\"155\":1}}],[\"巴士\",{\"1\":{\"138\":1}}],[\"騎士\",{\"1\":{\"138\":1}}],[\"→\",{\"0\":{\"201\":2,\"202\":2},\"1\":{\"138\":2,\"139\":1,\"178\":2,\"179\":5,\"181\":1,\"182\":2,\"201\":1,\"202\":1}}],[\"換言之\",{\"1\":{\"218\":1,\"220\":1}}],[\"換成\",{\"1\":{\"137\":1}}],[\"換句話說\",{\"1\":{\"50\":1}}],[\"重新規劃出更好的方案\",{\"1\":{\"258\":1}}],[\"重疊的部分以平均作為結果\",{\"1\":{\"135\":1}}],[\"重複步驟\",{\"1\":{\"254\":1}}],[\"重複\",{\"1\":{\"32\":1}}],[\"且單一\",{\"1\":{\"216\":1}}],[\"且包含了\",{\"1\":{\"134\":1}}],[\"且提出技巧避免\",{\"1\":{\"132\":1}}],[\"亦然\",{\"1\":{\"134\":1}}],[\"亦即\",{\"1\":{\"26\":1,\"38\":1}}],[\"額外考慮\",{\"1\":{\"134\":1}}],[\"額外的\",{\"1\":{\"128\":1}}],[\"別忘了\",{\"1\":{\"134\":1}}],[\"⊙y^​d\",{\"1\":{\"134\":1}}],[\"⊙y^​c​\",{\"1\":{\"134\":1}}],[\"外面補\",{\"1\":{\"134\":1}}],[\"寫給所有人的自然語言處理與深度學習入門指南\",{\"1\":{\"214\":2,\"231\":1}}],[\"寫成數學式也許比較難理解\",{\"1\":{\"134\":1}}],[\"寫了什麼\",{\"1\":{\"5\":1}}],[\"設計\",{\"1\":{\"258\":1}}],[\"設計部分類似\",{\"1\":{\"221\":1}}],[\"設計出一個通用的架構\",{\"1\":{\"182\":1}}],[\"設為\",{\"1\":{\"134\":1}}],[\"設定上會是\",{\"1\":{\"257\":1}}],[\"設定上只包含\",{\"1\":{\"257\":1}}],[\"設定\",{\"0\":{\"173\":1},\"1\":{\"172\":1}}],[\"設定請詳閱\",{\"1\":{\"79\":1}}],[\"設定如下\",{\"1\":{\"19\":1}}],[\"越像越好\",{\"1\":{\"253\":1}}],[\"越好\",{\"1\":{\"227\":2,\"253\":1}}],[\"越大\",{\"1\":{\"139\":2,\"140\":2}}],[\"越大變得越小\",{\"1\":{\"35\":1}}],[\"越靠近\",{\"1\":{\"134\":2}}],[\"σˉ\",{\"1\":{\"163\":2}}],[\"σˉ=nweights​1​i∑​∣σiw​∣\",{\"1\":{\"163\":1}}],[\"σi\",{\"1\":{\"160\":2}}],[\"σ​l\",{\"1\":{\"157\":2}}],[\"σ2i\",{\"1\":{\"151\":1}}],[\"σ\",{\"1\":{\"134\":1,\"157\":3,\"163\":2}}],[\"又分別要給多少的注意力\",{\"1\":{\"134\":1}}],[\"又有不少的\",{\"1\":{\"16\":1}}],[\"機率分布\",{\"1\":{\"254\":2}}],[\"機制是其中一個解方\",{\"1\":{\"216\":1}}],[\"機制\",{\"1\":{\"212\":1}}],[\"機制去判斷現在應該要注重\",{\"1\":{\"134\":1}}],[\"機器學習讀書會\",{\"1\":{\"3\":1}}],[\"藉此讓\",{\"1\":{\"251\":1,\"252\":1}}],[\"藉此讓模型能夠學習到更多的特徵\",{\"1\":{\"109\":1}}],[\"藉此確保後續的運作正常\",{\"1\":{\"133\":1}}],[\"保證選出來的座標可以被\",{\"1\":{\"133\":1}}],[\"變成一個\",{\"1\":{\"221\":1}}],[\"變成了上面的\",{\"1\":{\"220\":1}}],[\"變成跟\",{\"1\":{\"134\":1}}],[\"變成最後的預測\",{\"1\":{\"132\":1}}],[\"變化\",{\"1\":{\"83\":1}}],[\"混合\",{\"1\":{\"132\":1,\"142\":1}}],[\"混合成新的圖片\",{\"1\":{\"51\":1}}],[\"整體流程可以簡單描述成\",{\"1\":{\"254\":1}}],[\"整體流程被分成三個階段\",{\"1\":{\"198\":1}}],[\"整體流程\",{\"0\":{\"198\":1,\"250\":1}}],[\"整體的\",{\"1\":{\"197\":1}}],[\"整體的架構可以用底下這張圖來簡單了解\",{\"1\":{\"176\":1}}],[\"整體的架構如下圖\",{\"1\":{\"132\":1}}],[\"整除\",{\"1\":{\"133\":1}}],[\"整個訓練過程中都會固定住\",{\"1\":{\"192\":1}}],[\"整個\",{\"1\":{\"111\":1}}],[\"整個問題\",{\"1\":{\"93\":1}}],[\"記憶體過量\",{\"1\":{\"132\":1}}],[\"記錄在\",{\"1\":{\"97\":1}}],[\"關口\",{\"1\":{\"234\":1}}],[\"關於\",{\"1\":{\"131\":1}}],[\"關注在\",{\"1\":{\"128\":1}}],[\"關注的主題包含\",{\"1\":{\"0\":1}}],[\"∈∣a∣∑​a\",{\"1\":{\"154\":1}}],[\"∈\",{\"1\":{\"134\":1}}],[\"∈rohd​​×owd​​×c\",{\"1\":{\"133\":1}}],[\"∈rohc​​×owc​​×c\",{\"1\":{\"133\":1}}],[\"∈rst​ht​​×st​wt​​×3\",{\"1\":{\"131\":1}}],[\"∈cthings​\",{\"1\":{\"76\":1}}],[\"ζv​\",{\"1\":{\"160\":3}}],[\"ζπ​\",{\"1\":{\"160\":2}}],[\"ζ−\",{\"1\":{\"159\":2}}],[\"ζ=\",{\"1\":{\"157\":1,\"163\":1}}],[\"ζ\",{\"1\":{\"131\":1,\"133\":1,\"157\":3,\"159\":5}}],[\"則比較沒有顯著的差異\",{\"1\":{\"260\":1}}],[\"則包含了\",{\"1\":{\"249\":1}}],[\"則有辦法解決上述的三個痛點\",{\"1\":{\"247\":1}}],[\"則有\",{\"1\":{\"227\":1}}],[\"則會直接表示屬於哪一個\",{\"1\":{\"191\":1}}],[\"則大贏\",{\"1\":{\"179\":1}}],[\"則可以看到在絕大多數的類別都有了提升\",{\"1\":{\"179\":1}}],[\"則越相信\",{\"1\":{\"134\":2}}],[\"則透過\",{\"1\":{\"131\":1}}],[\"則是會讓\",{\"1\":{\"258\":1}}],[\"則是把輸出乘上機率\",{\"1\":{\"112\":1}}],[\"則是現實世界當中的影像\",{\"1\":{\"57\":1}}],[\"創造出在兩個狀況下都能夠順利辨認的方法\",{\"1\":{\"128\":1}}],[\"架構就是指這個部分\",{\"1\":{\"215\":1}}],[\"架構替換成\",{\"1\":{\"212\":1}}],[\"架構蔚為流行\",{\"1\":{\"210\":1}}],[\"架構\",{\"1\":{\"128\":1,\"135\":1,\"150\":1,\"212\":1}}],[\"架構對於\",{\"1\":{\"85\":1}}],[\"探討\",{\"1\":{\"122\":1}}],[\"探索機率更高\",{\"1\":{\"27\":1}}],[\"探索機率高\",{\"1\":{\"27\":1}}],[\"現在我們的實驗只做在單一的\",{\"1\":{\"260\":1}}],[\"現在換成了腳踏車\",{\"1\":{\"238\":1}}],[\"現在有了腳踏車也終於能夠比較輕鬆地前往各種地方\",{\"1\":{\"238\":1}}],[\"現在在\",{\"1\":{\"220\":1}}],[\"現在替換成\",{\"1\":{\"159\":2}}],[\"現在\",{\"1\":{\"159\":1,\"160\":1}}],[\"現在即便丟到未知的環境當中\",{\"1\":{\"119\":1}}],[\"現實很骨感\",{\"1\":{\"6\":1}}],[\"反倒對有氣泡水的可爾必思念念不忘\",{\"1\":{\"237\":1}}],[\"反而彼此會難以配合\",{\"1\":{\"119\":1}}],[\"反之推遠\",{\"1\":{\"174\":1}}],[\"反之在\",{\"1\":{\"151\":1}}],[\"反之大的解析度會不好預測大物件\",{\"1\":{\"139\":1}}],[\"反之會走出最短路\",{\"1\":{\"35\":1}}],[\"反之則要有較為不同的\",{\"1\":{\"174\":1}}],[\"反之則越相信\",{\"1\":{\"134\":2}}],[\"反之則相遠\",{\"1\":{\"46\":1}}],[\"反之則是\",{\"1\":{\"35\":1}}],[\"反之則會去走那些比較熟悉的\",{\"1\":{\"19\":1}}],[\"修正\",{\"1\":{\"119\":1}}],[\"錯誤\",{\"1\":{\"119\":1}}],[\"篇\",{\"1\":{\"118\":1}}],[\"種植水果\",{\"1\":{\"257\":1}}],[\"種\",{\"1\":{\"227\":2}}],[\"種大主題\",{\"1\":{\"117\":1}}],[\"種不同的視角\",{\"1\":{\"220\":1}}],[\"種不同的類別\",{\"1\":{\"116\":1}}],[\"種不同模型一樣\",{\"1\":{\"111\":1}}],[\"要越接近目標的\",{\"1\":{\"253\":1}}],[\"要訓練模型產出\",{\"1\":{\"251\":1}}],[\"要訓練的\",{\"1\":{\"197\":1}}],[\"要是什麼\",{\"1\":{\"245\":1}}],[\"要件又是什麼\",{\"1\":{\"239\":1}}],[\"要我們馬上過去排隊\",{\"1\":{\"236\":1}}],[\"要有類似的\",{\"1\":{\"174\":1}}],[\"要先乘\",{\"1\":{\"113\":1}}],[\"要傾向\",{\"1\":{\"27\":2}}],[\"理想上\",{\"1\":{\"163\":1,\"196\":1}}],[\"理想上將輸入經過\",{\"1\":{\"109\":1}}],[\"理應因此得到較好的結果\",{\"1\":{\"76\":1}}],[\"很難真的放鬆下來享受這趟旅程\",{\"1\":{\"239\":1}}],[\"很熱情地邀請致越去試試各種腳踏車\",{\"1\":{\"237\":1}}],[\"很像的時候\",{\"1\":{\"216\":1}}],[\"很直覺地\",{\"1\":{\"192\":1}}],[\"很大\",{\"1\":{\"187\":1}}],[\"很顯然地\",{\"1\":{\"107\":1}}],[\"很多時候我們並不會直接去蒐集真實的資料\",{\"1\":{\"46\":1}}],[\"本人也不知道其實他需要在\",{\"1\":{\"236\":1}}],[\"本身\",{\"1\":{\"107\":1}}],[\"本名林禾堃\",{\"1\":{\"0\":1}}],[\"資料具有偏差\",{\"1\":{\"107\":1}}],[\"資料不足\",{\"1\":{\"107\":1}}],[\"資訊\",{\"1\":{\"128\":1,\"135\":2}}],[\"資訊之芽\",{\"1\":{\"3\":1}}],[\"資訊安全\",{\"1\":{\"0\":1}}],[\"證明了\",{\"1\":{\"101\":1}}],[\"類似\",{\"1\":{\"101\":1}}],[\"人工提取\",{\"1\":{\"101\":1}}],[\"人類也會依據現有的資訊持續解決當前任務\",{\"1\":{\"245\":1}}],[\"人類\",{\"1\":{\"94\":1}}],[\"人類的\",{\"1\":{\"16\":2}}],[\"即可\",{\"1\":{\"100\":1,\"160\":1,\"252\":1}}],[\"即便自己知道當下用哪一個\",{\"1\":{\"151\":1}}],[\"即便只用了\",{\"1\":{\"141\":1}}],[\"即便在\",{\"1\":{\"74\":1,\"196\":1}}],[\"即便在參數比較異常的狀況下仍然能有很不錯的學習成果\",{\"1\":{\"37\":1}}],[\"即便這些\",{\"1\":{\"74\":1}}],[\"即便這個環境設定是相當簡單的\",{\"1\":{\"35\":1}}],[\"即便\",{\"1\":{\"37\":1,\"49\":1,\"128\":1}}],[\"ϕ\",{\"1\":{\"97\":1}}],[\"ϕt+1​←αϕt​+\",{\"1\":{\"73\":1}}],[\"筆\",{\"1\":{\"97\":1,\"193\":1}}],[\"筆記\",{\"1\":{\"3\":3,\"204\":1}}],[\"固定住\",{\"1\":{\"192\":1}}],[\"固定使用的\",{\"1\":{\"140\":1}}],[\"固定為\",{\"1\":{\"99\":1}}],[\"固定\",{\"1\":{\"95\":1,\"120\":2}}],[\"舒緩\",{\"1\":{\"95\":1}}],[\"發現混合\",{\"1\":{\"261\":1}}],[\"發現弱模型可以引導強模型有更好的效益\",{\"1\":{\"261\":1}}],[\"發現到\",{\"1\":{\"84\":1}}],[\"發表\",{\"1\":{\"244\":1}}],[\"發表在\",{\"1\":{\"151\":1}}],[\"發布\",{\"1\":{\"244\":1}}],[\"發散\",{\"1\":{\"95\":1}}],[\"算法當中\",{\"1\":{\"164\":1}}],[\"算法\",{\"1\":{\"94\":1}}],[\"既然我們想要得到全部\",{\"1\":{\"135\":1}}],[\"既然\",{\"1\":{\"93\":1}}],[\"既然有\",{\"1\":{\"21\":1}}],[\"應該與經過弱增強的相同\",{\"1\":{\"196\":1}}],[\"應該要是\",{\"1\":{\"163\":1}}],[\"應該要能夠好好收斂\",{\"1\":{\"163\":1}}],[\"應該要慢慢變大\",{\"1\":{\"100\":1}}],[\"應該要給\",{\"1\":{\"16\":1}}],[\"應用在過去各種\",{\"1\":{\"138\":1}}],[\"應用在\",{\"1\":{\"91\":1}}],[\"好許多\",{\"1\":{\"201\":1}}],[\"好像不太能直接看出\",{\"1\":{\"116\":1}}],[\"好\",{\"1\":{\"91\":1,\"219\":2}}],[\"能執行的步驟當中語意最接近的\",{\"1\":{\"247\":1}}],[\"能用日文溝通真的很開心\",{\"1\":{\"240\":1}}],[\"能不能被認抵\",{\"1\":{\"239\":1}}],[\"能不能把\",{\"1\":{\"91\":1}}],[\"能代表不同的表達\",{\"1\":{\"220\":1}}],[\"能好好表示出\",{\"1\":{\"196\":1}}],[\"能辨識細節的優點\",{\"1\":{\"128\":1,\"133\":1}}],[\"能辨識大範圍特徵的優點\",{\"1\":{\"128\":1,\"133\":1}}],[\"能夠漸漸改正\",{\"1\":{\"195\":1}}],[\"能夠帶來許多的好處\",{\"1\":{\"195\":1}}],[\"能夠應用在過去的各種\",{\"1\":{\"170\":1}}],[\"能夠輕易地套用在所有的\",{\"1\":{\"164\":1}}],[\"能夠使用\",{\"1\":{\"155\":1}}],[\"能夠使模型更好去學習各自的\",{\"1\":{\"25\":1}}],[\"能夠透過在訓練當中忽略其他的\",{\"1\":{\"119\":1}}],[\"能夠在\",{\"1\":{\"164\":1}}],[\"能夠在所有的\",{\"1\":{\"38\":1}}],[\"能夠在上述\",{\"1\":{\"18\":1}}],[\"能夠上\",{\"1\":{\"35\":1}}],[\"能夠調整自己的\",{\"1\":{\"29\":1}}],[\"能夠盡可能去正確探索環境\",{\"1\":{\"16\":1}}],[\"語音資料等\",{\"1\":{\"91\":1}}],[\"視覺影像\",{\"1\":{\"91\":1}}],[\"阅读笔记\",{\"1\":{\"86\":1}}],[\"仍然有更好的\",{\"1\":{\"84\":1}}],[\"儘管\",{\"1\":{\"84\":1}}],[\"αu\",{\"1\":{\"254\":15}}],[\"α=0\",{\"1\":{\"195\":1}}],[\"α\",{\"1\":{\"83\":1,\"154\":3,\"181\":2}}],[\"αt​\",{\"1\":{\"19\":1}}],[\"另外也考慮到\",{\"1\":{\"83\":1}}],[\"另一方面\",{\"1\":{\"26\":1,\"35\":1,\"37\":1}}],[\"後記\",{\"0\":{\"240\":1}}],[\"後來才發現這些流程還會需要\",{\"1\":{\"236\":1}}],[\"後帶來的優勢\",{\"1\":{\"230\":1}}],[\"後對於\",{\"1\":{\"162\":1}}],[\"後的\",{\"1\":{\"119\":1,\"252\":1}}],[\"後都可以獲得很棒的結果\",{\"1\":{\"118\":1}}],[\"後續部分與\",{\"1\":{\"221\":1}}],[\"後續還有一些類似的實驗用來驗證在各個領域採用\",{\"1\":{\"118\":1}}],[\"後續的研究中則發現到\",{\"1\":{\"95\":1}}],[\"後從\",{\"1\":{\"118\":1}}],[\"後\",{\"1\":{\"82\":1,\"175\":1,\"216\":1,\"252\":1,\"254\":1}}],[\"後可以得到更好的成效\",{\"1\":{\"37\":1}}],[\"預測\",{\"1\":{\"135\":1}}],[\"預測的機率\",{\"1\":{\"131\":1}}],[\"預測的\",{\"1\":{\"83\":1}}],[\"預測的結果並不是每一個點都會考慮到\",{\"1\":{\"135\":1}}],[\"預測的結果\",{\"1\":{\"82\":1}}],[\"預設是不會知道的\",{\"1\":{\"73\":1}}],[\"已經完成\",{\"1\":{\"249\":1}}],[\"已經看得到日本了\",{\"1\":{\"236\":1}}],[\"已經有比\",{\"1\":{\"141\":1}}],[\"已經放不進\",{\"1\":{\"141\":1}}],[\"已經能夠在這種資料上去擷取特徵\",{\"1\":{\"91\":1}}],[\"已經\",{\"1\":{\"79\":1}}],[\"已知\",{\"1\":{\"27\":1}}],[\"忘記\",{\"1\":{\"76\":1,\"197\":1}}],[\"搞壞所導致\",{\"1\":{\"76\":1}}],[\"被決定的次數越高\",{\"1\":{\"254\":1}}],[\"被輸出的機率\",{\"1\":{\"227\":1}}],[\"被錯誤的資訊誤導了\",{\"1\":{\"192\":1}}],[\"被改寫如下\",{\"1\":{\"173\":1}}],[\"被\",{\"1\":{\"75\":1,\"82\":1,\"227\":1,\"254\":2}}],[\"被預測成\",{\"1\":{\"53\":2}}],[\"出\",{\"1\":{\"158\":1}}],[\"出現一些無腦的\",{\"1\":{\"245\":1}}],[\"出現在預測機率最高的前\",{\"1\":{\"116\":1}}],[\"出現的頻率\",{\"1\":{\"75\":1}}],[\"出來的結果可能極端地靠近\",{\"1\":{\"218\":1}}],[\"出來的結果會比沒有調整還要降低約\",{\"1\":{\"139\":1}}],[\"出來的\",{\"1\":{\"25\":1,\"196\":1}}],[\"干擾形成\",{\"1\":{\"75\":1}}],[\"給定一個\",{\"1\":{\"254\":1}}],[\"給定目標\",{\"1\":{\"254\":1}}],[\"給定\",{\"1\":{\"253\":1,\"254\":1}}],[\"給定輸入序列數量\",{\"1\":{\"226\":1}}],[\"給出\",{\"1\":{\"254\":1}}],[\"給出更穩健的\",{\"1\":{\"135\":1}}],[\"給出的\",{\"1\":{\"261\":1}}],[\"給出的幫助開始出現\",{\"1\":{\"121\":1}}],[\"給出的輸出取平均\",{\"1\":{\"107\":1}}],[\"給出不同\",{\"1\":{\"74\":1}}],[\"給你學習\",{\"1\":{\"26\":1}}],[\"通常會對模型的結果有不小的影響\",{\"1\":{\"121\":1}}],[\"通常會有個通病是在\",{\"1\":{\"74\":1}}],[\"通常在\",{\"1\":{\"76\":1}}],[\"通常並不會差太多\",{\"1\":{\"49\":1}}],[\"根據前後文\",{\"1\":{\"216\":1}}],[\"根據剛剛計算出來的結果\",{\"1\":{\"216\":1}}],[\"根據\",{\"1\":{\"155\":1,\"198\":1,\"254\":2,\"260\":1}}],[\"根據實驗的結果我們選擇使用\",{\"1\":{\"74\":1}}],[\"根據過去的研究\",{\"1\":{\"73\":1}}],[\"再次來到成田機場\",{\"1\":{\"239\":1}}],[\"再次找到對應的櫃台後\",{\"1\":{\"236\":1}}],[\"再次說明了\",{\"1\":{\"38\":1}}],[\"再經過\",{\"1\":{\"221\":1}}],[\"再\",{\"1\":{\"220\":1}}],[\"再一起\",{\"1\":{\"220\":1}}],[\"再接回前面的\",{\"1\":{\"220\":1}}],[\"再額外多一個\",{\"1\":{\"163\":1}}],[\"再多搭配另一個\",{\"1\":{\"140\":1}}],[\"再多一點調整後可以再提升約\",{\"1\":{\"80\":1}}],[\"再使用\",{\"1\":{\"135\":1}}],[\"再使用不同的\",{\"1\":{\"74\":1}}],[\"再結合\",{\"1\":{\"134\":1}}],[\"再進一步裁切得到\",{\"1\":{\"133\":1}}],[\"再依據得到的affinity\",{\"1\":{\"74\":1}}],[\"圖中圈起來的是各種交通工具\",{\"1\":{\"74\":1}}],[\"圖片中以\",{\"1\":{\"37\":1}}],[\"他的\",{\"1\":{\"257\":1}}],[\"他的輸出會被\",{\"1\":{\"112\":1}}],[\"他要去近似的是\",{\"1\":{\"93\":1}}],[\"他可以很好地把不同的\",{\"1\":{\"74\":1}}],[\"他們仍然無法說明\",{\"1\":{\"261\":1}}],[\"他們讓\",{\"1\":{\"247\":1}}],[\"他們的\",{\"1\":{\"227\":1}}],[\"他們會對\",{\"1\":{\"173\":1}}],[\"他們會先透過\",{\"1\":{\"96\":1}}],[\"他們比較了\",{\"1\":{\"139\":1}}],[\"他們提出的\",{\"1\":{\"128\":1}}],[\"他們透過實驗發現這很不錯\",{\"1\":{\"77\":1}}],[\"他們後來發現\",{\"1\":{\"74\":1}}],[\"他們也試著用相同的手段訓練模型\",{\"1\":{\"63\":1}}],[\"他們選擇用\",{\"1\":{\"63\":1}}],[\"他們選擇其中\",{\"1\":{\"61\":1}}],[\"他們認為在其他的\",{\"1\":{\"63\":1}}],[\"他們並不是採用\",{\"1\":{\"34\":1}}],[\"他們將整個環境以\",{\"1\":{\"249\":1}}],[\"他們將原本的\",{\"1\":{\"218\":1}}],[\"他們將\",{\"1\":{\"19\":1}}],[\"訓練使用了\",{\"1\":{\"259\":1}}],[\"訓練當中使用\",{\"1\":{\"259\":1}}],[\"訓練方式採用\",{\"1\":{\"258\":1}}],[\"訓練中包含\",{\"1\":{\"258\":1}}],[\"訓練了一個\",{\"1\":{\"251\":1}}],[\"訓練期間他們採用了\",{\"1\":{\"227\":1}}],[\"訓練前首先透過\",{\"1\":{\"200\":1}}],[\"訓練過程中不會考慮到\",{\"1\":{\"254\":1}}],[\"訓練過程中根據與\",{\"1\":{\"192\":1}}],[\"訓練過程中讓每個\",{\"1\":{\"111\":1}}],[\"訓練的環境都不同\",{\"1\":{\"155\":1}}],[\"訓練的過程包含了一點專家系統的概念\",{\"1\":{\"101\":2}}],[\"訓練成效也就翻倍\",{\"1\":{\"155\":1}}],[\"訓練避免偏差\",{\"1\":{\"152\":1}}],[\"訓練時就會使用\",{\"1\":{\"134\":1}}],[\"訓練\",{\"1\":{\"74\":1,\"85\":1,\"96\":1,\"259\":1}}],[\"至此我們有了新的方法取得\",{\"1\":{\"195\":1}}],[\"至於\",{\"1\":{\"191\":1}}],[\"至於那些被忽略的\",{\"1\":{\"99\":1}}],[\"至於詳細的\",{\"1\":{\"79\":1}}],[\"至少信心度要超過\",{\"1\":{\"73\":1}}],[\"至今平均的\",{\"1\":{\"27\":1}}],[\"至今被嘗試的次數\",{\"1\":{\"27\":1}}],[\"信心度的標準\",{\"1\":{\"73\":1}}],[\"期待預測的\",{\"1\":{\"73\":1}}],[\"避免錯誤的\",{\"1\":{\"261\":1}}],[\"避免執行怪異的行為以及無腦的嘗試\",{\"1\":{\"245\":1}}],[\"避免去考慮到後面的內容\",{\"1\":{\"221\":1}}],[\"避免了訓練目標經常地變動造成訓練效果差\",{\"1\":{\"152\":1}}],[\"避免了\",{\"1\":{\"109\":1,\"152\":1}}],[\"避免模型忘記\",{\"1\":{\"198\":1}}],[\"避免模型\",{\"1\":{\"76\":1}}],[\"避免\",{\"1\":{\"70\":1}}],[\"避免太大或是太小\",{\"1\":{\"19\":1}}],[\"等方式指引\",{\"1\":{\"258\":1}}],[\"等模型架構\",{\"1\":{\"210\":1}}],[\"等\",{\"1\":{\"149\":1,\"230\":1}}],[\"等架構\",{\"1\":{\"70\":1}}],[\"等不同的做法\",{\"1\":{\"49\":1}}],[\"物件偵測的領域自適應\",{\"1\":{\"65\":1}}],[\"挑選\",{\"1\":{\"63\":1}}],[\"針對上述兩個問題\",{\"1\":{\"187\":1}}],[\"針對這幾個部分同樣去研究對應的影響\",{\"1\":{\"142\":1}}],[\"針對\",{\"1\":{\"63\":1,\"74\":1,\"109\":1,\"140\":1,\"173\":1,\"181\":3,\"182\":1}}],[\"針對不同的\",{\"1\":{\"20\":1,\"115\":1}}],[\"判斷要不要\",{\"1\":{\"63\":1}}],[\"甚至可以比沒有混合的\",{\"1\":{\"260\":1}}],[\"甚至還有完整的\",{\"1\":{\"236\":1}}],[\"甚至那些\",{\"1\":{\"80\":1}}],[\"甚至對\",{\"1\":{\"62\":1}}],[\"甚至連\",{\"1\":{\"10\":1}}],[\"考慮兩個機率分布決定最終\",{\"1\":{\"254\":1}}],[\"考慮兩者的機率分布給出最終的\",{\"1\":{\"254\":1}}],[\"考慮在時間\",{\"1\":{\"155\":1}}],[\"考慮到\",{\"1\":{\"62\":1}}],[\"考慮有限的\",{\"1\":{\"21\":1}}],[\"單純依賴\",{\"1\":{\"212\":1}}],[\"單純\",{\"1\":{\"142\":1}}],[\"單純在\",{\"1\":{\"116\":1}}],[\"單純加上\",{\"1\":{\"116\":1}}],[\"單純用\",{\"1\":{\"61\":1,\"62\":1}}],[\"單純的\",{\"1\":{\"28\":1,\"53\":1}}],[\"建構的\",{\"1\":{\"60\":1}}],[\">r\",{\"1\":{\"76\":1}}],[\">τ\",{\"1\":{\"73\":1}}],[\">\",{\"0\":{\"61\":1,\"62\":1},\"1\":{\"57\":2,\"257\":5}}],[\"作用只在於訓練的前中期提供\",{\"1\":{\"163\":1}}],[\"作法上會把\",{\"1\":{\"175\":1}}],[\"作法上\",{\"1\":{\"157\":1}}],[\"作為最後的輸出\",{\"1\":{\"220\":1}}],[\"作為下一個網路的輸入\",{\"1\":{\"214\":1}}],[\"作為輸入會太過於耗費記憶體\",{\"1\":{\"141\":1}}],[\"作為輸出\",{\"1\":{\"111\":1}}],[\"作為一種\",{\"1\":{\"109\":1}}],[\"作為\",{\"1\":{\"56\":1,\"74\":1,\"91\":1,\"141\":1,\"162\":1,\"245\":2,\"254\":1,\"260\":1}}],[\"作者發現到這樣的作法除了能大幅超越其他的方法外\",{\"1\":{\"260\":1}}],[\"作者發現在每一個遊戲當中最後一個\",{\"1\":{\"163\":1}}],[\"作者嘗試混合各個\",{\"1\":{\"260\":1}}],[\"作者嘗試使用不同\",{\"1\":{\"260\":1}}],[\"作者猜想這是因為比較弱的模型包含了強的模型當中缺乏的資訊所帶來的結果\",{\"1\":{\"260\":1}}],[\"作者進一步加上了\",{\"1\":{\"197\":1}}],[\"作者進一步設計一個\",{\"1\":{\"196\":1}}],[\"作者進一步去研究\",{\"1\":{\"163\":1}}],[\"作者進一步去比較自己改良的\",{\"1\":{\"84\":1}}],[\"作者進一步去忽略畫面上方\",{\"1\":{\"83\":1}}],[\"作者進一步去分析究竟是哪一個部分使最後得到好的結果\",{\"1\":{\"74\":1}}],[\"作者分別去計算兩者的\",{\"1\":{\"196\":1}}],[\"作者分別使用\",{\"1\":{\"187\":1}}],[\"作者透過對所擁有的\",{\"1\":{\"196\":1}}],[\"作者有在\",{\"1\":{\"178\":1}}],[\"作者提及因為\",{\"1\":{\"173\":1}}],[\"作者提出的\",{\"1\":{\"247\":1,\"249\":1}}],[\"作者提出了\",{\"1\":{\"212\":1}}],[\"作者提出\",{\"1\":{\"170\":1}}],[\"作者除了分開兩個部分去產出\",{\"1\":{\"134\":1}}],[\"作者在訓練兩個模型都是用相同的\",{\"1\":{\"253\":1}}],[\"作者在\",{\"1\":{\"132\":1}}],[\"作者在實驗的過程當中發現到\",{\"1\":{\"75\":1}}],[\"作者在實驗當中發現如果採用\",{\"1\":{\"30\":1}}],[\"作者比較了同樣架構下不同\",{\"1\":{\"121\":1}}],[\"作者比較兩個模型在不同\",{\"1\":{\"35\":1}}],[\"作者設計了兩種狀況來看\",{\"1\":{\"120\":1}}],[\"作者使用一個簡單的\",{\"1\":{\"119\":1}}],[\"作者去挑了在該領域的\",{\"1\":{\"115\":1}}],[\"作者選了許多不同領域的資料集如下\",{\"1\":{\"115\":1}}],[\"作者從啟發當中發想提出了\",{\"1\":{\"107\":1}}],[\"作者從\",{\"1\":{\"107\":1}}],[\"作者推測是因為權重即便只有小的變化也會對\",{\"1\":{\"100\":1}}],[\"作者把這些\",{\"1\":{\"76\":1}}],[\"作者認為這樣的結果是因為\",{\"1\":{\"260\":1}}],[\"作者認為這是因為\",{\"1\":{\"83\":1}}],[\"作者認為這是因為若這些\",{\"1\":{\"75\":1}}],[\"作者認為這是好的\",{\"1\":{\"76\":1}}],[\"作者認為每經過一個\",{\"1\":{\"192\":1}}],[\"作者認為目前的做法存在兩個問題\",{\"1\":{\"187\":1}}],[\"作者認為普遍來說\",{\"1\":{\"181\":1}}],[\"作者認為\",{\"1\":{\"119\":1,\"181\":1}}],[\"作者認為過去大模型之所以會\",{\"1\":{\"119\":1}}],[\"作者認為在這兩者都有一個共通點\",{\"1\":{\"36\":1}}],[\"作者懷疑會不會其實我們應該要試著採用更好的\",{\"1\":{\"70\":1}}],[\"作者將\",{\"1\":{\"36\":1,\"37\":1}}],[\"作者也大膽預測\",{\"1\":{\"260\":1}}],[\"作者也注意到\",{\"1\":{\"83\":1}}],[\"作者也給出每個\",{\"1\":{\"80\":1}}],[\"作者也觀察了在幾款遊戲訓練過程中當中\",{\"1\":{\"37\":1}}],[\"作者也發現到當使用\",{\"1\":{\"119\":1}}],[\"作者也發現到\",{\"1\":{\"35\":1}}],[\"作者也發現如果把\",{\"1\":{\"35\":1}}],[\"作者也在論文當中證明了這種拆開來訓練的方法是等價於沒有拆開來訓練的樣子\",{\"1\":{\"25\":1}}],[\"作者建構一個簡單的\",{\"1\":{\"35\":1}}],[\"λ=0\",{\"1\":{\"194\":1}}],[\"λfd​\",{\"1\":{\"76\":1}}],[\"λ\",{\"1\":{\"54\":1,\"155\":1}}],[\"得知下一班已經客滿\",{\"1\":{\"236\":1}}],[\"得\",{\"1\":{\"181\":1}}],[\"得出一個對應的\",{\"1\":{\"212\":1}}],[\"得出來的效果都比起直接使用高解析度圖片還要差\",{\"1\":{\"140\":1}}],[\"得出\",{\"1\":{\"132\":1}}],[\"得出的\",{\"1\":{\"74\":1}}],[\"得到輸出\",{\"1\":{\"253\":1}}],[\"得到了\",{\"1\":{\"218\":1}}],[\"得到對應的特徵\",{\"1\":{\"175\":1}}],[\"得到對應的\",{\"1\":{\"174\":1}}],[\"得到相對應的圖片\",{\"1\":{\"173\":1}}],[\"得到不同的經驗\",{\"1\":{\"151\":1}}],[\"得到比\",{\"1\":{\"143\":1}}],[\"得到的結果有大幅度的成長\",{\"1\":{\"260\":1}}],[\"得到的結果如上\",{\"1\":{\"260\":1}}],[\"得到的結果會更好\",{\"1\":{\"140\":2}}],[\"得到的\",{\"1\":{\"249\":1,\"253\":1}}],[\"得到的資料也就不同\",{\"1\":{\"155\":1}}],[\"得到的最大\",{\"1\":{\"93\":1}}],[\"得到細節的資訊\",{\"1\":{\"134\":1}}],[\"得到大範圍的資訊\",{\"1\":{\"134\":1}}],[\"得到\",{\"1\":{\"54\":1,\"133\":1,\"134\":1}}],[\"得以有不同程度的影響\",{\"1\":{\"40\":1}}],[\"經過他很多次\",{\"1\":{\"239\":1}}],[\"經過許多的努力\",{\"1\":{\"235\":1}}],[\"經過一個\",{\"1\":{\"220\":1}}],[\"經過強增強的\",{\"1\":{\"196\":1}}],[\"經過實驗後發現無論是採用\",{\"1\":{\"81\":1}}],[\"經過\",{\"1\":{\"54\":1,\"80\":1,\"131\":1,\"174\":1,\"175\":2,\"221\":1,\"222\":1}}],[\"經過相同的\",{\"1\":{\"49\":1}}],[\"取決於對應到的投影矩陣\",{\"1\":{\"220\":1}}],[\"取出第一層\",{\"1\":{\"119\":1}}],[\"取出圖片\",{\"1\":{\"54\":1}}],[\"取出圖片與\",{\"1\":{\"54\":1}}],[\"取得鑰匙後\",{\"1\":{\"236\":1}}],[\"取得的\",{\"1\":{\"93\":1}}],[\"取得\",{\"1\":{\"32\":1,\"54\":1,\"198\":1}}],[\"取得不同的\",{\"1\":{\"22\":1}}],[\"覆蓋\",{\"1\":{\"53\":1}}],[\"許多的\",{\"1\":{\"53\":1}}],[\"邊界上的判斷會因為圖片跟相鄰環境的相似導致模糊不清\",{\"1\":{\"51\":1}}],[\"先輸出一個步驟\",{\"1\":{\"247\":1}}],[\"先轉成\",{\"1\":{\"51\":1}}],[\"先在\",{\"1\":{\"50\":1}}],[\"像是去烤箱裡面找雞蛋\",{\"1\":{\"245\":1}}],[\"像是需要去市役所辦理的転入届\",{\"1\":{\"237\":1}}],[\"像是文意理解或是語言翻譯這種任務\",{\"1\":{\"214\":1}}],[\"像是\",{\"1\":{\"51\":1,\"53\":1,\"61\":1,\"128\":1}}],[\"像是直接有一台車會去蒐集真實街景資料\",{\"1\":{\"46\":1}}],[\"拿到微波爐裡\",{\"1\":{\"245\":1}}],[\"拿出來做一些\",{\"1\":{\"97\":1}}],[\"拿出來\",{\"1\":{\"76\":1}}],[\"拿出兩張\",{\"1\":{\"51\":1}}],[\"拿去訓練\",{\"1\":{\"53\":1,\"54\":1}}],[\"拿來比較加上\",{\"1\":{\"37\":1}}],[\"缺乏\",{\"1\":{\"50\":1}}],[\"畢竟每個\",{\"1\":{\"155\":1}}],[\"畢竟都還在熟悉新的環境\",{\"1\":{\"119\":1}}],[\"畢竟在\",{\"1\":{\"63\":1}}],[\"畢竟\",{\"1\":{\"50\":1}}],[\"了\",{\"1\":{\"50\":1,\"93\":1,\"159\":1,\"160\":1,\"163\":1,\"187\":1,\"252\":1}}],[\"舉例而言\",{\"1\":{\"151\":1}}],[\"舉例來說\",{\"1\":{\"50\":1}}],[\"舉一個簡單的例子來說明\",{\"1\":{\"245\":1}}],[\"舉一個在\",{\"1\":{\"46\":1}}],[\"舉一個例子來說\",{\"1\":{\"26\":1}}],[\"半監督式學習\",{\"1\":{\"50\":1,\"65\":2}}],[\"天\",{\"1\":{\"227\":1}}],[\"天空之類的就通常會像是在半空中\",{\"1\":{\"49\":1}}],[\"天生就有一些自己的偏好\",{\"1\":{\"26\":1}}],[\"號誌\",{\"1\":{\"49\":1}}],[\"行動\",{\"1\":{\"258\":1}}],[\"行動之前需要先看運氣抽接下來使用的武器\",{\"1\":{\"151\":1}}],[\"行人都還是會跟地板黏在一起\",{\"1\":{\"49\":1}}],[\"行為模式改變\",{\"1\":{\"28\":1}}],[\"汽車\",{\"1\":{\"49\":1,\"138\":1,\"170\":2}}],[\"同樣是給予額外知識的\",{\"1\":{\"260\":1}}],[\"同樣是圓圈的部份會被拉進\",{\"1\":{\"174\":1}}],[\"同樣可以看到\",{\"1\":{\"202\":1}}],[\"同樣可以觀察到\",{\"1\":{\"116\":1}}],[\"同樣可以觀察到在加上\",{\"1\":{\"116\":1}}],[\"同樣也可以看到與\",{\"1\":{\"179\":1}}],[\"同樣也包含了\",{\"1\":{\"109\":1}}],[\"同樣都是以\",{\"1\":{\"170\":1}}],[\"同樣地也可以對\",{\"1\":{\"159\":1}}],[\"同樣的\",{\"1\":{\"140\":1}}],[\"同樣以自駕車的例子來說\",{\"1\":{\"49\":1}}],[\"同時將\",{\"1\":{\"143\":1}}],[\"同時\",{\"1\":{\"139\":1,\"214\":1}}],[\"同時避免\",{\"1\":{\"74\":1}}],[\"同時也要讓這個狀況下產出的\",{\"1\":{\"253\":1}}],[\"同時也要確認想要選哪些課程\",{\"1\":{\"239\":1}}],[\"同時也能在\",{\"1\":{\"38\":1}}],[\"同時也可以看到\",{\"1\":{\"38\":1}}],[\"做法是製作\",{\"1\":{\"252\":1}}],[\"做決策的研究\",{\"1\":{\"247\":1}}],[\"做修改\",{\"1\":{\"159\":1}}],[\"做調整\",{\"1\":{\"157\":1,\"261\":1}}],[\"做了詳細的比較如下表\",{\"1\":{\"140\":1}}],[\"做了兩個小修正\",{\"1\":{\"29\":1}}],[\"做出來的\",{\"1\":{\"135\":1}}],[\"做出一個\",{\"1\":{\"51\":1}}],[\"做的事情簡單來說就是\",{\"1\":{\"132\":1}}],[\"做在\",{\"1\":{\"119\":1}}],[\"做為新的\",{\"1\":{\"74\":1}}],[\"做得很棒不能直接表達在整體會表達很棒\",{\"1\":{\"63\":1}}],[\"做\",{\"1\":{\"50\":1}}],[\"做對抗式學習\",{\"1\":{\"49\":1}}],[\"還未能做到\",{\"1\":{\"261\":1}}],[\"還要更亮眼\",{\"1\":{\"260\":1}}],[\"還要強\",{\"1\":{\"37\":1}}],[\"還要強許多\",{\"1\":{\"6\":1}}],[\"還強時\",{\"1\":{\"260\":1}}],[\"還需要考慮\",{\"1\":{\"253\":1}}],[\"還順道買了紀念品和太空食物\",{\"1\":{\"239\":1}}],[\"還在確認的畢業條件\",{\"1\":{\"239\":1}}],[\"還有基本生活所需的各種用品補齊等等\",{\"1\":{\"237\":1}}],[\"還有順利地溝通\",{\"1\":{\"236\":1}}],[\"還沒有腳踏車的我們只能步行緩緩前進\",{\"1\":{\"236\":1}}],[\"還是相差許多\",{\"1\":{\"187\":1}}],[\"還是存在差異的\",{\"1\":{\"50\":1}}],[\"還是\",{\"1\":{\"49\":1,\"134\":1}}],[\"直覺上直接把產出的\",{\"1\":{\"252\":1}}],[\"直覺上能帶來對事物更加深刻與全面的理解\",{\"1\":{\"220\":1}}],[\"直覺上會覺得\",{\"1\":{\"135\":1}}],[\"直接從\",{\"1\":{\"102\":1}}],[\"直接使用\",{\"1\":{\"74\":1}}],[\"直接把訓練在虛擬環境的模型應用在真實環境\",{\"1\":{\"47\":1}}],[\"直到結束\",{\"1\":{\"254\":1}}],[\"直到我遇到了一日分の野菜\",{\"1\":{\"237\":1}}],[\"直到\",{\"1\":{\"32\":1}}],[\"擅自用自己的思維解讀\",{\"1\":{\"47\":1}}],[\"影像分割\",{\"1\":{\"47\":1}}],[\"近年來大型語言模型\",{\"1\":{\"245\":1}}],[\"近年來自然語言處理\",{\"1\":{\"210\":1}}],[\"近年來流行\",{\"1\":{\"187\":1}}],[\"近年來透過\",{\"1\":{\"47\":1}}],[\"近似\",{\"1\":{\"25\":2}}],[\"讓模型去產出當前步驟的總結\",{\"1\":{\"252\":1}}],[\"讓模型可以參考兩個做法去推論\",{\"1\":{\"251\":1}}],[\"讓模型比較好學\",{\"1\":{\"227\":1}}],[\"讓我在來到這個陌生的地方時並不是獨自面對這些未知的環境\",{\"1\":{\"236\":1}}],[\"讓我們知道要著重於哪些部分\",{\"1\":{\"216\":1}}],[\"讓我們得以用較低的成本在虛擬環境中訓練模型\",{\"1\":{\"46\":1}}],[\"讓他看起來就像是機率一樣\",{\"1\":{\"216\":1}}],[\"讓他越來越貼合真實的狀況\",{\"1\":{\"192\":1}}],[\"讓他們的\",{\"1\":{\"170\":1}}],[\"讓這個模型有辦法對\",{\"1\":{\"170\":1}}],[\"讓選擇更多樣\",{\"1\":{\"151\":1}}],[\"讓\",{\"1\":{\"99\":1,\"152\":1,\"153\":1,\"258\":1}}],[\"讓每個分身在各自的環境當中訓練\",{\"1\":{\"155\":1}}],[\"讓每個\",{\"1\":{\"26\":1}}],[\"降低無腦\",{\"1\":{\"261\":1}}],[\"降低訓練資料之間的關聯性\",{\"1\":{\"155\":1}}],[\"降低了資料之間的相關性\",{\"1\":{\"152\":1}}],[\"降低到了\",{\"1\":{\"118\":1}}],[\"降低\",{\"1\":{\"46\":1,\"54\":1}}],[\"到了新學校的環境適應\",{\"1\":{\"239\":1}}],[\"到二手用品店找便宜電器\",{\"1\":{\"238\":1}}],[\"到目前為止我們看到的\",{\"1\":{\"220\":1}}],[\"到高解析度\",{\"1\":{\"140\":1}}],[\"到的時間有所相關\",{\"1\":{\"82\":1}}],[\"到的機率\",{\"1\":{\"75\":1,\"254\":1}}],[\"到\",{\"1\":{\"46\":1,\"75\":1,\"83\":1}}],[\"環境與虛擬世界有差距\",{\"1\":{\"46\":1}}],[\"環境中取得\",{\"1\":{\"32\":1}}],[\"虛擬世界\",{\"1\":{\"46\":1}}],[\"投射到同一個平面上\",{\"1\":{\"46\":1}}],[\"共同發表\",{\"1\":{\"45\":1}}],[\"查爾摩斯理工大學\",{\"1\":{\"45\":1}}],[\"<>\",{\"1\":{\"41\":1}}],[\"值得一看的文章們\",{\"0\":{\"41\":1,\"65\":1,\"86\":1,\"123\":1,\"144\":1,\"165\":1,\"204\":1,\"231\":1}}],[\"值得一提的是\",{\"1\":{\"35\":1,\"216\":1}}],[\"使模型產出\",{\"1\":{\"258\":1}}],[\"使模型具有更好的普遍性\",{\"1\":{\"40\":1}}],[\"使基於\",{\"1\":{\"211\":1}}],[\"使\",{\"1\":{\"210\":1}}],[\"使最終的結果進一步提昇\",{\"1\":{\"197\":1}}],[\"使得\",{\"1\":{\"196\":1,\"214\":1}}],[\"使得子代能夠適應整個環境\",{\"1\":{\"107\":1}}],[\"使得所有\",{\"1\":{\"99\":1}}],[\"使得同類型的資料會相近\",{\"1\":{\"46\":1}}],[\"使用了\",{\"1\":{\"116\":1,\"227\":1}}],[\"使用的有無造成的結果\",{\"1\":{\"121\":1}}],[\"使用的有無有相當大的不同\",{\"1\":{\"116\":1}}],[\"使用的一些\",{\"1\":{\"112\":1}}],[\"使用線性的\",{\"1\":{\"95\":1}}],[\"使用非線性的\",{\"1\":{\"95\":1}}],[\"使用\",{\"1\":{\"22\":1,\"62\":1,\"74\":2,\"113\":1,\"133\":1,\"142\":1,\"152\":1,\"154\":1,\"227\":1,\"260\":1}}],[\"使用不同的\",{\"1\":{\"22\":1,\"140\":1}}],[\"自行車的停讓看起來已經是一種習慣了\",{\"1\":{\"240\":1}}],[\"自適應不同環境\",{\"1\":{\"40\":1}}],[\"自己調整\",{\"1\":{\"37\":1}}],[\"自己的\",{\"1\":{\"6\":1}}],[\"解決過去\",{\"1\":{\"102\":1}}],[\"解決\",{\"1\":{\"74\":1,\"211\":1}}],[\"解決訓練不穩定\",{\"1\":{\"40\":1}}],[\"解決了前面提及的第三個問題\",{\"1\":{\"93\":1}}],[\"解決了\",{\"1\":{\"25\":1,\"29\":1,\"30\":1,\"38\":1}}],[\"解決了一些\",{\"1\":{\"23\":1}}],[\"卻又擔心花太多錢在報名費跟回台灣面試的來回機票之類的成本過高\",{\"1\":{\"239\":1}}],[\"卻又不會有過高的計算量\",{\"1\":{\"111\":1}}],[\"卻有兩個很大的缺點\",{\"1\":{\"210\":1}}],[\"卻不太了解同樣\",{\"1\":{\"170\":1}}],[\"卻並不一定了\",{\"1\":{\"163\":1}}],[\"卻會受到\",{\"1\":{\"151\":1}}],[\"卻反而往往得到很糟糕的結果\",{\"1\":{\"76\":1}}],[\"卻是最差的\",{\"1\":{\"38\":1}}],[\"卻在一些簡單的問題做得很差\",{\"1\":{\"23\":1}}],[\"提供一個簡單又有效的\",{\"1\":{\"164\":1}}],[\"提供了更大的普遍性\",{\"1\":{\"37\":1}}],[\"提出的方法略為不同的地方在於他並不是直接對\",{\"1\":{\"157\":1}}],[\"提出了\",{\"1\":{\"245\":1}}],[\"提出了解決這個高估問題的方法\",{\"1\":{\"153\":1}}],[\"提出了三個方法避免\",{\"1\":{\"85\":1}}],[\"提出新的\",{\"1\":{\"143\":1}}],[\"提出一個新的\",{\"1\":{\"203\":1}}],[\"提出一個可以即時修正\",{\"1\":{\"203\":1}}],[\"提出一個能夠搭配許多\",{\"1\":{\"143\":1}}],[\"提出一個簡單避免\",{\"1\":{\"122\":1}}],[\"提出如\",{\"1\":{\"50\":1}}],[\"提出\",{\"1\":{\"40\":1,\"261\":1}}],[\"提出透過\",{\"1\":{\"40\":1}}],[\"提升模型的預測能力\",{\"1\":{\"132\":1}}],[\"提升了約\",{\"1\":{\"80\":4}}],[\"提升\",{\"1\":{\"35\":1}}],[\"±\",{\"1\":{\"37\":10}}],[\"款比較困難的遊戲當中有些甚至是能夠比\",{\"1\":{\"37\":1}}],[\"異常地大\",{\"1\":{\"37\":1}}],[\"尤其對於行人\",{\"1\":{\"240\":1}}],[\"尤其\",{\"1\":{\"196\":1}}],[\"尤其預測的過程當中並不包含\",{\"1\":{\"135\":1}}],[\"尤其是從\",{\"1\":{\"47\":1}}],[\"尤其在\",{\"1\":{\"36\":1,\"50\":1,\"260\":1}}],[\"尤其當\",{\"1\":{\"23\":1}}],[\"可能會很大\",{\"1\":{\"218\":1}}],[\"可能已經\",{\"1\":{\"192\":1}}],[\"可對應到\",{\"1\":{\"59\":1}}],[\"可見\",{\"1\":{\"35\":1}}],[\"可以達到更加效益\",{\"1\":{\"261\":1}}],[\"可以帶來更強大的效益\",{\"1\":{\"260\":1}}],[\"可以帶來很不錯的\",{\"1\":{\"162\":1}}],[\"可以降低模型產生怪異\",{\"1\":{\"260\":1}}],[\"可以推斷\",{\"1\":{\"260\":1}}],[\"可以接收錯誤的\",{\"1\":{\"260\":1}}],[\"可以用來測試模型的一般性\",{\"1\":{\"257\":1}}],[\"可以用單一的\",{\"1\":{\"157\":1}}],[\"可以釐清當前的狀況\",{\"1\":{\"252\":1}}],[\"可以學習規劃整體的解決步驟\",{\"1\":{\"251\":1}}],[\"可以理解成當\",{\"1\":{\"223\":1}}],[\"可以簡單理解成\",{\"1\":{\"221\":1}}],[\"可以把\",{\"1\":{\"221\":1}}],[\"可以寫成底下的數學表達\",{\"1\":{\"220\":1}}],[\"可以變成我們期待的\",{\"1\":{\"216\":1}}],[\"可以比較接近\",{\"1\":{\"196\":1}}],[\"可以看成是資訊流失的程度\",{\"1\":{\"226\":1}}],[\"可以看成是更新較慢的\",{\"1\":{\"193\":1}}],[\"可以看到無論在哪個\",{\"1\":{\"260\":1}}],[\"可以看到無論是把\",{\"1\":{\"179\":1}}],[\"可以看到兩者分別使最後結果提升了\",{\"1\":{\"181\":1}}],[\"可以看到在大多數的遊戲加上了\",{\"1\":{\"162\":1}}],[\"可以看到在綠色線的部分\",{\"1\":{\"83\":1}}],[\"可以看到左邊的部分\",{\"1\":{\"139\":1}}],[\"可以看到\",{\"1\":{\"116\":1,\"226\":1}}],[\"可以看到所有\",{\"1\":{\"80\":1}}],[\"可以看到比較明顯的結果\",{\"1\":{\"36\":1}}],[\"可以從分數上明顯看出來加上了\",{\"1\":{\"162\":1}}],[\"可以平行化加速訓練\",{\"1\":{\"155\":1}}],[\"可以進一步提升\",{\"1\":{\"142\":2}}],[\"可以有\",{\"1\":{\"141\":1}}],[\"可以有更好的\",{\"1\":{\"74\":1}}],[\"可以讓模型學習到更好的高解析度特徵\",{\"1\":{\"134\":1}}],[\"可以告訴我們要考慮多少的\",{\"1\":{\"134\":1}}],[\"可以得到最終的\",{\"1\":{\"176\":1}}],[\"可以得到\",{\"1\":{\"139\":2}}],[\"可以得到每個\",{\"1\":{\"131\":1}}],[\"可以得到更好的結果\",{\"1\":{\"70\":1,\"116\":2}}],[\"可以參考過去的文章\",{\"1\":{\"131\":1}}],[\"可以預測出\",{\"1\":{\"131\":1}}],[\"可以設定\",{\"1\":{\"120\":1}}],[\"可以描述其輸入與輸出如下\",{\"1\":{\"112\":1}}],[\"可以單純透過觀察模型在\",{\"1\":{\"100\":1}}],[\"可以直接把資料之間的關聯建構起來\",{\"1\":{\"91\":1}}],[\"可以觀察到當\",{\"1\":{\"260\":1}}],[\"可以觀察到無論是加上\",{\"1\":{\"260\":1}}],[\"可以觀察到\",{\"1\":{\"80\":1,\"83\":1,\"139\":1,\"260\":1}}],[\"可以分成\",{\"1\":{\"49\":1}}],[\"可以發現搭配了\",{\"1\":{\"82\":1}}],[\"可以發現\",{\"1\":{\"61\":1,\"62\":1}}],[\"可以發現單純用\",{\"1\":{\"47\":1}}],[\"可以發現到結果甚至會比起沒有\",{\"1\":{\"260\":1}}],[\"可以發現到無論是哪個環境當中\",{\"1\":{\"260\":1}}],[\"可以發現到在資料量小的狀況下使用\",{\"1\":{\"121\":1}}],[\"可以發現到在左邊的圖中選擇\",{\"1\":{\"120\":1}}],[\"可以發現到對應的\",{\"1\":{\"116\":1}}],[\"可以發現到通常越大的模型能夠提供更好的效益\",{\"1\":{\"74\":1}}],[\"可以發現到當\",{\"1\":{\"74\":1}}],[\"可以發現到\",{\"1\":{\"38\":1,\"74\":2,\"138\":1,\"140\":1}}],[\"可以發現到不同的遊戲會有不同的偏好\",{\"1\":{\"37\":1}}],[\"可以發現到加上\",{\"1\":{\"37\":1}}],[\"可以認為\",{\"1\":{\"37\":1,\"260\":1}}],[\"可以移除\",{\"1\":{\"29\":1}}],[\"可以選擇\",{\"1\":{\"27\":1}}],[\"可以拿到多少\",{\"1\":{\"21\":1}}],[\"可以幫助我們去評估如果我們\",{\"1\":{\"21\":1}}],[\"可以嘗試\",{\"1\":{\"6\":1}}],[\"可以自己使用的免費咖啡機\",{\"1\":{\"6\":1}}],[\"移除\",{\"1\":{\"35\":1}}],[\"確實可以避免無謂的\",{\"1\":{\"260\":1}}],[\"確實已經透過前面的各個實驗證實了他的效度是相當不錯\",{\"1\":{\"119\":1}}],[\"確實能夠避免最初提及\",{\"1\":{\"226\":1}}],[\"確實能夠得到好的結果\",{\"1\":{\"84\":1}}],[\"確實能夠帶來相當好的效益\",{\"1\":{\"35\":1}}],[\"確實它會傾向讓\",{\"1\":{\"27\":1}}],[\"增加通用性\",{\"1\":{\"155\":1}}],[\"增加模型的更新與\",{\"1\":{\"154\":1}}],[\"增加\",{\"1\":{\"35\":1,\"149\":1,\"157\":1,\"227\":1}}],[\"導致對結果的特徵很雜亂\",{\"1\":{\"187\":1}}],[\"導致我們對於不同的設計誤以為是不同類型的物件\",{\"1\":{\"128\":1}}],[\"導致複雜的模型學習過於有偏差\",{\"1\":{\"107\":1}}],[\"導致過度複雜的模型直接學習到\",{\"1\":{\"107\":1}}],[\"導致接下來會經過的\",{\"1\":{\"100\":1}}],[\"導致卡車時常被預測成汽車\",{\"1\":{\"50\":1}}],[\"導致預測失準\",{\"1\":{\"47\":1}}],[\"導致\",{\"1\":{\"35\":1,\"83\":1,\"128\":1,\"173\":1}}],[\"與環境\",{\"0\":{\"257\":1}}],[\"與前面相同\",{\"1\":{\"253\":1}}],[\"與一些些的\",{\"1\":{\"216\":1}}],[\"與機器翻譯等任務上時常使用\",{\"1\":{\"210\":1}}],[\"與過去相同\",{\"1\":{\"178\":1}}],[\"與過去的資源消耗相較減輕甚多\",{\"1\":{\"85\":1}}],[\"與過去的\",{\"1\":{\"84\":2,\"138\":1,\"192\":1}}],[\"與過去的方法相比\",{\"1\":{\"69\":1}}],[\"與過往我們讀過的幾篇論文一樣\",{\"1\":{\"173\":1}}],[\"與傳統的\",{\"1\":{\"74\":1,\"195\":1}}],[\"與\",{\"1\":{\"35\":1,\"38\":1,\"45\":1,\"73\":1,\"79\":1,\"91\":1,\"96\":1,\"109\":2,\"128\":1,\"132\":3,\"134\":5,\"138\":2,\"139\":3,\"140\":2,\"142\":3,\"143\":1,\"153\":1,\"154\":1,\"173\":3,\"174\":1,\"175\":1,\"176\":1,\"179\":1,\"181\":3,\"182\":3,\"187\":1,\"191\":1,\"196\":2,\"212\":1,\"215\":1,\"216\":2,\"218\":1,\"221\":3,\"222\":1,\"226\":2,\"245\":2,\"253\":5,\"258\":1,\"260\":1,\"261\":1}}],[\"由左至右地逐漸理解文意\",{\"1\":{\"214\":1}}],[\"由此可見\",{\"1\":{\"35\":1}}],[\"由於這部分是針對\",{\"1\":{\"258\":1}}],[\"由於這裡需要\",{\"1\":{\"174\":1}}],[\"由於每個\",{\"1\":{\"210\":1}}],[\"由於過去的研究當中發現\",{\"1\":{\"131\":1}}],[\"由於標記\",{\"1\":{\"70\":1}}],[\"由於\",{\"1\":{\"35\":2,\"74\":1,\"113\":1,\"134\":1,\"159\":1,\"160\":1,\"176\":1,\"187\":1}}],[\"所需的時間仍然是\",{\"1\":{\"226\":1}}],[\"所影響如下\",{\"1\":{\"112\":1}}],[\"所有的算法\",{\"1\":{\"101\":1}}],[\"所示\",{\"1\":{\"83\":1}}],[\"所使用的環境\",{\"1\":{\"249\":1}}],[\"所使用的\",{\"1\":{\"76\":1}}],[\"所謂的半監督式學習也就是說\",{\"1\":{\"50\":1}}],[\"所謂的\",{\"1\":{\"46\":1,\"50\":1}}],[\"所在的位置\",{\"1\":{\"35\":1}}],[\"所以買了一些碗盤\",{\"1\":{\"237\":1}}],[\"所以比較晚才和我們會合\",{\"1\":{\"236\":1}}],[\"所以也會把\",{\"1\":{\"197\":1}}],[\"所以特徵也必須要相同\",{\"1\":{\"175\":1}}],[\"所以對於一個\",{\"1\":{\"157\":1}}],[\"所以要讓大小都相等\",{\"1\":{\"134\":1}}],[\"所以沒有\",{\"1\":{\"134\":1}}],[\"所以漸漸地每個人都會有一定的能力水平\",{\"1\":{\"119\":1}}],[\"所以作為一個團隊來說\",{\"1\":{\"119\":1}}],[\"所以就一起去了腳踏車店找二手腳踏車\",{\"1\":{\"237\":1}}],[\"所以就不再贅述\",{\"1\":{\"118\":1}}],[\"所以就讓他專門來回答這個問題\",{\"1\":{\"8\":1}}],[\"所以設定\",{\"1\":{\"99\":1}}],[\"所以\",{\"1\":{\"91\":1,\"154\":1}}],[\"所以圖片底下的部分實際上並不是跟街景相關\",{\"1\":{\"83\":1}}],[\"所以採用這個方法\",{\"1\":{\"73\":1}}],[\"所以他們認為這樣不太公平\",{\"1\":{\"63\":1}}],[\"所以在數據上\",{\"1\":{\"62\":1}}],[\"所以在前面加上了一些\",{\"1\":{\"7\":1}}],[\"所以我們後續就主要專攻\",{\"1\":{\"6\":1}}],[\"所以我們後續的方向都著重在\",{\"1\":{\"6\":1}}],[\"所以晚上\",{\"1\":{\"6\":1}}],[\"所以會有額外的提問\",{\"1\":{\"5\":1}}],[\"綠色圓點表示\",{\"1\":{\"35\":1}}],[\"軸表示\",{\"1\":{\"35\":2}}],[\"各有其好處\",{\"1\":{\"134\":1}}],[\"各有一個\",{\"1\":{\"26\":1}}],[\"各自的\",{\"1\":{\"47\":1}}],[\"各自最傾向\",{\"1\":{\"35\":1}}],[\"接在\",{\"1\":{\"252\":1}}],[\"接著\",{\"1\":{\"260\":1}}],[\"接著是\",{\"1\":{\"260\":1}}],[\"接著會詳細說明兩個模型的訓練\",{\"1\":{\"253\":1}}],[\"接著會經過\",{\"1\":{\"221\":1}}],[\"接著就要去實踐\",{\"1\":{\"245\":1}}],[\"接著就順利取得了在留卡以及資格外活動證明的印章\",{\"1\":{\"236\":1}}],[\"接著到了海關\",{\"1\":{\"236\":1}}],[\"接著透過\",{\"1\":{\"133\":1,\"174\":1}}],[\"接著作者比較\",{\"1\":{\"35\":1}}],[\"接下來作者想確認\",{\"1\":{\"260\":1}}],[\"接下來構建出\",{\"1\":{\"258\":1}}],[\"接下來把訓練中失敗的\",{\"1\":{\"258\":1}}],[\"接下來我該做什麼\",{\"1\":{\"252\":1}}],[\"接下來我們就仔細看看\",{\"1\":{\"217\":1}}],[\"接下來就可以把它們作為作為參考資訊\",{\"1\":{\"251\":1}}],[\"接下來再讓這個\",{\"1\":{\"251\":1}}],[\"接下來為了要讓輸出的動作變成環境能夠接收的詞句\",{\"1\":{\"247\":1}}],[\"接下來空閒的時間需要先好好準備推甄的申請\",{\"1\":{\"240\":1}}],[\"接下來要選擇的課程等等\",{\"1\":{\"239\":1}}],[\"接下來對於校園還有附近環境的熟悉也許也是頗重要\",{\"1\":{\"238\":1}}],[\"接下來可以看到連接了一個\",{\"1\":{\"221\":1}}],[\"接下來可以詳細看\",{\"1\":{\"221\":1}}],[\"接下來直接看\",{\"1\":{\"200\":1}}],[\"接下來看到\",{\"1\":{\"179\":1}}],[\"接下來評估加上\",{\"1\":{\"162\":1}}],[\"接下來這一整個\",{\"1\":{\"157\":1}}],[\"接下來\",{\"1\":{\"134\":1,\"221\":1,\"254\":1}}],[\"接下來透過\",{\"1\":{\"133\":1}}],[\"接下來依照你的需求不同\",{\"1\":{\"26\":1}}],[\"接下來用\",{\"1\":{\"20\":1}}],[\"最外層的期望值是對\",{\"1\":{\"159\":1}}],[\"最一開始提到的問題就是使用\",{\"1\":{\"141\":1}}],[\"最一開始的假設是認為小的解析度會不好預測小物體\",{\"1\":{\"139\":1}}],[\"最相近的一個研究\",{\"1\":{\"96\":1}}],[\"最小化\",{\"1\":{\"93\":1}}],[\"最大化\",{\"1\":{\"93\":1}}],[\"最明顯\",{\"1\":{\"82\":1}}],[\"最主要的貢獻\",{\"1\":{\"74\":1}}],[\"最好的\",{\"1\":{\"61\":1}}],[\"最\",{\"1\":{\"53\":1,\"73\":1}}],[\"最初被用於把\",{\"1\":{\"51\":1}}],[\"最初是為了解決\",{\"1\":{\"50\":1}}],[\"最終我在兩年內取得\",{\"1\":{\"235\":1}}],[\"最終也可以看到\",{\"1\":{\"84\":1}}],[\"最終得到更好的結果\",{\"1\":{\"83\":1}}],[\"最終則是\",{\"1\":{\"70\":1}}],[\"最終在所有的\",{\"1\":{\"38\":1}}],[\"最終\",{\"1\":{\"35\":1,\"249\":1}}],[\"最傾向\",{\"1\":{\"35\":2}}],[\"最多\",{\"1\":{\"35\":1}}],[\"最後經過\",{\"1\":{\"221\":1}}],[\"最後再\",{\"1\":{\"220\":1}}],[\"最後再將這三個部份結合起來\",{\"1\":{\"172\":1}}],[\"最後我們還有一個向量\",{\"1\":{\"216\":1}}],[\"最後就剩下更新的\",{\"1\":{\"195\":1}}],[\"最後結合上面三個部分\",{\"1\":{\"176\":1}}],[\"最後整體的\",{\"1\":{\"155\":1}}],[\"最後合併成\",{\"1\":{\"154\":1}}],[\"最後的\",{\"1\":{\"138\":1,\"226\":1}}],[\"最後要把結果\",{\"1\":{\"134\":1}}],[\"最後輸出會乘上\",{\"1\":{\"113\":1}}],[\"最後得出的結果為\",{\"1\":{\"162\":2}}],[\"最後得出來的結果\",{\"1\":{\"101\":1}}],[\"最後得到的結果會是一個大小為\",{\"1\":{\"220\":1}}],[\"最後得到的\",{\"1\":{\"82\":1}}],[\"最後將\",{\"1\":{\"101\":1}}],[\"最後比較\",{\"1\":{\"38\":1}}],[\"最後是針對\",{\"1\":{\"37\":1}}],[\"最後\",{\"1\":{\"29\":1,\"37\":1,\"38\":1,\"128\":1,\"181\":1,\"196\":1,\"197\":1,\"254\":1,\"260\":1}}],[\"最後你一樣可以透過這些\",{\"1\":{\"26\":1}}],[\"最後只需要設定\",{\"1\":{\"22\":1}}],[\"最後只有\",{\"1\":{\"6\":1}}],[\"最後分數越多越好\",{\"1\":{\"16\":1}}],[\"最後在\",{\"1\":{\"9\":1}}],[\"右邊則是\",{\"1\":{\"217\":1}}],[\"右邊是\",{\"1\":{\"20\":1}}],[\"右移動\",{\"1\":{\"35\":1}}],[\"左邊還有個廁所和淋浴間沒有拍到\",{\"1\":{\"236\":1}}],[\"左邊是\",{\"1\":{\"20\":1}}],[\"左\",{\"1\":{\"35\":1}}],[\"下一步該做什麼\",{\"1\":{\"245\":1}}],[\"下面則是\",{\"1\":{\"201\":1}}],[\"下有多強\",{\"1\":{\"74\":1}}],[\"下拍攝\",{\"1\":{\"60\":1}}],[\"下拍攝的\",{\"1\":{\"59\":1}}],[\"下\",{\"1\":{\"35\":1,\"65\":1,\"140\":1,\"229\":1,\"254\":1}}],[\"範圍變成\",{\"1\":{\"34\":1}}],[\"總共有\",{\"1\":{\"116\":1,\"227\":1}}],[\"總和最大化\",{\"1\":{\"93\":1}}],[\"總之就是一頓大買特買\",{\"1\":{\"237\":1}}],[\"總之\",{\"1\":{\"32\":1}}],[\"總結來說\",{\"1\":{\"230\":1}}],[\"總結\",{\"0\":{\"11\":1}}],[\"交給\",{\"1\":{\"32\":1,\"215\":1}}],[\"交叉做了一些\",{\"1\":{\"7\":1}}],[\"將輸入與\",{\"1\":{\"221\":1}}],[\"將許多的向量以矩陣改寫後\",{\"1\":{\"216\":1}}],[\"將上述的種種\",{\"1\":{\"196\":1}}],[\"將沒有做任何修正的\",{\"1\":{\"162\":1}}],[\"將兩者結合後\",{\"1\":{\"139\":1}}],[\"將裁切後的圖片\",{\"1\":{\"133\":1}}],[\"將圖片切成\",{\"1\":{\"132\":1}}],[\"將產出的\",{\"1\":{\"49\":1}}],[\"將\",{\"1\":{\"32\":1,\"53\":1,\"54\":1,\"97\":1,\"133\":1,\"215\":1,\"216\":1,\"220\":1}}],[\"結合了\",{\"1\":{\"261\":1}}],[\"結合後分別得到\",{\"1\":{\"182\":1}}],[\"結合\",{\"0\":{\"176\":1},\"1\":{\"102\":1,\"196\":1,\"245\":1}}],[\"結合起來\",{\"1\":{\"97\":1}}],[\"結束的時機分成\",{\"1\":{\"249\":1}}],[\"結束\",{\"1\":{\"32\":1}}],[\"結果會很差\",{\"1\":{\"260\":1}}],[\"結果有些趨近於\",{\"1\":{\"218\":1}}],[\"結果相當糟糕\",{\"1\":{\"47\":1}}],[\"結果\",{\"0\":{\"9\":1}}],[\"⋅pknow​\",{\"1\":{\"254\":1}}],[\"⋅∣ρstateknow​\",{\"1\":{\"252\":1}}],[\"⋅∣ρtaskknow​\",{\"1\":{\"251\":1}}],[\"⋅∣u\",{\"1\":{\"249\":1,\"254\":1}}],[\"⋅∣ht​\",{\"1\":{\"249\":1,\"254\":1}}],[\"⋅∣st+i​\",{\"1\":{\"155\":1,\"160\":1}}],[\"⋅∣s\",{\"1\":{\"152\":1}}],[\"⋅k\",{\"1\":{\"133\":4}}],[\"⋅mthings\",{\"1\":{\"76\":1}}],[\"⋅\",{\"1\":{\"32\":1,\"73\":1,\"93\":1,\"131\":1}}],[\"估計當前\",{\"1\":{\"32\":1}}],[\"估計也是沒救\",{\"1\":{\"6\":1}}],[\"4​​otherwise​\",{\"1\":{\"134\":2}}],[\"4​=bd\",{\"1\":{\"133\":1}}],[\"4​=bc\",{\"1\":{\"133\":1}}],[\"4​\",{\"1\":{\"133\":2}}],[\"4≤p≤0\",{\"1\":{\"120\":2}}],[\"49\",{\"1\":{\"63\":1}}],[\"4\",{\"1\":{\"50\":1,\"80\":3,\"97\":1,\"138\":1,\"139\":1,\"142\":1,\"181\":2,\"227\":1,\"254\":1,\"257\":4,\"259\":2}}],[\"4336\",{\"1\":{\"37\":1}}],[\"4596\",{\"1\":{\"37\":1}}],[\"4753\",{\"1\":{\"37\":1}}],[\"400\",{\"1\":{\"32\":1}}],[\"40\",{\"1\":{\"30\":1,\"260\":2}}],[\"44\",{\"1\":{\"9\":1}}],[\"多了\",{\"1\":{\"261\":1}}],[\"多加上\",{\"1\":{\"29\":1}}],[\"多多探索\",{\"1\":{\"19\":1}}],[\"對方用日文問了我有沒有打算要打工\",{\"1\":{\"236\":1}}],[\"對所有\",{\"1\":{\"139\":1}}],[\"對應的特徵\",{\"1\":{\"174\":1}}],[\"對應的\",{\"1\":{\"51\":2,\"73\":2,\"131\":1,\"173\":1,\"174\":1,\"190\":1,\"196\":1,\"253\":1}}],[\"對陌生人的認識\",{\"1\":{\"50\":1}}],[\"對抗式學習\",{\"1\":{\"49\":1}}],[\"對於英文轉法文的部分使用的是\",{\"1\":{\"227\":1}}],[\"對於語句的理解也會有不同\",{\"1\":{\"224\":1}}],[\"對於一開始預測錯誤的\",{\"1\":{\"195\":1}}],[\"對於每個\",{\"1\":{\"192\":1,\"216\":2}}],[\"對於每個可訓練的參數拆解成\",{\"1\":{\"157\":1}}],[\"對於每個實驗的\",{\"1\":{\"34\":1}}],[\"對於效能提升大約\",{\"1\":{\"142\":1}}],[\"對於原本\",{\"1\":{\"133\":1}}],[\"對於模型的訓練沒有什麼幫助\",{\"1\":{\"121\":1}}],[\"對於這些\",{\"1\":{\"75\":1}}],[\"對於\",{\"1\":{\"30\":1,\"50\":1,\"139\":1,\"143\":1,\"155\":1,\"173\":1,\"196\":1}}],[\"對於結果並不會有影響\",{\"1\":{\"29\":1}}],[\"對於學習是並沒有幫助的\",{\"1\":{\"23\":1}}],[\"對\",{\"1\":{\"29\":1,\"116\":1,\"134\":1,\"159\":1,\"195\":1}}],[\"剩餘的都是相同的\",{\"1\":{\"28\":1}}],[\"剩下的四款遊戲則是因為環境太大\",{\"1\":{\"16\":1}}],[\"剩下這些遊戲有怎樣的共通點呢\",{\"1\":{\"16\":1}}],[\"僅僅是加上\",{\"1\":{\"28\":1}}],[\"來扮演\",{\"1\":{\"249\":1}}],[\"來描述\",{\"1\":{\"249\":1}}],[\"來處理物理環境中的規劃問題\",{\"1\":{\"245\":1}}],[\"來到了陌生的日本\",{\"1\":{\"236\":1}}],[\"來決定輸出特徵\",{\"1\":{\"221\":1}}],[\"來源相同\",{\"1\":{\"221\":1}}],[\"來估計\",{\"1\":{\"194\":1}}],[\"來表示\",{\"1\":{\"191\":1}}],[\"來表示一個點要偏向用\",{\"1\":{\"134\":1}}],[\"來轉換\",{\"1\":{\"191\":1}}],[\"來調整模型的機率分布\",{\"1\":{\"191\":1}}],[\"來調整整體\",{\"1\":{\"35\":1}}],[\"來得高\",{\"1\":{\"181\":1}}],[\"來達成\",{\"1\":{\"151\":1}}],[\"來分別處理這兩個部分\",{\"1\":{\"133\":1}}],[\"來近似\",{\"1\":{\"93\":1}}],[\"來說可能導致收斂不穩定以及緩慢等問題\",{\"1\":{\"155\":1}}],[\"來說它造成了怎樣的影響呢\",{\"1\":{\"119\":1}}],[\"來說\",{\"1\":{\"93\":1,\"112\":1,\"157\":1}}],[\"來說由於缺乏對於\",{\"1\":{\"50\":1}}],[\"來說是相當大的問題\",{\"1\":{\"50\":1}}],[\"來讓每個\",{\"1\":{\"40\":1}}],[\"來比較\",{\"1\":{\"36\":1}}],[\"來限制要考慮多久之前的經驗\",{\"1\":{\"28\":1}}],[\"來解決它\",{\"1\":{\"27\":1}}],[\"來解決\",{\"1\":{\"26\":1,\"187\":1}}],[\"採取\",{\"1\":{\"93\":1}}],[\"採取的次數以及得到的\",{\"1\":{\"28\":1}}],[\"採用的設計也幾乎基於這個\",{\"1\":{\"258\":1}}],[\"採用的模型有三種\",{\"1\":{\"258\":1}}],[\"採用的是\",{\"1\":{\"174\":1}}],[\"採用的\",{\"1\":{\"134\":1}}],[\"採用\",{\"1\":{\"79\":1,\"142\":2,\"143\":1,\"179\":1}}],[\"採用了\",{\"1\":{\"76\":1,\"131\":1}}],[\"採用不同的\",{\"1\":{\"26\":1}}],[\"採用分散式學習\",{\"1\":{\"22\":1}}],[\"而由於\",{\"1\":{\"260\":1}}],[\"而兩者結合也可以帶來更好的結果\",{\"1\":{\"260\":1}}],[\"而非直接加入\",{\"1\":{\"260\":1}}],[\"而非\",{\"1\":{\"253\":1}}],[\"而要洗乾淨最常去洗手檯\",{\"1\":{\"245\":1}}],[\"而實際在執行的過程當中\",{\"1\":{\"245\":1}}],[\"而近期開始出現一些使用\",{\"1\":{\"245\":1}}],[\"而大的模型則訓練約\",{\"1\":{\"227\":1}}],[\"而訊息流失則是看\",{\"1\":{\"226\":1}}],[\"而不要用\",{\"1\":{\"226\":1}}],[\"而串接的\",{\"1\":{\"221\":1}}],[\"而總共需要的計算量與\",{\"1\":{\"220\":1}}],[\"而達到忽略未來的效果\",{\"1\":{\"219\":1}}],[\"而對於細部每一個預測的\",{\"1\":{\"179\":1}}],[\"而對於這一層\",{\"1\":{\"112\":1}}],[\"而接下來作者給出一個\",{\"1\":{\"158\":1}}],[\"而這裡則選擇在\",{\"1\":{\"151\":1}}],[\"而這種探索的困難度甚至是指數性地成長\",{\"1\":{\"149\":1}}],[\"而言都獲得更好的結果\",{\"1\":{\"138\":1}}],[\"而言就算他答對\",{\"1\":{\"116\":1}}],[\"而且可以發現到幾乎所有的\",{\"1\":{\"119\":1}}],[\"而且也往往會經過一段時間的延遲才取得\",{\"1\":{\"91\":1}}],[\"而在執行\",{\"1\":{\"245\":1}}],[\"而在\",{\"1\":{\"151\":2,\"257\":1}}],[\"而在右邊的圖當中可以看到如果是固定\",{\"1\":{\"120\":1}}],[\"而在所有的\",{\"1\":{\"116\":1}}],[\"而在這些許多不同的架構下\",{\"1\":{\"116\":1}}],[\"而在測試階段\",{\"1\":{\"112\":1}}],[\"而最一開始的結果其實是還不錯的\",{\"1\":{\"83\":1}}],[\"而一個\",{\"1\":{\"75\":1,\"191\":1}}],[\"而\",{\"1\":{\"50\":1,\"56\":1,\"57\":1,\"93\":1,\"96\":1,\"116\":1,\"131\":2,\"173\":1,\"179\":1,\"194\":1,\"221\":1,\"226\":3,\"227\":1,\"228\":1,\"249\":1,\"257\":1,\"260\":2}}],[\"而半監督式學習困難的點在於雖然對於\",{\"1\":{\"50\":1}}],[\"而被提出的\",{\"1\":{\"50\":1}}],[\"而是給了參數\",{\"1\":{\"157\":1}}],[\"而是另外定義了一個\",{\"1\":{\"154\":1}}],[\"而是自駕車車體\",{\"1\":{\"83\":1}}],[\"而是包含了機率的概念\",{\"1\":{\"73\":1}}],[\"而是將\",{\"1\":{\"54\":1}}],[\"而是\",{\"1\":{\"34\":1}}],[\"而已\",{\"1\":{\"28\":1,\"134\":1}}],[\"而隨著\",{\"1\":{\"28\":1}}],[\"而整體而言\",{\"1\":{\"253\":1}}],[\"而整體的\",{\"1\":{\"19\":1}}],[\"而整體\",{\"1\":{\"19\":1}}],[\"並選擇其中機率最高的作為最後的\",{\"1\":{\"254\":1}}],[\"並總結出\",{\"1\":{\"252\":1}}],[\"並給出接下來目標方向\",{\"1\":{\"252\":1}}],[\"並找到雞蛋\",{\"1\":{\"245\":2}}],[\"並行度差的狀況\",{\"1\":{\"226\":1}}],[\"並超越過去的\",{\"1\":{\"170\":1}}],[\"並預測出結果\",{\"1\":{\"132\":1}}],[\"並無法繼續擴充到其他的領域\",{\"1\":{\"94\":1}}],[\"並無法好好只透過一個\",{\"1\":{\"35\":1}}],[\"並\",{\"1\":{\"93\":1}}],[\"並沒有兩方面都顧慮到\",{\"1\":{\"247\":1}}],[\"並沒有得到比\",{\"1\":{\"74\":1}}],[\"並沒有\",{\"1\":{\"63\":1}}],[\"並不是這篇論文的首創\",{\"1\":{\"258\":1}}],[\"並不是所有的\",{\"1\":{\"50\":1}}],[\"並不會都得出\",{\"1\":{\"163\":1}}],[\"並不會有更好的\",{\"1\":{\"142\":1}}],[\"並不會是一個好的選項\",{\"1\":{\"28\":1}}],[\"並不一定要是\",{\"1\":{\"61\":1}}],[\"並且可以很好適應到\",{\"1\":{\"260\":1}}],[\"並且依據\",{\"1\":{\"258\":1}}],[\"並且與各式物件互動\",{\"1\":{\"257\":1}}],[\"並且都是使用\",{\"1\":{\"253\":1}}],[\"並且都比\",{\"1\":{\"181\":1}}],[\"並且只有一位工作人員\",{\"1\":{\"236\":1}}],[\"並且隨後取得了\",{\"1\":{\"235\":1}}],[\"並且這個權重會依照現在選用的\",{\"1\":{\"223\":1}}],[\"並且將\",{\"1\":{\"212\":1}}],[\"並且預期能夠讓\",{\"1\":{\"198\":1}}],[\"並且分成了弱增強\",{\"1\":{\"196\":1}}],[\"並且兩者結合後可以再帶來更高的\",{\"1\":{\"181\":1}}],[\"並且他們之間有一個重疊的區塊\",{\"1\":{\"175\":1}}],[\"並且沒有使用\",{\"1\":{\"155\":1}}],[\"並且達到了很棒的效果\",{\"1\":{\"151\":1}}],[\"並且如果進一步把\",{\"1\":{\"141\":1}}],[\"並且如果將\",{\"1\":{\"138\":1}}],[\"並且對於大的物件也同樣保有更好的結果\",{\"1\":{\"138\":1}}],[\"並且選擇了一個\",{\"1\":{\"133\":1}}],[\"並且進一步透過其他的技巧可以再做得更好\",{\"1\":{\"116\":1}}],[\"並且發現這樣的做法實際上對於訓練有相當好的幫助\",{\"1\":{\"109\":1}}],[\"並且通常會有更好的表現\",{\"1\":{\"107\":1}}],[\"並且做出相對應的\",{\"1\":{\"99\":1}}],[\"並且會使用\",{\"1\":{\"99\":1}}],[\"並且\",{\"1\":{\"97\":1,\"138\":1,\"141\":1}}],[\"並且證明了底下兩個狀況是可以確保收斂\",{\"1\":{\"95\":1}}],[\"並且最終的\",{\"1\":{\"84\":1}}],[\"並且普遍最後的\",{\"1\":{\"82\":1}}],[\"並且也可以觀察到那些比較早開始有所提升的\",{\"1\":{\"82\":1}}],[\"並且也是\",{\"1\":{\"25\":1}}],[\"並且透過觀察訓練過程作者發現到\",{\"1\":{\"76\":1}}],[\"並且有趣的是\",{\"1\":{\"74\":1}}],[\"並且不同\",{\"1\":{\"40\":1}}],[\"並且每個\",{\"1\":{\"35\":1}}],[\"然後只訓練一個\",{\"1\":{\"260\":1}}],[\"然後突然發現我講不出英文\",{\"1\":{\"240\":1}}],[\"然後拿去\",{\"1\":{\"198\":1}}],[\"然後拿到了\",{\"1\":{\"63\":1}}],[\"然後會有一個\",{\"1\":{\"49\":1}}],[\"然後應用在真實的環境當中\",{\"1\":{\"46\":1}}],[\"然後再次相加\",{\"1\":{\"221\":1}}],[\"然後再拿去訓練\",{\"1\":{\"187\":1}}],[\"然後再透過\",{\"1\":{\"157\":1}}],[\"然後再把這些特徵丟去給\",{\"1\":{\"96\":1}}],[\"然後再應用在真實的世界當中\",{\"1\":{\"46\":1}}],[\"然後再讓\",{\"1\":{\"22\":1}}],[\"然後結束這個\",{\"1\":{\"35\":1}}],[\"然後繼續跟環境互動\",{\"1\":{\"32\":1}}],[\"然而很直覺地\",{\"1\":{\"224\":1}}],[\"然而因為\",{\"1\":{\"196\":1}}],[\"然而若觀察倒數第二個\",{\"1\":{\"163\":1}}],[\"然而若直接使用高解析度的圖片\",{\"1\":{\"128\":1}}],[\"然而存在幾個缺點\",{\"1\":{\"155\":1}}],[\"然而使用了\",{\"1\":{\"153\":1}}],[\"然而在現實狀況下往往並不會如此簡單\",{\"1\":{\"149\":1}}],[\"然而由於當前\",{\"1\":{\"245\":1}}],[\"然而由於\",{\"1\":{\"135\":1}}],[\"然而我們會希望每一個地方都能夠精確地考慮\",{\"1\":{\"135\":1}}],[\"然而我們也看到近幾年\",{\"1\":{\"91\":1}}],[\"然而這種做法每次要更新\",{\"1\":{\"194\":1}}],[\"然而這種方法的\",{\"1\":{\"73\":1}}],[\"然而這些研究都並未能夠給出用非線性去學\",{\"1\":{\"95\":1}}],[\"然而\",{\"1\":{\"28\":1,\"76\":1,\"107\":1,\"119\":1,\"153\":1,\"170\":1,\"192\":1}}],[\"從房間陽台往外看\",{\"1\":{\"237\":1}}],[\"從當中切出兩個大小相同的\",{\"1\":{\"175\":1}}],[\"從訓練中的曲線也可以明顯看到\",{\"1\":{\"162\":1}}],[\"從圖片上的結果可以看到結果有了一些提升\",{\"1\":{\"180\":1}}],[\"從圖片上來觀察\",{\"1\":{\"138\":1}}],[\"從圖片當中可以再次觀察到\",{\"1\":{\"142\":1}}],[\"從上表可以觀察到\",{\"1\":{\"142\":1}}],[\"從上面的圖片中也可以觀察到這樣的\",{\"1\":{\"37\":1}}],[\"從生物學的角度來看\",{\"1\":{\"107\":1}}],[\"從\",{\"1\":{\"54\":2,\"134\":2,\"254\":2}}],[\"從這裡可以看出\",{\"1\":{\"260\":1}}],[\"從這裡也可以了解到實際上讓每個\",{\"1\":{\"37\":1}}],[\"從這些觀察當中可以得到兩個待改善的地方\",{\"1\":{\"16\":1}}],[\"從結果可以發現到\",{\"1\":{\"35\":1}}],[\"從式子當中也可以觀察到\",{\"1\":{\"27\":1}}],[\"嘗試機率高\",{\"1\":{\"27\":1}}],[\"嘗試機率低\",{\"1\":{\"27\":1}}],[\"嘗試次數少\",{\"1\":{\"27\":1}}],[\"嘗試次數少的選擇\",{\"1\":{\"27\":1}}],[\"嘗試次數多的選擇\",{\"1\":{\"27\":1}}],[\"更差\",{\"1\":{\"260\":1}}],[\"更詳細的參數設定請參考論文\",{\"1\":{\"259\":1}}],[\"更加密集\",{\"1\":{\"196\":1}}],[\"更細節來說\",{\"1\":{\"133\":1}}],[\"更早看到它\",{\"1\":{\"75\":1}}],[\"更多關於模型選擇的實驗\",{\"1\":{\"74\":1}}],[\"更好的\",{\"1\":{\"141\":1}}],[\"更好的結果\",{\"1\":{\"138\":1,\"143\":1}}],[\"更好的表現\",{\"1\":{\"74\":1}}],[\"更好的成效\",{\"1\":{\"40\":1}}],[\"更好學習\",{\"1\":{\"21\":1}}],[\"更新參數是從\",{\"1\":{\"152\":1}}],[\"更新模型參數\",{\"1\":{\"32\":1}}],[\"更新\",{\"1\":{\"28\":1}}],[\"更高\",{\"1\":{\"27\":1}}],[\"高估的問題消失\",{\"1\":{\"153\":1}}],[\"高估的狀況如底下的綠線\",{\"1\":{\"153\":1}}],[\"高\",{\"1\":{\"27\":5}}],[\"高中接觸了演算法\",{\"1\":{\"0\":1}}],[\"➡️\",{\"1\":{\"27\":8}}],[\"低\",{\"1\":{\"27\":3}}],[\"未知\",{\"1\":{\"27\":1}}],[\"你的目標是要把一個乾淨的雞蛋放進微波爐當中\",{\"1\":{\"245\":1}}],[\"你的失誤\",{\"1\":{\"119\":1}}],[\"你只能考慮時間\",{\"1\":{\"219\":1}}],[\"你是不會知道你現在要輸出\",{\"1\":{\"219\":1}}],[\"你好\",{\"1\":{\"219\":1}}],[\"你可以知道兩個向量在方向上的相似性\",{\"1\":{\"216\":1}}],[\"你知道cross\",{\"1\":{\"204\":1}}],[\"你們整體看起來的表現會很不錯\",{\"1\":{\"119\":1}}],[\"你選擇\",{\"1\":{\"27\":1}}],[\"你分別把這幾個\",{\"1\":{\"26\":1}}],[\"01\",{\"0\":{\"234\":1}}],[\"017\",{\"1\":{\"160\":1}}],[\"0​ifobd\",{\"1\":{\"134\":1}}],[\"0​ifs⋅obd\",{\"1\":{\"134\":1}}],[\"05\",{\"1\":{\"118\":1,\"244\":1}}],[\"0~9\",{\"1\":{\"116\":1}}],[\"02\",{\"1\":{\"63\":1,\"116\":1}}],[\"04\",{\"1\":{\"37\":1}}],[\"000\",{\"1\":{\"116\":1}}],[\"00\",{\"1\":{\"37\":6}}],[\"06\",{\"1\":{\"37\":1}}],[\"0\",{\"1\":{\"27\":1,\"28\":2,\"29\":2,\"34\":3,\"35\":3,\"37\":5,\"63\":1,\"73\":1,\"80\":1,\"83\":1,\"99\":1,\"112\":1,\"113\":1,\"119\":1,\"120\":3,\"131\":2,\"133\":4,\"134\":8,\"142\":2,\"151\":1,\"154\":2,\"162\":1,\"163\":2,\"179\":1,\"191\":1,\"192\":2,\"195\":1,\"216\":1,\"218\":2,\"219\":2,\"222\":1,\"227\":2,\"249\":1,\"257\":3,\"259\":3}}],[\"把時間\",{\"1\":{\"219\":1}}],[\"把所有對應到\",{\"1\":{\"194\":1}}],[\"把四隻腳站立的動物都當成草食類動物一樣\",{\"1\":{\"187\":1}}],[\"把兩個圖片\",{\"1\":{\"51\":1}}],[\"把這個上界當成是它預期的\",{\"1\":{\"27\":1}}],[\"把\",{\"1\":{\"27\":1,\"51\":3,\"53\":1,\"54\":1,\"74\":1,\"154\":1,\"218\":1}}],[\"把輸出結果套入\",{\"1\":{\"10\":1}}],[\"ϵ=10−9\",{\"1\":{\"227\":1}}],[\"ϵj​∈rp\",{\"1\":{\"158\":1}}],[\"ϵj​\",{\"1\":{\"158\":2}}],[\"ϵi​∈rq\",{\"1\":{\"158\":1}}],[\"ϵi​\",{\"1\":{\"158\":1}}],[\"ϵi\",{\"1\":{\"158\":1}}],[\"ϵb∈rq\",{\"1\":{\"158\":1}}],[\"ϵb\",{\"1\":{\"157\":1}}],[\"ϵw∈rq×p\",{\"1\":{\"158\":1}}],[\"ϵw\",{\"1\":{\"157\":1}}],[\"ϵucb​\",{\"1\":{\"29\":1}}],[\"ϵ\",{\"1\":{\"26\":1,\"29\":1,\"97\":1,\"149\":1,\"150\":1,\"151\":3,\"152\":1,\"154\":1,\"157\":1,\"159\":8,\"160\":3,\"163\":2}}],[\"ϵl​=ϵ1+αl−11​\",{\"1\":{\"32\":1}}],[\"ϵl​\",{\"1\":{\"26\":1,\"32\":2}}],[\"中並沒有呈現這樣的結果\",{\"1\":{\"260\":1}}],[\"中擷取的\",{\"1\":{\"260\":1}}],[\"中每個位置的元素依據與其他元素之間的關係\",{\"1\":{\"212\":1}}],[\"中這一點尤其重要\",{\"1\":{\"195\":1}}],[\"中心點越近\",{\"1\":{\"192\":1}}],[\"中隨意挑一筆\",{\"1\":{\"152\":1}}],[\"中調整\",{\"1\":{\"139\":1}}],[\"中提出的\",{\"1\":{\"137\":1}}],[\"中間的差異在於人類會具有跟真實世界相關的知識\",{\"1\":{\"245\":1}}],[\"中間的部分才會有\",{\"1\":{\"134\":1}}],[\"中間還有提供午餐\",{\"1\":{\"6\":1}}],[\"中的元素\",{\"1\":{\"216\":1,\"220\":1}}],[\"中的\",{\"1\":{\"97\":1}}],[\"中文敘述參考\",{\"1\":{\"74\":1}}],[\"中各取圖片\",{\"1\":{\"49\":1}}],[\"中\",{\"1\":{\"26\":1}}],[\"進入\",{\"1\":{\"214\":2,\"231\":1}}],[\"進入決賽\",{\"1\":{\"2\":1}}],[\"進一步訓練\",{\"1\":{\"258\":1}}],[\"進一步提升效能\",{\"1\":{\"142\":1}}],[\"進一步提高\",{\"1\":{\"83\":1}}],[\"進一步產出合併結果\",{\"1\":{\"142\":1}}],[\"進一步去看每個\",{\"1\":{\"139\":1}}],[\"進一步去研究不同大小的\",{\"1\":{\"74\":1}}],[\"進而影響預測的\",{\"1\":{\"192\":1}}],[\"進而影響到結果\",{\"1\":{\"35\":1}}],[\"進而傾向\",{\"1\":{\"192\":1}}],[\"進而避免\",{\"1\":{\"119\":1}}],[\"進而去完成許多複雜的任務\",{\"1\":{\"91\":1}}],[\"進而產生這樣的結果\",{\"1\":{\"83\":1}}],[\"進而提出\",{\"1\":{\"82\":1}}],[\"進而解決這個問題\",{\"1\":{\"51\":1}}],[\"進而使得整體訓練採用的\",{\"1\":{\"26\":1}}],[\"照片是在\",{\"1\":{\"59\":1,\"60\":1}}],[\"照片是在城市當中開車拍下的各種照片\",{\"1\":{\"58\":1}}],[\"照片中有多少人\",{\"1\":{\"5\":1}}],[\"照著這樣的想法\",{\"1\":{\"26\":1}}],[\"具有相同的\",{\"1\":{\"196\":1}}],[\"具有更高的靈活性\",{\"1\":{\"26\":1}}],[\"具體而言有三個部分\",{\"1\":{\"74\":1}}],[\"具體來說就是他們試圖在\",{\"1\":{\"151\":1}}],[\"具體來說如下圖\",{\"1\":{\"34\":1}}],[\"具體來說\",{\"1\":{\"18\":1,\"51\":1,\"219\":1}}],[\"蒐集的\",{\"1\":{\"26\":1}}],[\"蒐集一些\",{\"1\":{\"26\":1}}],[\"加入\",{\"1\":{\"260\":1}}],[\"加總後平均\",{\"1\":{\"194\":1}}],[\"加\",{\"1\":{\"151\":1}}],[\"加上這一項能夠促使模型更好\",{\"1\":{\"155\":1}}],[\"加上總和為\",{\"1\":{\"154\":1}}],[\"加上後會再進一步提升效果\",{\"1\":{\"113\":1}}],[\"加上了一些些的\",{\"1\":{\"216\":1}}],[\"加上了一個\",{\"1\":{\"28\":1}}],[\"加上了\",{\"1\":{\"35\":1,\"112\":1,\"157\":1}}],[\"加上\",{\"1\":{\"35\":1,\"74\":1,\"80\":4,\"109\":3,\"115\":1,\"116\":1,\"118\":1,\"142\":2,\"151\":1,\"221\":1}}],[\"加上不同的偏好\",{\"1\":{\"26\":1}}],[\"加權分數\",{\"1\":{\"9\":1}}],[\"什麼時候該\",{\"1\":{\"26\":1}}],[\"透過與當前\",{\"1\":{\"245\":1}}],[\"透過前面的步驟得到的模型稱為\",{\"1\":{\"197\":1}}],[\"透過如\",{\"1\":{\"191\":1}}],[\"透過儲存\",{\"1\":{\"152\":1}}],[\"透過同時考慮\",{\"1\":{\"132\":1}}],[\"透過一個\",{\"1\":{\"94\":1}}],[\"透過一些方式混在一起\",{\"1\":{\"51\":1}}],[\"透過固定訓練的目標\",{\"1\":{\"93\":1}}],[\"透過產生假想的\",{\"1\":{\"73\":1}}],[\"透過這個模型我們就有辦法給\",{\"1\":{\"50\":1}}],[\"透過上一個\",{\"1\":{\"32\":1}}],[\"透過各自的\",{\"1\":{\"32\":1}}],[\"透過\",{\"1\":{\"29\":1,\"30\":1,\"32\":1,\"38\":1,\"49\":1,\"54\":1,\"97\":1,\"102\":1,\"131\":2,\"132\":2,\"134\":1,\"152\":1,\"154\":1,\"163\":1,\"187\":1,\"211\":1,\"216\":1,\"245\":1}}],[\"透過它決定接下來要使用的\",{\"1\":{\"26\":1}}],[\"透過加上\",{\"1\":{\"26\":1,\"83\":1}}],[\"透過拆開訓練\",{\"1\":{\"25\":1}}],[\"雖然去了不少地方\",{\"1\":{\"239\":1}}],[\"雖然有許多手續要接著做\",{\"1\":{\"238\":1}}],[\"雖然並不是第一次搭飛機\",{\"1\":{\"236\":1}}],[\"雖然\",{\"1\":{\"230\":1}}],[\"雖然這樣的做法會讓模型的困惑度\",{\"1\":{\"227\":1}}],[\"雖然這種做法開始能夠讓\",{\"1\":{\"187\":1}}],[\"雖然後續有一些研究試圖在\",{\"1\":{\"210\":1}}],[\"雖然強大\",{\"1\":{\"210\":1}}],[\"雖然就結果而言他們的\",{\"1\":{\"119\":1}}],[\"雖然已經有\",{\"1\":{\"50\":1}}],[\"雖然在部分的例子當中可以看到這種相關的做法還不錯\",{\"1\":{\"247\":1}}],[\"雖然在\",{\"1\":{\"38\":1}}],[\"雖然兩個模型都會把\",{\"1\":{\"25\":1}}],[\"雖然說在比賽之前我們的想法是\",{\"1\":{\"6\":1}}],[\"雖然說是黑客松\",{\"1\":{\"6\":1}}],[\"於是就接受了喝水道水\",{\"1\":{\"237\":1}}],[\"於是我們的\",{\"1\":{\"220\":1}}],[\"於是我們再加上一個\",{\"1\":{\"134\":1}}],[\"於是他們定義了底下的\",{\"1\":{\"152\":1}}],[\"於是把每個點跟\",{\"1\":{\"134\":1}}],[\"於是對於\",{\"1\":{\"112\":1}}],[\"於是乎最後的整體\",{\"1\":{\"76\":1}}],[\"於是作者嘗試修改\",{\"1\":{\"74\":1}}],[\"於是\",{\"1\":{\"25\":1,\"73\":1,\"74\":1,\"75\":1}}],[\"為何\",{\"1\":{\"260\":1}}],[\"為例來說明\",{\"1\":{\"247\":1}}],[\"為什麼我們要選擇\",{\"1\":{\"226\":1}}],[\"為甚麼可以直接拿去\",{\"1\":{\"198\":1}}],[\"為了比較的公平性採用\",{\"1\":{\"258\":2}}],[\"為了確保最終輸出結果的品質\",{\"1\":{\"252\":1}}],[\"為了研究所推甄也是煞費苦心\",{\"1\":{\"239\":1}}],[\"為了買各種生活用品\",{\"1\":{\"237\":1}}],[\"為了搭筑波大學的專車回去\",{\"1\":{\"236\":1}}],[\"為了避免這個問題\",{\"1\":{\"219\":1}}],[\"為了避免這個情況\",{\"1\":{\"218\":1}}],[\"為了避免\",{\"1\":{\"197\":1}}],[\"為了避免所謂的\",{\"1\":{\"196\":1}}],[\"為了進一步去釐清這樣的做法為什麼是可行\",{\"1\":{\"163\":1}}],[\"為了同時保有\",{\"1\":{\"133\":1}}],[\"為了測試\",{\"1\":{\"115\":1}}],[\"為當前\",{\"1\":{\"25\":1}}],[\"為目標\",{\"1\":{\"25\":1}}],[\"為\",{\"1\":{\"25\":3,\"93\":1,\"99\":1,\"134\":1,\"173\":1,\"227\":1,\"254\":1,\"260\":1}}],[\"表示任務完成率\",{\"1\":{\"257\":1}}],[\"表示是否完成\",{\"1\":{\"257\":1}}],[\"表示前面產出的\",{\"1\":{\"253\":1}}],[\"表示讓模型產出總結的\",{\"1\":{\"252\":1}}],[\"表示當前\",{\"1\":{\"194\":1}}],[\"表示第\",{\"1\":{\"193\":1}}],[\"表示在\",{\"1\":{\"173\":2}}],[\"表示在時間\",{\"1\":{\"93\":1}}],[\"表示著在當前這個\",{\"1\":{\"154\":1}}],[\"表示要訓練的模型\",{\"1\":{\"131\":1}}],[\"表示每個\",{\"1\":{\"112\":1}}],[\"表示不影響\",{\"1\":{\"99\":1}}],[\"表示不同的\",{\"1\":{\"26\":1}}],[\"表示\",{\"1\":{\"25\":1,\"37\":1,\"73\":8,\"112\":6,\"131\":5,\"133\":1,\"173\":2,\"174\":3,\"175\":1,\"190\":1,\"191\":1,\"193\":2,\"249\":5,\"251\":4,\"252\":4,\"253\":5,\"254\":3}}],[\"表示從\",{\"1\":{\"25\":1}}],[\"表示使用的是哪一個\",{\"1\":{\"25\":1}}],[\"個步驟\",{\"1\":{\"254\":1}}],[\"個特徵\",{\"1\":{\"174\":1}}],[\"個資料\",{\"1\":{\"173\":4}}],[\"個數值\",{\"1\":{\"158\":1}}],[\"個主題\",{\"1\":{\"118\":1}}],[\"個講者的演講資料\",{\"1\":{\"117\":1}}],[\"個當中\",{\"1\":{\"116\":1}}],[\"個類別\",{\"1\":{\"116\":1}}],[\"個小時\",{\"1\":{\"85\":1}}],[\"個共通的\",{\"1\":{\"73\":1,\"190\":1}}],[\"個的平均\",{\"1\":{\"62\":1,\"179\":1}}],[\"個平均跟\",{\"1\":{\"62\":1,\"179\":1}}],[\"個比較困難的遊戲當中\",{\"1\":{\"37\":1}}],[\"個比較難的遊戲當中測試的結果\",{\"1\":{\"36\":1}}],[\"個\",{\"1\":{\"25\":1,\"27\":1,\"32\":1,\"35\":1,\"60\":1,\"62\":2,\"76\":1,\"97\":1,\"99\":2,\"111\":1,\"158\":1,\"162\":1,\"179\":2,\"220\":1,\"254\":2}}],[\"個遊戲場景\",{\"1\":{\"16\":1}}],[\"個遊戲是\",{\"1\":{\"16\":1}}],[\"個遊戲當中有\",{\"1\":{\"16\":1}}],[\"因為我們現在對每一個\",{\"1\":{\"201\":1}}],[\"因為部分\",{\"1\":{\"179\":1}}],[\"因為同樣都是對應到相同的\",{\"1\":{\"175\":1}}],[\"因為缺少了\",{\"1\":{\"173\":1}}],[\"因為缺乏對他人的理解\",{\"1\":{\"47\":1}}],[\"因為無法好好擷取特徵\",{\"1\":{\"128\":1}}],[\"因為\",{\"1\":{\"116\":1,\"195\":1,\"196\":1}}],[\"因為遊戲當中的雷射會跑很快\",{\"1\":{\"99\":1}}],[\"因為過去的經驗即便在現實狀況改變仍然有大影響力\",{\"1\":{\"28\":1}}],[\"因為是一次更新\",{\"1\":{\"25\":1}}],[\"因此計算了每個方法在各個環境當中執行的\",{\"1\":{\"260\":1}}],[\"因此也延伸出如\",{\"1\":{\"258\":1}}],[\"因此也不太適合將\",{\"1\":{\"181\":1}}],[\"因此定義新的\",{\"1\":{\"253\":1}}],[\"因此本身就已經包含順序的特徵\",{\"1\":{\"224\":1}}],[\"因此乘上一個\",{\"1\":{\"223\":1}}],[\"因此適合用來教\",{\"1\":{\"196\":1}}],[\"因此透過\",{\"1\":{\"196\":1}}],[\"因此只會使用到\",{\"1\":{\"174\":1}}],[\"因此不太需要考慮上述的\",{\"1\":{\"160\":1}}],[\"因此不少組別在實際進到\",{\"1\":{\"6\":1}}],[\"因此上述的\",{\"1\":{\"159\":1}}],[\"因此在這裡我們會再加上位置的資訊\",{\"1\":{\"224\":1}}],[\"因此在數據上\",{\"1\":{\"179\":1}}],[\"因此在測試階段的時候這兩個部分是不會參與的\",{\"1\":{\"176\":1}}],[\"因此在參數上也就包含了兩項\",{\"1\":{\"155\":1}}],[\"因此在評分上我們可以考慮\",{\"1\":{\"116\":1}}],[\"因此細節上是還會對\",{\"1\":{\"154\":1}}],[\"因此仍然沒有解決問題\",{\"1\":{\"149\":1}}],[\"因此驗證了假設是正確的\",{\"1\":{\"139\":1}}],[\"因此當訓練時與預測時採用的解析度大小相同會是最理想的\",{\"1\":{\"135\":1}}],[\"因此會在心中先預想解決問題的方式\",{\"1\":{\"245\":1}}],[\"因此會是\",{\"1\":{\"226\":2}}],[\"因此會讓每個\",{\"1\":{\"220\":1}}],[\"因此會再進一步加上\",{\"1\":{\"216\":1}}],[\"因此會包含了\",{\"1\":{\"173\":1}}],[\"因此會使用\",{\"1\":{\"173\":1}}],[\"因此會希望能夠取得所有點的\",{\"1\":{\"135\":1}}],[\"因此會比較能夠好好評估\",{\"1\":{\"34\":1}}],[\"因此我們會訓練出一個\",{\"1\":{\"131\":1}}],[\"因此對於上一層的\",{\"1\":{\"112\":1}}],[\"因此作者製作了一個\",{\"1\":{\"252\":1}}],[\"因此作者首先透過\",{\"1\":{\"251\":1}}],[\"因此作者希望產出另一組\",{\"1\":{\"251\":1}}],[\"因此作者依照上面的想法\",{\"1\":{\"245\":1}}],[\"因此作者改用一個\",{\"1\":{\"194\":1}}],[\"因此作者改成\",{\"1\":{\"100\":1}}],[\"因此作者提出的方法是將\",{\"1\":{\"192\":1}}],[\"因此作者採用相同解析度大小\",{\"1\":{\"135\":1}}],[\"因此作者認為選擇\",{\"1\":{\"120\":1}}],[\"因此作者認為這是跟圖片被\",{\"1\":{\"82\":1}}],[\"因此作者認為\",{\"1\":{\"37\":1}}],[\"因此評估一個\",{\"1\":{\"100\":1}}],[\"因此這裡也統計了一個\",{\"1\":{\"260\":1}}],[\"因此這裡的拉近只會針對\",{\"1\":{\"76\":1}}],[\"因此這類的結果很難直接應用到其他的環境當中\",{\"1\":{\"247\":1}}],[\"因此這一部分會針對記憶體用量的部分去做討論\",{\"1\":{\"141\":1}}],[\"因此這一部分就是要驗這這個假設\",{\"1\":{\"139\":1}}],[\"因此這一篇論文提出一個方法試圖去消除\",{\"1\":{\"149\":1}}],[\"因此這一篇\",{\"1\":{\"70\":1}}],[\"因此這時候\",{\"1\":{\"51\":1}}],[\"因此\",{\"1\":{\"25\":1,\"28\":1,\"46\":1,\"76\":1,\"83\":1,\"97\":1,\"113\":1,\"153\":1,\"157\":1,\"163\":1,\"173\":2,\"218\":1,\"226\":1}}],[\"時決定製作一個\",{\"1\":{\"260\":1}}],[\"時考慮下一個\",{\"1\":{\"254\":1}}],[\"時產出\",{\"1\":{\"253\":1}}],[\"時常會產生怪異的行為\",{\"1\":{\"245\":1}}],[\"時常出現兩個問題\",{\"1\":{\"245\":1}}],[\"時常我們會訓練在合成資料上\",{\"1\":{\"46\":1}}],[\"時於\",{\"1\":{\"155\":1}}],[\"時的\",{\"1\":{\"128\":1}}],[\"時訓練不佳的問題\",{\"1\":{\"102\":1}}],[\"時間\",{\"0\":{\"158\":1},\"1\":{\"25\":2}}],[\"時\",{\"1\":{\"25\":1,\"218\":1}}],[\"跟目標\",{\"1\":{\"253\":1}}],[\"跟目標相同\",{\"1\":{\"73\":1}}],[\"跟學長買的腳踏車\",{\"1\":{\"237\":1}}],[\"跟每個\",{\"1\":{\"193\":1}}],[\"跟對\",{\"1\":{\"112\":1}}],[\"跟幾個\",{\"1\":{\"101\":1}}],[\"跟環境的互動過程當中的\",{\"1\":{\"97\":1}}],[\"跟這一篇\",{\"1\":{\"96\":1}}],[\"跟其他架構相比\",{\"1\":{\"84\":1}}],[\"跟\",{\"1\":{\"25\":1,\"37\":1,\"51\":2,\"54\":1,\"56\":1,\"101\":1,\"157\":1,\"162\":2,\"192\":1,\"221\":1,\"224\":1,\"236\":1,\"260\":1}}],[\"跟過去自己想像當中在無塵室裏面處理晶圓的那種印象是完全不同\",{\"1\":{\"11\":1}}],[\"相信可以給你更多的啟發\",{\"1\":{\"216\":1}}],[\"相比更加重要\",{\"1\":{\"139\":1}}],[\"相比多了\",{\"1\":{\"138\":2}}],[\"相較之下有更大的影響\",{\"1\":{\"142\":1}}],[\"相較之下\",{\"1\":{\"84\":1,\"91\":1,\"151\":1,\"179\":1}}],[\"相鄰而導致的誤判被稱為\",{\"1\":{\"53\":1}}],[\"相差過大\",{\"1\":{\"46\":1}}],[\"相對的\",{\"1\":{\"35\":1}}],[\"相同\",{\"1\":{\"32\":1,\"73\":1,\"134\":1,\"173\":1,\"221\":1}}],[\"相同的結果\",{\"1\":{\"179\":1}}],[\"相同的\",{\"1\":{\"25\":1,\"116\":1}}],[\"相當重要的問題\",{\"1\":{\"16\":1}}],[\"相當佩服\",{\"1\":{\"10\":1}}],[\"兩者沒有太大的差異\",{\"1\":{\"226\":1}}],[\"兩者差異只在於使用的\",{\"1\":{\"196\":1}}],[\"兩者去作出判斷\",{\"1\":{\"134\":1}}],[\"兩部分影響程度的參數\",{\"1\":{\"154\":1}}],[\"兩種方法\",{\"1\":{\"116\":1}}],[\"兩個狀況\",{\"1\":{\"249\":1}}],[\"兩個可以同時處理\",{\"1\":{\"192\":1}}],[\"兩個參數的\",{\"1\":{\"155\":1}}],[\"兩個部分\",{\"1\":{\"74\":1,\"132\":1,\"191\":1}}],[\"兩個模型都是使用\",{\"1\":{\"25\":1}}],[\"兩個\",{\"1\":{\"25\":1,\"49\":1}}],[\"兩篇\",{\"1\":{\"16\":1}}],[\"首先可以看到無論採用的\",{\"1\":{\"260\":1}}],[\"首先會產出\",{\"1\":{\"254\":1}}],[\"首先我們在某個機器上掃描了指紋\",{\"1\":{\"236\":1}}],[\"首先是對每個\",{\"1\":{\"227\":1}}],[\"首先在\",{\"1\":{\"226\":1}}],[\"首先將\",{\"1\":{\"221\":1}}],[\"首先將圖片經過裁切出一部分\",{\"1\":{\"133\":1}}],[\"首先看到\",{\"1\":{\"179\":1}}],[\"首先看到上面的表格\",{\"1\":{\"80\":1}}],[\"首先把\",{\"1\":{\"162\":1}}],[\"首先說明這篇\",{\"1\":{\"112\":1}}],[\"首先\",{\"1\":{\"74\":1,\"245\":1}}],[\"首先針對\",{\"1\":{\"25\":1}}],[\"首先定義從\",{\"1\":{\"21\":1}}],[\"問題上\",{\"1\":{\"170\":1}}],[\"問題的解法是把所有可能的\",{\"1\":{\"107\":1}}],[\"問題在於不同的\",{\"1\":{\"47\":1}}],[\"問題\",{\"1\":{\"23\":1,\"27\":1,\"191\":1}}],[\"問題描述\",{\"0\":{\"16\":1,\"47\":1,\"70\":1,\"91\":1,\"107\":1,\"128\":1,\"149\":1,\"170\":1,\"187\":1,\"209\":1,\"245\":1},\"1\":{\"70\":1}}],[\"丟在\",{\"1\":{\"22\":1}}],[\"丟進去訓練還是出現\",{\"1\":{\"6\":1}}],[\"∣au​∣\",{\"1\":{\"254\":3}}],[\"∣​\",{\"1\":{\"253\":3}}],[\"∣b∣\",{\"1\":{\"252\":1}}],[\"∣st+i​\",{\"1\":{\"160\":1}}],[\"∣st​\",{\"1\":{\"20\":1}}],[\"∣x∣​\",{\"1\":{\"158\":1}}],[\"∣z∣+1+ϵ\",{\"1\":{\"21\":1}}],[\"∣z∣+1​−1\",{\"1\":{\"21\":1}}],[\"zt\",{\"1\":{\"196\":3}}],[\"zt​∥zt\",{\"1\":{\"196\":1}}],[\"zt​\",{\"1\":{\"196\":2}}],[\"zhejiang\",{\"1\":{\"244\":1}}],[\"zheng\",{\"1\":{\"169\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3}}],[\"zhedong\",{\"1\":{\"169\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3}}],[\"zhang1\",{\"1\":{\"186\":1}}],[\"zhang\",{\"1\":{\"47\":1,\"186\":2,\"187\":3,\"192\":3,\"201\":3,\"244\":1,\"250\":1,\"260\":7}}],[\"zero\",{\"1\":{\"157\":2,\"247\":3}}],[\"ziyu\",{\"1\":{\"152\":1,\"154\":1}}],[\"zi\",{\"1\":{\"112\":4}}],[\"zurich\",{\"1\":{\"69\":1,\"127\":1}}],[\"zou\",{\"1\":{\"50\":1}}],[\"z\",{\"1\":{\"21\":4,\"112\":1}}],[\"∀z∈r\",{\"1\":{\"21\":2}}],[\"+lcet​\",{\"1\":{\"197\":1}}],[\"+\",{\"1\":{\"179\":2,\"194\":1,\"254\":1}}],[\"+p1​​\",{\"1\":{\"160\":1}}],[\"+p3​​\",{\"1\":{\"160\":1}}],[\"+​a\",{\"1\":{\"154\":1}}],[\"+a\",{\"1\":{\"154\":1}}],[\"+λlv\",{\"1\":{\"155\":1}}],[\"+λd​lce​\",{\"1\":{\"134\":2}}],[\"+λh\",{\"1\":{\"54\":1}}],[\"+ζ\",{\"1\":{\"134\":1}}],[\"+bi\",{\"1\":{\"112\":2}}],[\"+0\",{\"1\":{\"63\":1}}],[\"+1\",{\"1\":{\"63\":2}}],[\"+2\",{\"1\":{\"63\":1}}],[\"+βkl\",{\"1\":{\"197\":1}}],[\"+βlce​\",{\"1\":{\"195\":1}}],[\"+βi=0∑k​∇θπ​​h\",{\"1\":{\"155\":1,\"160\":1}}],[\"+βnk−1​\",{\"1\":{\"27\":1,\"28\":1,\"29\":1}}],[\"+βj​q\",{\"1\":{\"25\":1}}],[\"+ϵz=sgn\",{\"1\":{\"21\":1}}],[\"+t≥0∑​γt\",{\"1\":{\"21\":1}}],[\"+s=t∑t+k−1​γs−t\",{\"1\":{\"21\":1}}],[\"版本\",{\"1\":{\"21\":1}}],[\"改成透過\",{\"1\":{\"195\":1}}],[\"改成\",{\"1\":{\"21\":1}}],[\"−∞\",{\"1\":{\"216\":1}}],[\"−∥f\",{\"1\":{\"196\":2}}],[\"−∥f~​\",{\"1\":{\"193\":2,\"196\":2}}],[\"−η\",{\"1\":{\"193\":2,\"196\":4}}],[\"−scorerandom​scoreagent​−scorebaseline​​\",{\"1\":{\"162\":1}}],[\"−pv​ˉ​mixloghcls​\",{\"1\":{\"173\":1}}],[\"−pv​ˉ​tloghcls​\",{\"1\":{\"173\":1}}],[\"−pus​loghcls​\",{\"1\":{\"173\":1}}],[\"−p1​​\",{\"1\":{\"160\":1}}],[\"−p3​​\",{\"1\":{\"160\":1}}],[\"−∣a∣1​a\",{\"1\":{\"154\":1}}],[\"−v\",{\"1\":{\"154\":1}}],[\"−trt\",{\"1\":{\"93\":1}}],[\"−t^r\",{\"1\":{\"25\":1}}],[\"−fθ​\",{\"1\":{\"76\":1}}],[\"−1\",{\"1\":{\"21\":1,\"99\":1}}],[\"−h−1q\",{\"1\":{\"21\":1}}],[\"−yt​^​\",{\"1\":{\"21\":1}}],[\"−q\",{\"1\":{\"21\":1,\"152\":1,\"154\":1,\"159\":4}}],[\"θ=defμ+σ⊙ϵ\",{\"1\":{\"157\":1,\"163\":1}}],[\"θv\",{\"1\":{\"155\":3}}],[\"θv​\",{\"1\":{\"154\":1,\"155\":5,\"160\":3}}],[\"θπ​\",{\"1\":{\"155\":5,\"160\":3}}],[\"θa​\",{\"1\":{\"154\":1}}],[\"θ~=θ+n\",{\"1\":{\"151\":1}}],[\"θt​\",{\"1\":{\"73\":1}}],[\"θl​\",{\"1\":{\"32\":1}}],[\"θi−1​\",{\"1\":{\"93\":2}}],[\"θi​\",{\"1\":{\"93\":3}}],[\"θi\",{\"1\":{\"25\":2}}],[\"θe∪θi\",{\"1\":{\"25\":1}}],[\"θe\",{\"1\":{\"25\":2}}],[\"θ\",{\"1\":{\"21\":2,\"25\":6,\"54\":1,\"152\":2,\"153\":2,\"154\":8,\"155\":3,\"157\":3,\"159\":5,\"249\":1}}],[\"θ−\",{\"1\":{\"21\":2,\"25\":3,\"152\":2,\"153\":3,\"154\":1,\"159\":2}}],[\"​∈au​\",{\"1\":{\"254\":1}}],[\"​∈s\",{\"1\":{\"253\":2}}],[\"​∈rohc​​×owc​​\",{\"1\":{\"134\":1}}],[\"​∣u\",{\"1\":{\"253\":4}}],[\"​∼d\",{\"1\":{\"253\":1}}],[\"​∼bernoulli\",{\"1\":{\"112\":1}}],[\"​exp\",{\"1\":{\"193\":1,\"196\":2}}],[\"​otherwise​\",{\"1\":{\"191\":1}}],[\"​if\",{\"1\":{\"191\":1}}],[\"​log\",{\"1\":{\"191\":1}}],[\"​logπ\",{\"1\":{\"155\":1}}],[\"​loggθ​\",{\"1\":{\"73\":2}}],[\"​bd\",{\"1\":{\"133\":1}}],[\"​bc\",{\"1\":{\"133\":1}}],[\"​gϕ​\",{\"1\":{\"131\":1}}],[\"​c=1∑c​qij​yijc​logζ\",{\"1\":{\"131\":1}}],[\"​j=1∑w\",{\"1\":{\"131\":1}}],[\"​y~​\",{\"1\":{\"112\":2}}],[\"​y\",{\"1\":{\"112\":1}}],[\"​yi\",{\"1\":{\"112\":2}}],[\"​∑j=1hf​×wf​​d\",{\"1\":{\"76\":1}}],[\"​⋅\",{\"1\":{\"76\":1}}],[\"​pt\",{\"1\":{\"73\":1,\"191\":1,\"192\":1,\"195\":1}}],[\"​hϕ​\",{\"1\":{\"73\":2}}],[\"​​=wi\",{\"1\":{\"112\":2}}],[\"​​\",{\"1\":{\"27\":1,\"28\":1,\"76\":1,\"155\":2}}],[\"​​​∀0≤k≤n−1∀n≤k≤k−1​\",{\"1\":{\"27\":1,\"28\":1}}],[\"​=sin\",{\"1\":{\"224\":1}}],[\"​=sgn\",{\"1\":{\"21\":1}}],[\"​==1\",{\"1\":{\"194\":2}}],[\"​=∑k\",{\"1\":{\"193\":1,\"196\":2}}],[\"​=∑j​mthings\",{\"1\":{\"76\":1}}],[\"​=ξ\",{\"1\":{\"192\":1,\"195\":1}}],[\"​=−i=1∑∣y∣​logπϕ​\",{\"1\":{\"253\":1}}],[\"​=−e\",{\"1\":{\"160\":1}}],[\"​=−eπ\",{\"1\":{\"155\":1,\"160\":1}}],[\"​=−j=1∑h×w​c=1∑c​qt\",{\"1\":{\"73\":1}}],[\"​=−j=1∑h×w​c=1∑c​ys\",{\"1\":{\"73\":1}}],[\"​=lce​\",{\"1\":{\"131\":1}}],[\"​=pw\",{\"1\":{\"112\":1}}],[\"​=f\",{\"1\":{\"112\":2}}],[\"​=cos\",{\"1\":{\"224\":1}}],[\"​=c\",{\"1\":{\"76\":1}}],[\"​=h⋅w∑j=1h×w​\",{\"1\":{\"73\":1}}],[\"​=\",{\"1\":{\"73\":1,\"76\":1,\"191\":1,\"253\":1,\"254\":1}}],[\"​=m=max\",{\"1\":{\"28\":1}}],[\"​=m=0∑k−1​1\",{\"1\":{\"27\":1}}],[\"​=nk​\",{\"1\":{\"27\":1,\"28\":1}}],[\"​−1​\",{\"1\":{\"21\":1}}],[\"​\",{\"1\":{\"21\":3,\"73\":11,\"75\":3,\"76\":5,\"93\":3,\"112\":5,\"131\":2,\"134\":5,\"154\":1,\"155\":3,\"158\":1,\"160\":1,\"173\":1,\"174\":1,\"175\":1,\"191\":4,\"192\":5,\"193\":1,\"194\":1,\"195\":1,\"196\":5,\"197\":2,\"220\":1,\"224\":1,\"253\":8,\"254\":16}}],[\"δth​=rt​+γa∈a∑​π\",{\"1\":{\"21\":1}}],[\"δth​\",{\"1\":{\"21\":1}}],[\"δt​cs​​=rt​+γa∈a∑​π\",{\"1\":{\"21\":1}}],[\"δs​\",{\"1\":{\"21\":1}}],[\"=−j=1∑∣x\",{\"1\":{\"253\":1}}],[\"=−j=1∑∣x∣​\",{\"1\":{\"253\":1}}],[\"=−eτw​∼d​\",{\"1\":{\"253\":1}}],[\"=−i=1∑h\",{\"1\":{\"131\":1}}],[\"=t=0∏n​πθ​\",{\"1\":{\"249\":1}}],[\"=t∑t​γt\",{\"1\":{\"93\":1}}],[\"=max\",{\"1\":{\"222\":1}}],[\"=softmax\",{\"1\":{\"216\":1,\"218\":2}}],[\"=sgn\",{\"1\":{\"158\":1}}],[\"=∑xt​∈xt​​∑i​1\",{\"1\":{\"194\":1}}],[\"=∑c\",{\"1\":{\"75\":1}}],[\"=o2​\",{\"1\":{\"175\":1}}],[\"=c\",{\"1\":{\"174\":1}}],[\"=f\",{\"1\":{\"158\":1}}],[\"=∇e\",{\"1\":{\"157\":1}}],[\"=lπ\",{\"1\":{\"155\":1}}],[\"=i=0∑k​eπ\",{\"1\":{\"155\":1,\"160\":1}}],[\"=v\",{\"1\":{\"154\":2}}],[\"=r\",{\"1\":{\"112\":1}}],[\"=x\",{\"1\":{\"112\":1}}],[\"=πmax​e\",{\"1\":{\"93\":1}}],[\"=1\",{\"1\":{\"254\":1}}],[\"=1=nni​​​\",{\"1\":{\"254\":1}}],[\"=1∑c​ys\",{\"1\":{\"76\":1}}],[\"=1c​e1−fc\",{\"1\":{\"75\":1}}],[\"=∥fimagenet​\",{\"1\":{\"76\":1}}],[\"=b=0∑b−1​s=t∑t+h−1​\",{\"1\":{\"25\":1}}],[\"=arga∈amax​q\",{\"1\":{\"25\":1}}],[\"=\",{\"1\":{\"21\":1,\"73\":1,\"131\":1,\"134\":2,\"253\":2,\"254\":2}}],[\"=λmin\",{\"1\":{\"21\":1}}],[\"=q\",{\"1\":{\"21\":1,\"25\":1,\"154\":1}}],[\"=exp\",{\"1\":{\"174\":1}}],[\"=es\",{\"1\":{\"93\":1}}],[\"=eμ​\",{\"1\":{\"21\":1}}],[\"=e\",{\"1\":{\"20\":1,\"54\":1,\"152\":1,\"154\":1,\"157\":2,\"159\":4,\"160\":1,\"173\":1}}],[\"定義有加上了\",{\"1\":{\"254\":1}}],[\"定義了有哪些\",{\"1\":{\"249\":1}}],[\"定義底下的平均\",{\"1\":{\"163\":1}}],[\"定義一個\",{\"1\":{\"75\":1}}],[\"定義\",{\"1\":{\"21\":1,\"76\":2,\"93\":1,\"174\":1,\"175\":1}}],[\"x<j\",{\"1\":{\"253\":1}}],[\"x<j​\",{\"1\":{\"253\":1}}],[\"xj\",{\"1\":{\"253\":3}}],[\"xj​\",{\"1\":{\"253\":1}}],[\"xj​∣u\",{\"1\":{\"253\":1}}],[\"xj​∈a\",{\"1\":{\"253\":2}}],[\"x∣x\",{\"1\":{\"253\":1}}],[\"x∣x∣​\",{\"1\":{\"253\":1}}],[\"x1\",{\"1\":{\"253\":1}}],[\"x1​\",{\"1\":{\"253\":1}}],[\"x=\",{\"1\":{\"253\":1}}],[\"xw1​+b1​\",{\"1\":{\"222\":1}}],[\"xvmix​\",{\"1\":{\"173\":2}}],[\"xvt​\",{\"1\":{\"173\":2}}],[\"x+\",{\"1\":{\"157\":1}}],[\"xus​\",{\"1\":{\"173\":2}}],[\"xu\",{\"1\":{\"144\":1}}],[\"xd​=xc\",{\"1\":{\"133\":1}}],[\"xd​\",{\"1\":{\"133\":2}}],[\"xc​=ζ\",{\"1\":{\"133\":1}}],[\"xc​\",{\"1\":{\"133\":3,\"134\":1}}],[\"xc\",{\"1\":{\"133\":3}}],[\"xlrt​\",{\"1\":{\"131\":1}}],[\"xlrt​=ζ\",{\"1\":{\"131\":1}}],[\"xlrs​\",{\"1\":{\"131\":1}}],[\"xhrt​\",{\"1\":{\"131\":1}}],[\"xhrt\",{\"1\":{\"131\":2}}],[\"xhrs\",{\"1\":{\"131\":2}}],[\"xm​\",{\"1\":{\"54\":4}}],[\"xs=\",{\"1\":{\"131\":1}}],[\"xs\",{\"1\":{\"73\":2,\"76\":3}}],[\"xs​=\",{\"1\":{\"73\":1,\"190\":1}}],[\"xs​\",{\"1\":{\"54\":5,\"190\":1,\"191\":1}}],[\"xsb​\",{\"1\":{\"25\":3}}],[\"xa​\",{\"1\":{\"51\":2}}],[\"x\",{\"1\":{\"21\":2,\"25\":6,\"35\":1,\"158\":2,\"222\":1,\"237\":1,\"238\":1,\"253\":1}}],[\"xt=\",{\"1\":{\"131\":1}}],[\"xt\",{\"1\":{\"73\":4,\"191\":1}}],[\"xt+1​\",{\"1\":{\"21\":3,\"32\":1,\"214\":1}}],[\"xt​=\",{\"1\":{\"73\":1,\"190\":1}}],[\"xt​\",{\"1\":{\"21\":8,\"32\":2,\"54\":3,\"190\":1,\"191\":1,\"193\":4,\"194\":1,\"196\":8,\"214\":1}}],[\"x2​\",{\"1\":{\"253\":1}}],[\"x2\",{\"1\":{\"5\":1,\"253\":1}}],[\"τw\",{\"1\":{\"253\":8}}],[\"τw​∣u\",{\"1\":{\"253\":4}}],[\"τw​\",{\"1\":{\"251\":2,\"253\":5}}],[\"τl​\",{\"1\":{\"251\":2}}],[\"τ∣u\",{\"1\":{\"249\":1}}],[\"τ∈n∗\",{\"1\":{\"28\":1}}],[\"τ=\",{\"1\":{\"21\":1}}],[\"τ\",{\"1\":{\"21\":1,\"28\":8,\"29\":2,\"73\":1,\"174\":1,\"193\":3,\"196\":4,\"249\":3}}],[\"πϕ​\",{\"1\":{\"253\":6}}],[\"πθ​\",{\"1\":{\"249\":2,\"253\":6}}],[\"π​q\",{\"1\":{\"25\":1}}],[\"π\",{\"1\":{\"21\":2,\"25\":4,\"93\":2,\"155\":3,\"160\":3,\"249\":1}}],[\"μi\",{\"1\":{\"160\":2}}],[\"μ+σ⊙ξ\",{\"1\":{\"157\":1}}],[\"μ+σ⊙ϵ\",{\"1\":{\"157\":1}}],[\"μb+σb⊙ϵb\",{\"1\":{\"157\":1}}],[\"μw+σw⊙ϵw\",{\"1\":{\"157\":1}}],[\"μ^​k​\",{\"1\":{\"27\":2,\"28\":2}}],[\"μ\",{\"1\":{\"21\":3,\"25\":3,\"157\":1,\"163\":1}}],[\"演算法\",{\"1\":{\"21\":1}}],[\"計算相似度的方法有許多種\",{\"1\":{\"216\":1}}],[\"計算方式如下\",{\"1\":{\"197\":1}}],[\"計算上負擔過大\",{\"1\":{\"194\":1}}],[\"計算上只會考慮\",{\"1\":{\"173\":1}}],[\"計算的是\",{\"1\":{\"193\":1}}],[\"計算分別如下\",{\"1\":{\"155\":1}}],[\"計算\",{\"0\":{\"194\":1,\"195\":1},\"1\":{\"21\":1,\"32\":1}}],[\"選擇一個\",{\"1\":{\"192\":1}}],[\"選擇採用\",{\"1\":{\"56\":1}}],[\"選擇中最大的\",{\"1\":{\"37\":1}}],[\"選擇出一組\",{\"1\":{\"32\":1}}],[\"選擇出現傾向\",{\"1\":{\"26\":1}}],[\"選擇其中最大的當成這次的選擇\",{\"1\":{\"27\":1}}],[\"選擇\",{\"1\":{\"26\":1,\"32\":1,\"254\":1}}],[\"選擇的分布\",{\"1\":{\"20\":1}}],[\"選大一些\",{\"1\":{\"20\":1}}],[\"選小一些\",{\"1\":{\"20\":1}}],[\"小川\",{\"1\":{\"234\":1}}],[\"小時\",{\"1\":{\"227\":1,\"259\":1}}],[\"小輸\",{\"1\":{\"179\":1}}],[\"小物件的預測更加精確\",{\"1\":{\"139\":1}}],[\"小\",{\"1\":{\"20\":1,\"28\":1,\"220\":1}}],[\"傾向在\",{\"1\":{\"187\":1}}],[\"傾向\",{\"1\":{\"20\":2}}],[\"γ=\",{\"1\":{\"259\":1}}],[\"γ=0\",{\"1\":{\"37\":1}}],[\"γ⋅pagent​\",{\"1\":{\"254\":1}}],[\"γ2​=0\",{\"1\":{\"196\":1}}],[\"γ1​=10\",{\"1\":{\"196\":1}}],[\"γj​\",{\"1\":{\"26\":1,\"32\":1}}],[\"γ\",{\"1\":{\"20\":4,\"29\":1,\"34\":1,\"37\":1,\"93\":1,\"254\":1}}],[\"qiao\",{\"1\":{\"244\":1,\"250\":1,\"260\":7}}],[\"qi​​\",{\"1\":{\"216\":2,\"221\":1}}],[\"qi​\",{\"1\":{\"155\":1}}],[\"qi​−v\",{\"1\":{\"155\":1,\"160\":2}}],[\"qwiq​\",{\"1\":{\"220\":1}}],[\"qkt\",{\"1\":{\"216\":1,\"218\":1}}],[\"quickly\",{\"1\":{\"257\":1}}],[\"query\",{\"1\":{\"216\":3,\"218\":1,\"221\":3}}],[\"qualitative\",{\"0\":{\"180\":1}}],[\"quality\",{\"1\":{\"134\":1}}],[\"qualification\",{\"1\":{\"2\":1}}],[\"quantitative\",{\"0\":{\"179\":1}}],[\"quan\",{\"1\":{\"32\":1}}],[\"qdt​\",{\"1\":{\"134\":1}}],[\"qc\",{\"1\":{\"134\":1}}],[\"qlrt​\",{\"1\":{\"131\":2}}],[\"q∗\",{\"1\":{\"93\":4,\"152\":2,\"154\":2}}],[\"qt\",{\"1\":{\"73\":2}}],[\"qπ\",{\"1\":{\"21\":1}}],[\"q\",{\"0\":{\"93\":1},\"1\":{\"20\":2,\"21\":6,\"25\":3,\"32\":1,\"90\":1,\"92\":3,\"93\":2,\"95\":2,\"97\":2,\"99\":1,\"100\":1,\"131\":1,\"152\":1,\"154\":4,\"216\":1,\"218\":2,\"220\":3,\"221\":1}}],[\"會在訓練過程中先訓練在\",{\"1\":{\"258\":1}}],[\"會搭配\",{\"1\":{\"254\":1}}],[\"會把跟\",{\"1\":{\"253\":2}}],[\"會把每個\",{\"1\":{\"26\":1}}],[\"會對應哪個\",{\"1\":{\"252\":1}}],[\"會對於\",{\"1\":{\"131\":1}}],[\"會是更好的做法\",{\"1\":{\"260\":1}}],[\"會是一個介於\",{\"1\":{\"249\":1}}],[\"會是比較好的選擇\",{\"1\":{\"120\":1}}],[\"會執行\",{\"1\":{\"249\":1}}],[\"會去決定下一個\",{\"1\":{\"249\":1}}],[\"會再去找到所有\",{\"1\":{\"247\":1}}],[\"會合\",{\"1\":{\"236\":1}}],[\"會依序接收上一個時間點的訊息\",{\"1\":{\"224\":1}}],[\"會讓他比較好學\",{\"1\":{\"223\":1}}],[\"會需要轉換成向量才能計算\",{\"1\":{\"223\":1}}],[\"會需要儲存許多不同\",{\"1\":{\"128\":1}}],[\"會先經過\",{\"1\":{\"221\":1}}],[\"會先將他們各自透過一個矩陣得到對應的\",{\"1\":{\"216\":1}}],[\"會相似\",{\"1\":{\"220\":1}}],[\"會與\",{\"1\":{\"214\":1}}],[\"會採用\",{\"1\":{\"197\":1}}],[\"會做為\",{\"1\":{\"197\":1}}],[\"會很容易忽略了細部的特徵\",{\"1\":{\"192\":1}}],[\"會想要讓\",{\"1\":{\"192\":1}}],[\"會太慢\",{\"1\":{\"192\":1}}],[\"會使模型被誤導\",{\"1\":{\"187\":1}}],[\"會使得\",{\"1\":{\"19\":1}}],[\"會時常導致\",{\"1\":{\"181\":1}}],[\"會加上知識蒸餾\",{\"1\":{\"173\":1}}],[\"會\",{\"1\":{\"162\":1}}],[\"會比較大\",{\"1\":{\"159\":1}}],[\"會直接去學\",{\"1\":{\"155\":1}}],[\"會隱晦地在訓練過程中學習到\",{\"1\":{\"135\":1}}],[\"會限制輸出在\",{\"1\":{\"134\":1}}],[\"會被其他的\",{\"1\":{\"119\":1}}],[\"會互相影響\",{\"1\":{\"119\":1}}],[\"會透過\",{\"1\":{\"97\":1,\"227\":1}}],[\"會將每個序列中的每個元素\",{\"1\":{\"214\":1}}],[\"會將\",{\"1\":{\"97\":1,\"221\":1,\"222\":1}}],[\"會有幾個明顯的問題\",{\"1\":{\"91\":1}}],[\"會有怎樣的影響\",{\"1\":{\"74\":1}}],[\"會有大量的下降\",{\"1\":{\"74\":1}}],[\"會有更好的\",{\"1\":{\"74\":1}}],[\"會有兩組總和\",{\"1\":{\"25\":1}}],[\"會有兩個\",{\"1\":{\"21\":1}}],[\"會期待預測出來的\",{\"1\":{\"73\":1}}],[\"會造成的問題是吻合的\",{\"1\":{\"53\":1}}],[\"會不同\",{\"1\":{\"47\":1}}],[\"會導致訓練前期較為緩慢\",{\"1\":{\"36\":1}}],[\"會掉\",{\"1\":{\"35\":1}}],[\"會偏向\",{\"1\":{\"35\":1}}],[\"會得到\",{\"1\":{\"35\":1}}],[\"會從\",{\"1\":{\"32\":1,\"133\":1}}],[\"會選擇不同的\",{\"1\":{\"20\":1}}],[\"會一直到遊戲的最後依照最後通過的時間決定\",{\"1\":{\"16\":1}}],[\"比\",{\"1\":{\"260\":1}}],[\"比預期晚了一些時間才順利抵達筑波大學\",{\"1\":{\"236\":1}}],[\"比起\",{\"1\":{\"251\":1,\"260\":1}}],[\"比起過去的\",{\"1\":{\"201\":1,\"202\":1}}],[\"比起單純的\",{\"1\":{\"141\":1}}],[\"比對\",{\"1\":{\"139\":2}}],[\"比較上分成三個部分\",{\"1\":{\"258\":1}}],[\"比較不敏感\",{\"1\":{\"195\":1}}],[\"比較不會對\",{\"1\":{\"181\":1}}],[\"比較\",{\"1\":{\"162\":2,\"226\":1,\"260\":1}}],[\"比較好\",{\"1\":{\"151\":1}}],[\"比較之後可以發現到\",{\"1\":{\"141\":1}}],[\"比較的結果如下圖所示\",{\"1\":{\"138\":1}}],[\"比較的部份為了公平有時會將\",{\"1\":{\"137\":1}}],[\"比較基準\",{\"0\":{\"101\":1}}],[\"比較差的結果\",{\"1\":{\"34\":1}}],[\"比較傾向去試試看那些不熟的\",{\"1\":{\"19\":1}}],[\"比較大的時候\",{\"1\":{\"19\":1}}],[\"比賽會在正式開賽前一周公布題目\",{\"1\":{\"6\":1}}],[\"比賽過程\",{\"0\":{\"6\":1}}],[\"比賽題目\",{\"0\":{\"5\":1}}],[\"β2​=0\",{\"1\":{\"227\":1}}],[\"β1​=0\",{\"1\":{\"227\":1}}],[\"β=1\",{\"1\":{\"195\":1}}],[\"β=0\",{\"1\":{\"22\":1}}],[\"βj​=0\",{\"1\":{\"35\":2}}],[\"βj​=maxj​βj​\",{\"1\":{\"35\":2}}],[\"βj​\",{\"1\":{\"26\":1,\"32\":1,\"35\":4}}],[\"β\",{\"1\":{\"19\":1,\"20\":1,\"29\":1,\"35\":6,\"154\":4,\"155\":1,\"181\":2}}],[\"βi​\",{\"1\":{\"19\":1,\"20\":5,\"22\":2,\"23\":1}}],[\"用\",{\"1\":{\"191\":1}}],[\"用量普遍很高\",{\"1\":{\"128\":1}}],[\"用來測試模型的一般性\",{\"1\":{\"257\":1}}],[\"用來描述\",{\"1\":{\"174\":1,\"249\":1}}],[\"用來描述裁切的\",{\"1\":{\"133\":1}}],[\"用來調整兩個\",{\"1\":{\"155\":1}}],[\"用來調整兩種\",{\"1\":{\"19\":1}}],[\"用來加上\",{\"1\":{\"150\":1}}],[\"用來比較\",{\"1\":{\"74\":1}}],[\"用來表示一個\",{\"1\":{\"27\":2}}],[\"用兩個\",{\"1\":{\"25\":1}}],[\"用其他\",{\"1\":{\"6\":1}}],[\"只能太有趣\",{\"1\":{\"238\":1}}],[\"只能說在設備上直接贏了\",{\"1\":{\"6\":1}}],[\"只不過是把\",{\"1\":{\"198\":1}}],[\"只不過輸入上會丟\",{\"1\":{\"22\":1}}],[\"只要我們的\",{\"1\":{\"196\":1}}],[\"只要正確的\",{\"1\":{\"116\":1}}],[\"只在\",{\"1\":{\"192\":1}}],[\"只選擇信度高於某個閥值的預測作為\",{\"1\":{\"187\":1}}],[\"只需要\",{\"1\":{\"226\":1}}],[\"只需要產出\",{\"1\":{\"158\":1}}],[\"只需要一張\",{\"1\":{\"85\":1}}],[\"只會每經過\",{\"1\":{\"99\":1}}],[\"只會取出最後\",{\"1\":{\"97\":1}}],[\"只會儲存最後\",{\"1\":{\"97\":1}}],[\"只是調整\",{\"1\":{\"154\":1}}],[\"只是類別增加到\",{\"1\":{\"116\":1}}],[\"只是為了\",{\"1\":{\"76\":1}}],[\"只是單純符合條件給\",{\"1\":{\"73\":1,\"131\":1}}],[\"只是用來限制\",{\"1\":{\"19\":1}}],[\"只對簡單的\",{\"1\":{\"61\":1}}],[\"只有\",{\"1\":{\"50\":1,\"134\":1}}],[\"只訓練在\",{\"1\":{\"47\":1,\"74\":1}}],[\"只用了一個\",{\"1\":{\"23\":1}}],[\"目標在於給定\",{\"1\":{\"191\":1}}],[\"目標\",{\"1\":{\"153\":2}}],[\"目標同樣是要分類\",{\"1\":{\"118\":1}}],[\"目標同樣是將每個圖片分類到正確的類別當中\",{\"1\":{\"116\":1}}],[\"目標是要在這個空間當中達成指定的任務\",{\"1\":{\"257\":1}}],[\"目標是要讓同樣都在\",{\"1\":{\"175\":1}}],[\"目標是要去把每個演講分類到正確的主題當中\",{\"1\":{\"117\":1}}],[\"目標是要辨識出門牌號碼\",{\"1\":{\"116\":1}}],[\"目標是在整個\",{\"1\":{\"27\":1}}],[\"目的是要訓練出一個\",{\"1\":{\"198\":1}}],[\"目的是要先訓練出一個\",{\"1\":{\"198\":1}}],[\"目的是要分類每張圖片至正確的類別\",{\"1\":{\"116\":1}}],[\"目的是要辨認出每個圖片是對應到哪個數字\",{\"1\":{\"116\":1}}],[\"目的也是希望能夠讓\",{\"1\":{\"18\":1}}],[\"目前只能在文字訊息上處理\",{\"1\":{\"261\":1}}],[\"目前正在朝向\",{\"1\":{\"0\":1}}],[\"目前就讀於清華大學資訊工程學系\",{\"1\":{\"0\":1}}],[\"u∈u\",{\"1\":{\"249\":1}}],[\"u\",{\"1\":{\"173\":2,\"249\":4,\"251\":2,\"253\":6,\"254\":6}}],[\"usage\",{\"0\":{\"141\":1}}],[\"uda\",{\"0\":{\"53\":1,\"73\":1,\"77\":1,\"139\":1,\"173\":1},\"1\":{\"50\":4,\"53\":2,\"64\":1,\"69\":1,\"70\":4,\"71\":1,\"73\":1,\"74\":5,\"76\":1,\"79\":1,\"84\":1,\"85\":2,\"128\":2,\"129\":1,\"131\":4,\"138\":1,\"139\":2,\"143\":2,\"170\":1,\"171\":1,\"172\":1,\"182\":1,\"187\":1,\"188\":1,\"191\":1,\"196\":1,\"203\":2,\"204\":1}}],[\"unseen\",{\"1\":{\"257\":2,\"260\":3,\"261\":1}}],[\"unsuvervised\",{\"1\":{\"70\":1}}],[\"unsupervised\",{\"1\":{\"50\":1,\"71\":1,\"113\":1,\"129\":1,\"170\":1,\"171\":1,\"187\":1,\"188\":1,\"204\":2}}],[\"unlabelled\",{\"1\":{\"53\":1}}],[\"unlabeled\",{\"1\":{\"50\":3,\"51\":1}}],[\"unlebelled\",{\"1\":{\"53\":1}}],[\"uncapped\",{\"1\":{\"38\":1}}],[\"undiscounted\",{\"1\":{\"34\":1}}],[\"unit\",{\"1\":{\"112\":4,\"113\":1,\"119\":4,\"120\":1}}],[\"units\",{\"1\":{\"109\":2,\"111\":4,\"119\":2}}],[\"unity\",{\"1\":{\"60\":1}}],[\"university\",{\"1\":{\"45\":1,\"106\":1,\"186\":1,\"244\":2},\"2\":{\"242\":1}}],[\"universal\",{\"1\":{\"20\":1}}],[\"unix\",{\"1\":{\"1\":1}}],[\"uk​\",{\"1\":{\"29\":1}}],[\"uk​<ϵucb​​\",{\"1\":{\"29\":1}}],[\"uk​≥ϵucb​∀n≤k≤k−1\",{\"1\":{\"29\":1}}],[\"ucb\",{\"0\":{\"27\":1,\"28\":1,\"29\":1},\"1\":{\"27\":5,\"28\":2,\"29\":1,\"41\":1}}],[\"uvfa\",{\"0\":{\"20\":1},\"1\":{\"18\":1,\"20\":1}}],[\"upsample\",{\"1\":{\"140\":1}}],[\"upernet\",{\"1\":{\"84\":1}}],[\"upper\",{\"0\":{\"27\":1}}],[\"up\",{\"0\":{\"18\":1},\"1\":{\"18\":1,\"41\":1,\"257\":1}}],[\"why\",{\"0\":{\"226\":1}}],[\"what\",{\"0\":{\"46\":1}}],[\"w2​∈rdff​×dm​\",{\"1\":{\"222\":1}}],[\"w2​+b2​\",{\"1\":{\"222\":1}}],[\"w1​∈rdm​×fff​\",{\"1\":{\"222\":1}}],[\"won\",{\"1\":{\"257\":1}}],[\"wo=attention\",{\"1\":{\"220\":1}}],[\"world\",{\"0\":{\"243\":1,\"254\":1},\"1\":{\"219\":1,\"245\":1,\"249\":2,\"253\":4,\"254\":3,\"257\":1,\"260\":1,\"261\":1}}],[\"work\",{\"1\":{\"47\":1}}],[\"works\",{\"0\":{\"17\":1,\"48\":1,\"71\":1,\"92\":1,\"108\":1,\"129\":1,\"150\":1,\"171\":1,\"188\":1,\"213\":1,\"246\":1},\"1\":{\"170\":1}}],[\"wv\",{\"1\":{\"220\":3}}],[\"wkm\",{\"1\":{\"245\":1,\"247\":1,\"249\":1,\"254\":6,\"260\":10,\"261\":5}}],[\"wk\",{\"1\":{\"220\":3}}],[\"wq\",{\"1\":{\"220\":3}}],[\"wmt\",{\"1\":{\"212\":2,\"227\":2}}],[\"wt\",{\"1\":{\"192\":3,\"193\":1,\"195\":1}}],[\"wtest\",{\"1\":{\"112\":1}}],[\"w∈rq×p\",{\"1\":{\"158\":1}}],[\"w−swc​\",{\"1\":{\"133\":1}}],[\"wc​=wd​\",{\"1\":{\"133\":1}}],[\"web\",{\"1\":{\"257\":1}}],[\"webshop\",{\"1\":{\"245\":1,\"257\":3,\"260\":1}}],[\"week\",{\"0\":{\"234\":1}}],[\"wen\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"wei\",{\"1\":{\"144\":1}}],[\"weight\",{\"1\":{\"113\":1,\"158\":1,\"192\":3,\"196\":1}}],[\"weights\",{\"1\":{\"112\":1,\"197\":1}}],[\"weak\",{\"1\":{\"70\":1}}],[\"wf​\",{\"1\":{\"76\":1}}],[\"w\",{\"1\":{\"73\":1,\"76\":1,\"112\":1}}],[\"wio​∈rhdv​×dm​\",{\"1\":{\"220\":1}}],[\"wiv​∈rdm​×dv​\",{\"1\":{\"220\":1}}],[\"wik​∈rdm​×dk​\",{\"1\":{\"220\":1}}],[\"wikipedia\",{\"1\":{\"109\":1}}],[\"wiq​∈rdm​×dk​\",{\"1\":{\"220\":1}}],[\"wise\",{\"0\":{\"168\":1,\"174\":1,\"175\":1,\"222\":1},\"1\":{\"170\":2,\"172\":2,\"174\":1,\"175\":2,\"176\":2,\"181\":1}}],[\"with\",{\"0\":{\"89\":1,\"135\":1,\"243\":1,\"254\":1},\"1\":{\"64\":1,\"75\":1,\"92\":1,\"151\":2,\"165\":2,\"257\":1,\"258\":2,\"259\":1}}],[\"wilhelm\",{\"1\":{\"53\":2,\"54\":2,\"61\":2,\"62\":1}}],[\"winter\",{\"1\":{\"45\":1}}],[\"window\",{\"0\":{\"28\":1,\"29\":1,\"30\":1,\"36\":1,\"135\":1},\"1\":{\"28\":2,\"29\":1,\"135\":1}}],[\"wang\",{\"1\":{\"144\":1,\"152\":1,\"154\":1,\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"way\",{\"0\":{\"105\":1}}],[\"warmup\",{\"0\":{\"77\":1,\"81\":1},\"1\":{\"74\":1,\"77\":1,\"80\":2,\"81\":1,\"131\":1,\"200\":1,\"227\":1}}],[\"warning\",{\"1\":{\"5\":3,\"26\":1,\"151\":1}}],[\"wall\",{\"1\":{\"60\":1}}],[\"wacv\",{\"1\":{\"45\":1},\"2\":{\"67\":1}}],[\"正確\",{\"1\":{\"16\":1}}],[\"之前我們需要去考慮\",{\"1\":{\"254\":1}}],[\"之前會經過\",{\"1\":{\"227\":1}}],[\"之前在清大的時候偶爾會自己騎著機車隨意的兜風\",{\"1\":{\"238\":1}}],[\"之前在\",{\"1\":{\"200\":1}}],[\"之前的想法是先去做一些\",{\"1\":{\"6\":1}}],[\"之類的\",{\"1\":{\"53\":1}}],[\"之間的數字表示完成率\",{\"1\":{\"257\":1}}],[\"之間的數值\",{\"1\":{\"249\":1,\"257\":1}}],[\"之間的部分都乘上一個很大的負數\",{\"1\":{\"219\":1}}],[\"之間的資料\",{\"1\":{\"219\":1}}],[\"之間的關聯\",{\"1\":{\"218\":1}}],[\"之間的距離\",{\"1\":{\"192\":1}}],[\"之間的距離去調整\",{\"1\":{\"192\":1}}],[\"之間的距離可以拉近\",{\"1\":{\"76\":1}}],[\"之間的\",{\"1\":{\"170\":1}}],[\"之間築起橋梁\",{\"1\":{\"170\":1}}],[\"之間\",{\"1\":{\"134\":1}}],[\"之間會是比較好的選擇\",{\"1\":{\"120\":1}}],[\"之間會比較平坦\",{\"1\":{\"120\":1}}],[\"之間都會具有相當高的相關性\",{\"1\":{\"91\":1}}],[\"之間有一些重疊的\",{\"1\":{\"37\":1}}],[\"之間均勻分布的隨機\",{\"1\":{\"29\":1}}],[\"之間均勻分布的隨機值\",{\"1\":{\"29\":1}}],[\"之後也許會開始嘗試自己料理\",{\"1\":{\"237\":1}}],[\"之後讓我填寫了文件\",{\"1\":{\"236\":1}}],[\"之後他的大小仍然會跟原本是相同的\",{\"1\":{\"222\":1}}],[\"之後可以得到底下的\",{\"1\":{\"154\":1}}],[\"之後可以來到\",{\"1\":{\"116\":1}}],[\"之後的結果都有些進步\",{\"1\":{\"162\":1}}],[\"之後的誤差\",{\"1\":{\"153\":1}}],[\"之後的預測結果\",{\"1\":{\"134\":1}}],[\"之後就可以降低到\",{\"1\":{\"116\":1}}],[\"之後能夠還原出輸入的原貌\",{\"1\":{\"109\":1}}],[\"之後作為實際上儲存進\",{\"1\":{\"97\":1}}],[\"之後得到的成效在\",{\"1\":{\"37\":1}}],[\"之後\",{\"1\":{\"37\":1,\"80\":4,\"83\":1,\"97\":1,\"112\":1,\"119\":1,\"157\":1,\"236\":1}}],[\"之後仍然不會停止的話就不會出現\",{\"1\":{\"35\":1}}],[\"之後交給\",{\"1\":{\"32\":1}}],[\"之所以說\",{\"1\":{\"16\":1}}],[\"過大就可能使我們誤判現在已經訓練差不多\",{\"1\":{\"218\":1}}],[\"過大時\",{\"1\":{\"181\":1}}],[\"過程中的現有資訊則被稱為local\",{\"1\":{\"245\":1}}],[\"過程中有許多陷阱\",{\"1\":{\"16\":1}}],[\"過程當中也會有一些機率出現\",{\"1\":{\"107\":1}}],[\"過於老舊的部分作者先透過一些實驗去尋找好的架構\",{\"1\":{\"74\":1}}],[\"過高\",{\"1\":{\"46\":1}}],[\"過去增加探索的方法大多都是在\",{\"1\":{\"151\":1}}],[\"過去類似的作法出現在\",{\"1\":{\"109\":1}}],[\"過去會透過多次遊戲中\",{\"1\":{\"100\":1}}],[\"過去在\",{\"1\":{\"91\":1}}],[\"過去訓練\",{\"1\":{\"77\":1}}],[\"過去對於\",{\"1\":{\"27\":1}}],[\"過去的這些\",{\"1\":{\"170\":1}}],[\"過去的經驗上都還需要額外設計一些\",{\"1\":{\"131\":1}}],[\"過去的研究當中發現到如果是\",{\"1\":{\"95\":1}}],[\"過去的\",{\"1\":{\"16\":1,\"131\":1}}],[\"過去基本上就是看過\",{\"1\":{\"11\":1}}],[\"過去曾擔任臺南一中資訊社社長\",{\"1\":{\"0\":1}}],[\"玩家要操作主角在\",{\"1\":{\"16\":1}}],[\"玩家要操作角色滑雪\",{\"1\":{\"16\":1}}],[\"需要透過查詢\",{\"1\":{\"257\":1}}],[\"需要透過與環境互動取得\",{\"1\":{\"91\":1}}],[\"需要跑過幾次\",{\"1\":{\"226\":1}}],[\"需要等待前面的輸入\",{\"1\":{\"226\":1}}],[\"需要我們自己告訴他\",{\"1\":{\"224\":1}}],[\"需要多注意\",{\"1\":{\"216\":1}}],[\"需要多注意元素\",{\"1\":{\"216\":1}}],[\"需要花費更多次運算得到彼此的關係\",{\"1\":{\"211\":1}}],[\"需要\",{\"1\":{\"155\":1}}],[\"需要額外的\",{\"1\":{\"155\":1}}],[\"需要特別注意到對於\",{\"1\":{\"93\":1}}],[\"需要看遠一些\",{\"1\":{\"20\":1}}],[\"需要相當大量的探索之後才能得到\",{\"1\":{\"16\":1}}],[\"需要解出一些簡單的演算法題目\",{\"1\":{\"4\":1}}],[\"秒的\",{\"1\":{\"16\":1}}],[\"每\",{\"1\":{\"162\":2}}],[\"每一個\",{\"1\":{\"26\":1}}],[\"每個部分的設計\",{\"1\":{\"217\":1}}],[\"每個詞語的意義都會與前面的內容相關\",{\"1\":{\"214\":1}}],[\"每個人也都還是能夠有一定的能力去解決\",{\"1\":{\"119\":1}}],[\"每個人題目會不太相同\",{\"1\":{\"4\":1}}],[\"每個遊戲的\",{\"1\":{\"99\":1}}],[\"每個\",{\"1\":{\"25\":1,\"26\":2,\"29\":1,\"32\":1,\"111\":1,\"112\":1,\"192\":1,\"195\":1,\"253\":1}}],[\"每種\",{\"1\":{\"23\":1,\"26\":1,\"29\":1}}],[\"每忽略一個\",{\"1\":{\"16\":1}}],[\"途中要盡可能快速通過指定數量的\",{\"1\":{\"16\":1}}],[\"以較隱晦的方式傳遞\",{\"1\":{\"260\":1}}],[\"以至於要多喝水消除那個味道\",{\"1\":{\"237\":1}}],[\"以至於實際上每個\",{\"1\":{\"119\":1}}],[\"以數學來描述\",{\"1\":{\"219\":1}}],[\"以下圖為例\",{\"1\":{\"216\":1}}],[\"以下就分別說明這三個部分的作法\",{\"1\":{\"74\":1}}],[\"以外\",{\"1\":{\"214\":1,\"260\":1}}],[\"以外的範圍都是黑的\",{\"1\":{\"134\":1}}],[\"以達成\",{\"1\":{\"151\":1}}],[\"以達到更好的訓練成效\",{\"1\":{\"25\":1}}],[\"以致於難以\",{\"1\":{\"75\":1}}],[\"以致於開始將研究的方向轉向如\",{\"1\":{\"70\":1}}],[\"以上面的圖片為例\",{\"1\":{\"257\":1}}],[\"以上的結果\",{\"1\":{\"132\":1}}],[\"以上\",{\"1\":{\"35\":1}}],[\"以\",{\"1\":{\"16\":2,\"131\":1}}],[\"以及用來提供\",{\"1\":{\"253\":1}}],[\"以及接收到的\",{\"1\":{\"249\":1}}],[\"以及各自的規範\",{\"1\":{\"249\":1}}],[\"以及各種新定義的\",{\"1\":{\"128\":1}}],[\"以及我的摯愛心瑤\",{\"1\":{\"234\":1}}],[\"以及資訊流失的狀況\",{\"1\":{\"226\":1}}],[\"以及多少的\",{\"1\":{\"134\":1}}],[\"以及畫面下方\",{\"1\":{\"83\":1}}],[\"以及訓練模型的\",{\"1\":{\"76\":1}}],[\"以及一般性都並不是很理想\",{\"1\":{\"73\":1}}],[\"以及一個對應的\",{\"1\":{\"192\":1}}],[\"以及一個\",{\"1\":{\"35\":1}}],[\"以及他對於\",{\"1\":{\"73\":1}}],[\"以及沒有的狀況\",{\"1\":{\"37\":1}}],[\"以及對應的\",{\"1\":{\"36\":1}}],[\"以及最傾向\",{\"1\":{\"35\":1}}],[\"以及加上\",{\"1\":{\"7\":1,\"162\":1}}],[\"以及\",{\"1\":{\"0\":1,\"6\":1,\"16\":1,\"19\":2,\"23\":1,\"25\":2,\"35\":2,\"36\":2,\"37\":1,\"46\":1,\"49\":2,\"53\":1,\"54\":1,\"57\":1,\"74\":1,\"97\":2,\"99\":1,\"102\":2,\"109\":1,\"116\":1,\"128\":1,\"131\":1,\"132\":1,\"133\":4,\"137\":1,\"139\":1,\"155\":3,\"162\":2,\"164\":1,\"170\":2,\"173\":2,\"178\":2,\"187\":1,\"191\":1,\"196\":1,\"198\":1,\"211\":1,\"216\":2,\"247\":1,\"249\":2,\"251\":1,\"257\":1,\"258\":3}}],[\"那也沒有太大的參考價值\",{\"1\":{\"251\":1}}],[\"那也就會有\",{\"1\":{\"21\":1}}],[\"那我找到雞蛋了嗎\",{\"1\":{\"245\":1}}],[\"那我們就可以用\",{\"1\":{\"50\":1}}],[\"那我們不要用\",{\"1\":{\"6\":1}}],[\"那味道讓我無法接受\",{\"1\":{\"237\":1}}],[\"那就直接把原圖交給\",{\"1\":{\"135\":1}}],[\"那就會在辨識上出現問題\",{\"1\":{\"128\":1}}],[\"那就必然會考慮到所有的面向\",{\"1\":{\"107\":1}}],[\"那麼定義\",{\"1\":{\"253\":1}}],[\"那麼我們定義\",{\"1\":{\"253\":1}}],[\"那麼也許會出現前後動作缺乏連貫性\",{\"1\":{\"252\":1}}],[\"那麼一個\",{\"1\":{\"249\":1}}],[\"那麼你會需要知道\",{\"1\":{\"245\":1}}],[\"那麼不同\",{\"1\":{\"220\":1}}],[\"那麼有更多的的視角去理解一個事情\",{\"1\":{\"220\":1}}],[\"那麼在處理輸出\",{\"1\":{\"219\":1}}],[\"那麼這裡加上的\",{\"1\":{\"163\":1}}],[\"那麼就會有更高的機會在\",{\"1\":{\"70\":1}}],[\"那麼\",{\"1\":{\"16\":1,\"158\":1,\"196\":1}}],[\"不一樣的地方是\",{\"1\":{\"253\":1}}],[\"不斷在心中提醒自己要有在留卡和資格外活動證明\",{\"1\":{\"236\":1}}],[\"不知道為什麼筑波大學附近常常看到這種青蛙雕像\",{\"1\":{\"237\":1}}],[\"不知道\",{\"1\":{\"191\":1}}],[\"不要想太多\",{\"1\":{\"157\":1}}],[\"不難發現到確實都存在高估的狀況\",{\"1\":{\"153\":1}}],[\"不被選到\",{\"1\":{\"111\":1}}],[\"不需要等待傳遞\",{\"1\":{\"226\":1}}],[\"不需要事先經過其他的分解\",{\"1\":{\"102\":1}}],[\"不需要看太遠\",{\"1\":{\"20\":1}}],[\"不是\",{\"1\":{\"96\":1}}],[\"不是那麼地\",{\"1\":{\"23\":1,\"29\":1}}],[\"不採用\",{\"1\":{\"74\":1}}],[\"不確定性低\",{\"1\":{\"27\":1}}],[\"不確定性高\",{\"1\":{\"27\":1}}],[\"不會被固定下來\",{\"1\":{\"26\":1}}],[\"不穩定\",{\"1\":{\"25\":1}}],[\"不同有不同上限\",{\"1\":{\"260\":1}}],[\"不同的地方\",{\"1\":{\"224\":1}}],[\"不同的地方在於他並不是直接去學習\",{\"1\":{\"154\":1}}],[\"不同的地方在於\",{\"1\":{\"96\":1}}],[\"不同的則被推遠\",{\"1\":{\"174\":1}}],[\"不同的環境下需要的\",{\"1\":{\"19\":1}}],[\"不同\",{\"1\":{\"23\":1,\"26\":2,\"49\":1,\"74\":1,\"109\":1,\"195\":1}}],[\"不太好\",{\"1\":{\"16\":1,\"47\":1}}],[\"不過同時也存在幾個\",{\"1\":{\"261\":1}}],[\"不過此篇\",{\"1\":{\"260\":1}}],[\"不過值得一提的是\",{\"1\":{\"253\":1}}],[\"不過如果這個產出的\",{\"1\":{\"251\":1}}],[\"不過如果遇到新的\",{\"1\":{\"47\":1}}],[\"不過作者發現過多的資訊反倒會導致模型會感到困惑\",{\"1\":{\"252\":1}}],[\"不過作者發現在他們的模型得出來的結果往往會是很不穩定的\",{\"1\":{\"100\":1}}],[\"不過作者認為如果我們只看\",{\"1\":{\"251\":1}}],[\"不過其中比較特別的是最初的\",{\"1\":{\"249\":1}}],[\"不過後續其實是在線上辦\",{\"1\":{\"240\":1}}],[\"不過腳踏車騎在人行道上的時候到底要往哪邊讓對向的行人\",{\"1\":{\"240\":1}}],[\"不過頻率真的比台灣低太多太多了\",{\"1\":{\"240\":1}}],[\"不過想到還有沒順利推進的專題進度\",{\"1\":{\"239\":1}}],[\"不過大四學生的我又再加上專題\",{\"1\":{\"239\":1}}],[\"不過大致上他的做法是會經過幾個\",{\"1\":{\"226\":1}}],[\"不過與機車不同\",{\"1\":{\"238\":1}}],[\"不過就持續推進進度就沒有問題了\",{\"1\":{\"238\":1}}],[\"不過致越目前還沒有\",{\"1\":{\"237\":1}}],[\"不過窗外的景色相當地美麗\",{\"1\":{\"236\":1}}],[\"不過幸好也許是因為緣分\",{\"1\":{\"236\":1}}],[\"不過他的靈活性在當前許多的領域都可以看到\",{\"1\":{\"230\":1}}],[\"不過有特別提到每一個需要\",{\"1\":{\"223\":1}}],[\"不過在各種購物以外\",{\"1\":{\"238\":1}}],[\"不過在\",{\"1\":{\"221\":1}}],[\"不過在計算\",{\"1\":{\"25\":1}}],[\"不過正因為這樣的架構設計\",{\"1\":{\"214\":1}}],[\"不過也會使得\",{\"1\":{\"211\":1}}],[\"不過也考慮到\",{\"1\":{\"76\":1}}],[\"不過實驗中\",{\"1\":{\"181\":1}}],[\"不過實際上我們在輸出的過程當中\",{\"1\":{\"219\":1}}],[\"不過實際上對於\",{\"1\":{\"119\":1}}],[\"不過實際上訓練時因為拆開來訓練\",{\"1\":{\"25\":1}}],[\"不過實際上在實作的時候光是硬體的資源可能就是一大障礙\",{\"1\":{\"11\":1}}],[\"不過進步主要在\",{\"1\":{\"162\":1}}],[\"不過並沒有保證收斂\",{\"1\":{\"149\":1}}],[\"不過相較之下\",{\"1\":{\"140\":1}}],[\"不過使用的目標是\",{\"1\":{\"134\":1}}],[\"不過只運用這樣的\",{\"1\":{\"131\":1}}],[\"不過當你們現在處在一個未知的環境當中\",{\"1\":{\"119\":1}}],[\"不過隨著參數量的上升\",{\"1\":{\"107\":1}}],[\"不過\",{\"1\":{\"96\":1,\"216\":1,\"227\":1}}],[\"不過從\",{\"1\":{\"91\":1}}],[\"不過過去使用\",{\"1\":{\"74\":1}}],[\"不過直覺上\",{\"1\":{\"70\":1}}],[\"不過這樣的數值範圍可以是\",{\"1\":{\"216\":1}}],[\"不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索\",{\"1\":{\"149\":1}}],[\"不過這裡的比較卻乏單純的\",{\"1\":{\"116\":1}}],[\"不過這裡的成功只停止在雙陸棋上\",{\"1\":{\"94\":1}}],[\"不過這裡最主要都是使用\",{\"1\":{\"56\":1}}],[\"不過這種情況下一個直覺的問題是\",{\"1\":{\"46\":1}}],[\"不過像是馬路\",{\"1\":{\"49\":1}}],[\"不過可惜的是\",{\"1\":{\"16\":1}}],[\"不過理想很美好\",{\"1\":{\"6\":1}}],[\"不過寄信去詢問之後得到希望還是使用到\",{\"1\":{\"6\":1}}],[\"不過場地因為是辦公室\",{\"1\":{\"6\":1}}],[\"就在描述在當前的\",{\"1\":{\"253\":1}}],[\"就去看現在他被輸出的機率是多少\",{\"1\":{\"253\":1}}],[\"就好\",{\"1\":{\"227\":1}}],[\"就得到\",{\"1\":{\"221\":1}}],[\"就只有投影到\",{\"1\":{\"220\":1}}],[\"就只是這樣而已\",{\"1\":{\"157\":1}}],[\"就能使\",{\"1\":{\"219\":1}}],[\"就能夠迫使模型對於這些略有不同的\",{\"1\":{\"196\":1}}],[\"就能夠比較好發揮作用\",{\"1\":{\"51\":1}}],[\"就需要看過整個\",{\"1\":{\"194\":1}}],[\"就被錯誤分類\",{\"1\":{\"187\":1}}],[\"就應該隨著訓練慢慢被忽視\",{\"1\":{\"163\":1}}],[\"就都是用這個\",{\"1\":{\"157\":1}}],[\"就小到幾乎不存在了\",{\"1\":{\"153\":1}}],[\"就比較有系統性一些\",{\"1\":{\"151\":1}}],[\"就比較像是在亂試\",{\"1\":{\"151\":1}}],[\"就像是模擬了人在閱讀文章的狀態\",{\"1\":{\"214\":1}}],[\"就像是看到有殼的動物就當成是昆蟲\",{\"1\":{\"187\":1}}],[\"就像是\",{\"1\":{\"154\":1}}],[\"就像是可以換個角度去想其他人會怎麼做\",{\"1\":{\"151\":1}}],[\"就像是獵人裡面的凱特\",{\"1\":{\"151\":1}}],[\"就像是這邊的\",{\"1\":{\"134\":1}}],[\"就像是有時候你的隊友會請假\",{\"1\":{\"119\":1}}],[\"就很不相同\",{\"1\":{\"100\":1}}],[\"就持續上一個做出的\",{\"1\":{\"99\":1}}],[\"就定義成\",{\"1\":{\"97\":1}}],[\"就基本上沒有\",{\"1\":{\"76\":1}}],[\"就不是單純的\",{\"1\":{\"73\":1}}],[\"就相當地雷同\",{\"1\":{\"49\":1}}],[\"就是單純把結果串接在一起而已\",{\"1\":{\"220\":1}}],[\"就是能夠告訴我們需要注意\",{\"1\":{\"216\":1}}],[\"就是上述的\",{\"1\":{\"192\":1}}],[\"就是在\",{\"1\":{\"151\":1}}],[\"就是用\",{\"1\":{\"93\":1}}],[\"就是用來描述一群資料他們的分布狀況\",{\"1\":{\"46\":1}}],[\"就是解決了使用更好的\",{\"1\":{\"74\":1}}],[\"就是希望\",{\"1\":{\"54\":1}}],[\"就是\",{\"1\":{\"49\":1,\"53\":1}}],[\"就是拿來做簡報\",{\"1\":{\"6\":1}}],[\"就會開始規劃\",{\"1\":{\"254\":1}}],[\"就會需要擔心了\",{\"1\":{\"226\":1}}],[\"就會導致互相的不理解\",{\"1\":{\"47\":1}}],[\"就會導致單純在\",{\"1\":{\"46\":1}}],[\"就會因為\",{\"1\":{\"26\":1}}],[\"就會去環境當中互動\",{\"1\":{\"26\":1}}],[\"就會多\",{\"1\":{\"16\":1}}],[\"就有不同重要程度了\",{\"1\":{\"26\":1}}],[\"就做得頗差\",{\"1\":{\"23\":1}}],[\"就可以避免這個問題\",{\"1\":{\"224\":1}}],[\"就可以分別得到\",{\"1\":{\"133\":1}}],[\"就可以用\",{\"1\":{\"75\":1}}],[\"就可以結合起來形成新的\",{\"1\":{\"73\":1}}],[\"就可以再拿去\",{\"1\":{\"50\":1}}],[\"就可以得到相當好的影像分割結果\",{\"1\":{\"47\":1}}],[\"就可以得到單純\",{\"1\":{\"22\":1}}],[\"就可以透過\",{\"1\":{\"21\":1,\"131\":1}}],[\"就跟\",{\"1\":{\"21\":1}}],[\"就通常完全沒辦法學習\",{\"1\":{\"16\":1}}],[\"和超市去找便宜實惠的東西\",{\"1\":{\"237\":1}}],[\"和一層\",{\"1\":{\"94\":1}}],[\"和\",{\"1\":{\"16\":2,\"19\":1,\"23\":1,\"25\":3,\"28\":1,\"49\":1,\"50\":1,\"76\":1,\"82\":1,\"97\":1,\"112\":1,\"149\":1,\"154\":3,\"155\":1,\"158\":1,\"159\":2,\"190\":4,\"196\":1,\"216\":1,\"257\":1}}],[\"分開\",{\"1\":{\"74\":1}}],[\"分布相當不同時\",{\"1\":{\"23\":1}}],[\"分成了兩個部分\",{\"1\":{\"19\":1}}],[\"分鐘的時間探索\",{\"1\":{\"16\":1}}],[\"分別帶來的影響\",{\"1\":{\"260\":1}}],[\"分別帶來的效益\",{\"1\":{\"181\":1}}],[\"分別經過\",{\"1\":{\"220\":1}}],[\"分別製作成\",{\"1\":{\"215\":1}}],[\"分別搭配\",{\"1\":{\"178\":1}}],[\"分別會希望我們的模型在\",{\"1\":{\"173\":1}}],[\"分別表示\",{\"1\":{\"155\":1,\"190\":3}}],[\"分別表示圖片的高寬\",{\"1\":{\"73\":1}}],[\"分別落在哪個\",{\"1\":{\"37\":1}}],[\"分別用\",{\"1\":{\"36\":1}}],[\"分別去針對\",{\"1\":{\"25\":1}}],[\"分別是用來決策的\",{\"1\":{\"253\":1}}],[\"分別是\",{\"1\":{\"19\":1,\"79\":1,\"196\":1,\"257\":1,\"258\":2}}],[\"分別是這些遊戲\",{\"1\":{\"16\":1}}],[\"分別在\",{\"1\":{\"16\":1}}],[\"分組第三名\",{\"1\":{\"2\":1}}],[\"例如要燒開水\",{\"1\":{\"257\":1}}],[\"例如現在你要完成第一個步驟\",{\"1\":{\"245\":1}}],[\"例如從上面可以知道\",{\"1\":{\"216\":1}}],[\"例如下圖\",{\"1\":{\"187\":1}}],[\"例如說\",{\"1\":{\"131\":1}}],[\"例如說在自駕車的道路辨識當中鄰近人行道這種時常出現的\",{\"1\":{\"50\":1}}],[\"例如在\",{\"1\":{\"23\":1,\"149\":1}}],[\"例如\",{\"1\":{\"16\":1,\"149\":1}}],[\"在前面我們設計\",{\"1\":{\"260\":1}}],[\"在前面的實驗當中\",{\"1\":{\"260\":1}}],[\"在決策的過程當中會先將\",{\"1\":{\"258\":1}}],[\"在規劃的過程當中會歷經\",{\"1\":{\"258\":1}}],[\"在許多自然語言處理的問題有很快速的成長\",{\"1\":{\"245\":1}}],[\"在抵達日本的第一天就遇到直接在面前嘖嘴嘆氣的工作人員著實嚇到我了\",{\"1\":{\"240\":1}}],[\"在中央公園裡面的火箭\",{\"1\":{\"239\":1}}],[\"在不安與焦躁當中的放鬆\",{\"0\":{\"239\":1}}],[\"在不同選擇下的結果\",{\"1\":{\"120\":1}}],[\"在不同\",{\"1\":{\"35\":1,\"75\":1}}],[\"在剛進到學校會有許多的任務需要處理\",{\"1\":{\"237\":1}}],[\"在走了一段全黑的道路後\",{\"1\":{\"236\":1}}],[\"在來筑波之前還以為需要自己準備床墊\",{\"1\":{\"236\":1}}],[\"在潮濕\",{\"1\":{\"236\":1}}],[\"在第二航廈等待了一些時間\",{\"1\":{\"236\":1}}],[\"在空中看到的可愛雲朵\",{\"1\":{\"236\":1}}],[\"在起飛的前一晚確認過無數次行李\",{\"1\":{\"236\":1}}],[\"在家人和朋友的目送下離開了熟悉的台灣\",{\"1\":{\"236\":1}}],[\"在學習日文以外\",{\"1\":{\"235\":1}}],[\"在同樣的\",{\"1\":{\"229\":1}}],[\"在同樣的網路架構下\",{\"1\":{\"121\":1}}],[\"在現在我們的序列數量基本上跟序列長度的數量級是差不多的狀況下\",{\"1\":{\"226\":1}}],[\"在計算的複雜度上比較一下\",{\"1\":{\"226\":1}}],[\"在計算上分別都只會拿自己的\",{\"1\":{\"25\":1}}],[\"在語言模型當中\",{\"1\":{\"218\":1}}],[\"在傳遞過程當中可能導致訊息的流失\",{\"1\":{\"216\":1}}],[\"在翻譯的任務上也打破\",{\"1\":{\"212\":1}}],[\"在絕大多數的類別當中也是比起過去的做法還要強\",{\"1\":{\"201\":1}}],[\"在絕大多數並非是最佳的結果上都不會離最佳太遠\",{\"1\":{\"61\":1,\"62\":1}}],[\"在兩個\",{\"1\":{\"200\":1}}],[\"在那些充滿噪點的\",{\"1\":{\"192\":1}}],[\"在一個人走一片漆黑的路回宿舍真的有些嚇人\",{\"1\":{\"240\":1}}],[\"在一個\",{\"1\":{\"192\":1}}],[\"在一起\",{\"1\":{\"134\":1}}],[\"在實驗上使用的\",{\"1\":{\"227\":1}}],[\"在實驗的幾組\",{\"1\":{\"181\":1}}],[\"在實務上為了避免像是\",{\"1\":{\"154\":1}}],[\"在實作上會包含\",{\"1\":{\"131\":1}}],[\"在過去也有一些透過\",{\"1\":{\"247\":1}}],[\"在過去的\",{\"1\":{\"149\":1}}],[\"在過去的方法上由於\",{\"1\":{\"128\":1}}],[\"在過往的研究可以發現到說往往我們在設計讓\",{\"1\":{\"151\":1}}],[\"在相同量級或甚至更小量級的記憶體使用量下\",{\"1\":{\"143\":1}}],[\"在相同的\",{\"1\":{\"116\":1}}],[\"在細節上的處理比起過去的\",{\"1\":{\"138\":1}}],[\"在部分與其他\",{\"1\":{\"137\":1}}],[\"在選擇上會是在圖片範圍當中的\",{\"1\":{\"133\":1}}],[\"在各種狀況下給出的效益\",{\"1\":{\"122\":1}}],[\"在訓練過程中不被考慮到\",{\"1\":{\"112\":1}}],[\"在訓練後期才出現\",{\"1\":{\"75\":1}}],[\"在做的事情可以視為是在\",{\"1\":{\"109\":1}}],[\"在有性生殖的過程當中會融合雙親的基因\",{\"1\":{\"107\":1}}],[\"在近年來發現到\",{\"1\":{\"107\":1}}],[\"在幾乎所有的遊戲當中都\",{\"1\":{\"101\":1}}],[\"在經過\",{\"1\":{\"97\":1,\"227\":1}}],[\"在畫面的上方也有部分的影像因為影像校正導致的失真如下圖所示\",{\"1\":{\"83\":1}}],[\"在上圖的橘色線是原本的模型隨著訓練後對於不同\",{\"1\":{\"83\":1}}],[\"在沒有使用\",{\"1\":{\"82\":1}}],[\"在加上不同的調整後得出的結果\",{\"1\":{\"80\":1}}],[\"在這邊是以\",{\"1\":{\"249\":1}}],[\"在這邊我們在意的是評估的部分\",{\"1\":{\"21\":1}}],[\"在這裡採用\",{\"1\":{\"159\":1,\"160\":1}}],[\"在這裡能夠提供更好的幫助\",{\"1\":{\"74\":1}}],[\"在這一篇\",{\"1\":{\"142\":1}}],[\"在這一篇論文當中主要探討的是過去\",{\"1\":{\"70\":1}}],[\"在邊界上往往會出現誤差的問題解決\",{\"1\":{\"51\":1}}],[\"在限定幾款遊戲有特別出色的成效\",{\"1\":{\"38\":1}}],[\"在所有的\",{\"1\":{\"139\":1}}],[\"在所有\",{\"1\":{\"38\":1}}],[\"在搭配了\",{\"1\":{\"37\":1}}],[\"在下表當中可以看到\",{\"1\":{\"37\":1}}],[\"在最傾向\",{\"1\":{\"35\":1}}],[\"在每個\",{\"1\":{\"35\":1,\"37\":1}}],[\"在每一個\",{\"1\":{\"26\":1,\"157\":1}}],[\"在時間\",{\"1\":{\"27\":1,\"97\":1,\"155\":1,\"254\":1}}],[\"在目標的\",{\"1\":{\"21\":1}}],[\"在整個訓練過程當中沒有踏足過的狀態\",{\"1\":{\"19\":1}}],[\"在遊戲的過程當中並不會馬上知道現在這個操作對未來會有正面或是負面的影響\",{\"1\":{\"16\":1}}],[\"在剩下的遊戲當中這些\",{\"1\":{\"16\":1}}],[\"在\",{\"1\":{\"16\":1,\"19\":2,\"27\":1,\"30\":1,\"34\":1,\"35\":1,\"36\":2,\"37\":2,\"46\":1,\"49\":1,\"50\":1,\"51\":2,\"53\":1,\"54\":1,\"56\":2,\"57\":1,\"69\":1,\"70\":1,\"73\":1,\"74\":1,\"75\":1,\"79\":2,\"84\":1,\"91\":1,\"93\":1,\"97\":1,\"100\":1,\"116\":1,\"134\":1,\"137\":2,\"138\":2,\"139\":1,\"140\":1,\"153\":1,\"155\":1,\"158\":1,\"179\":1,\"182\":1,\"192\":1,\"195\":1,\"201\":1,\"202\":1,\"203\":1,\"216\":1,\"219\":1,\"220\":2,\"221\":3,\"222\":1,\"224\":1,\"226\":1,\"236\":1,\"245\":1,\"252\":2,\"254\":1,\"260\":1,\"261\":1}}],[\"在提供的\",{\"1\":{\"7\":1}}],[\"kernel\",{\"1\":{\"226\":2}}],[\"keychain\",{\"1\":{\"257\":1}}],[\"key\",{\"1\":{\"216\":3,\"218\":2,\"221\":3,\"254\":1}}],[\"kwik​\",{\"1\":{\"220\":1}}],[\"kj​​\",{\"1\":{\"216\":1}}],[\"ki​​\",{\"1\":{\"216\":1,\"221\":1}}],[\"kick\",{\"1\":{\"2\":2}}],[\"kl\",{\"1\":{\"196\":1}}],[\"kd\",{\"1\":{\"173\":1,\"197\":1}}],[\"krizhevsky\",{\"1\":{\"106\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1}}],[\"k=argmaxk\",{\"1\":{\"191\":1}}],[\"k=s⋅o\",{\"1\":{\"133\":1}}],[\"k=4\",{\"1\":{\"99\":1}}],[\"k=3\",{\"1\":{\"99\":1}}],[\"k=0∑k−1​rk​\",{\"1\":{\"27\":1}}],[\"koray\",{\"1\":{\"90\":1,\"97\":1,\"100\":1,\"101\":1}}],[\"knowagent\",{\"1\":{\"258\":2,\"260\":2}}],[\"knowledgeκ\",{\"1\":{\"251\":1}}],[\"knowledge\",{\"0\":{\"243\":1,\"247\":1,\"251\":1,\"252\":1,\"254\":1},\"1\":{\"73\":1,\"128\":1,\"131\":1,\"173\":1,\"182\":1,\"197\":1,\"198\":1,\"200\":1,\"203\":1,\"245\":9,\"247\":3,\"249\":3,\"251\":5,\"252\":9,\"253\":13,\"254\":13,\"258\":2,\"260\":18,\"261\":3}}],[\"know\",{\"1\":{\"65\":1}}],[\"k−τ\",{\"1\":{\"28\":2}}],[\"k−1\",{\"1\":{\"27\":1,\"28\":1}}],[\"kalman\",{\"1\":{\"165\":1}}],[\"kavukcuoglu\",{\"1\":{\"90\":1,\"97\":1,\"100\":1,\"101\":1}}],[\"kargmax1≤a≤n​μ^​k−1​\",{\"1\":{\"27\":1,\"28\":1}}],[\"kapturowski\",{\"1\":{\"15\":1,\"25\":1,\"31\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2}}],[\"k\",{\"1\":{\"27\":2,\"28\":1,\"99\":1,\"133\":5,\"155\":1,\"190\":1,\"191\":7,\"192\":9,\"193\":6,\"194\":9,\"195\":3,\"196\":7,\"216\":1,\"218\":2,\"220\":3,\"221\":1,\"226\":1}}],[\"讀的時候都覺得嗯嗯嗯很有道理\",{\"1\":{\"11\":1}}],[\"當取得\",{\"1\":{\"254\":1}}],[\"當規劃出整體的步驟後\",{\"1\":{\"245\":1}}],[\"當時遇到的阿普魯老師用他的熱情感染了每一個人\",{\"1\":{\"235\":1}}],[\"當時有遇到問題去找他們的時候都可以得到即時的\",{\"1\":{\"11\":1}}],[\"當距離很小時\",{\"1\":{\"193\":1}}],[\"當距離很大時\",{\"1\":{\"193\":1}}],[\"當我們把\",{\"1\":{\"139\":1}}],[\"當然也有對應的\",{\"1\":{\"173\":1}}],[\"當然\",{\"1\":{\"116\":1,\"192\":1,\"226\":1}}],[\"當作\",{\"1\":{\"116\":1,\"258\":1}}],[\"當作輸入\",{\"1\":{\"102\":1}}],[\"當要去更新模型的時候\",{\"1\":{\"97\":1}}],[\"當兩個很大的向量座內積的時候可能會得到過大或是過小的數值\",{\"1\":{\"218\":1}}],[\"當兩個\",{\"1\":{\"46\":1}}],[\"當\",{\"1\":{\"19\":1,\"35\":2,\"181\":1,\"216\":1,\"218\":1,\"221\":1}}],[\"當中可以降低許多\",{\"1\":{\"260\":1}}],[\"當中都可以觀察到相同的趨勢\",{\"1\":{\"260\":1}}],[\"當中都獲得了超過人類的成效\",{\"1\":{\"38\":1}}],[\"當中語意最接近的前\",{\"1\":{\"254\":1}}],[\"當中尋找下一個\",{\"1\":{\"254\":1}}],[\"當中所提及\",{\"1\":{\"254\":1}}],[\"當中所有合法的\",{\"1\":{\"254\":1}}],[\"當中會重複\",{\"1\":{\"221\":1}}],[\"當中設定的參數如下\",{\"1\":{\"220\":1}}],[\"當中相距較遠的元素\",{\"1\":{\"211\":1}}],[\"當中中心點的\",{\"1\":{\"194\":1}}],[\"當中同樣是\",{\"1\":{\"170\":1}}],[\"當中同一個\",{\"1\":{\"91\":1}}],[\"當中使用\",{\"1\":{\"258\":1}}],[\"當中使用的\",{\"1\":{\"224\":1}}],[\"當中使用的是\",{\"1\":{\"51\":1}}],[\"當中使用了\",{\"1\":{\"155\":1}}],[\"當中加上\",{\"1\":{\"151\":1}}],[\"當中就至少探索了上千億甚至到幾兆個模擬的遊戲狀態\",{\"1\":{\"149\":1}}],[\"當中就是環境給予的\",{\"1\":{\"19\":1}}],[\"當中則對於大物件有較差的表現\",{\"1\":{\"142\":1}}],[\"當中則對大多數的\",{\"1\":{\"138\":1}}],[\"當中整體包含了幾個方法\",{\"1\":{\"142\":1}}],[\"當中全部的\",{\"1\":{\"138\":1}}],[\"當中切出來\",{\"1\":{\"133\":1}}],[\"當中有\",{\"1\":{\"120\":1}}],[\"當中有包含的\",{\"1\":{\"76\":1}}],[\"當中包含\",{\"1\":{\"118\":1}}],[\"當中包含了\",{\"1\":{\"117\":2,\"135\":1,\"137\":1,\"227\":2,\"257\":2}}],[\"當中包含的\",{\"1\":{\"74\":1}}],[\"當中並沒有\",{\"1\":{\"100\":1}}],[\"當中如果要評估一個\",{\"1\":{\"100\":1}}],[\"當中的哪些元素\",{\"1\":{\"216\":1}}],[\"當中的特徵\",{\"1\":{\"170\":1}}],[\"當中的信心水平\",{\"1\":{\"131\":1}}],[\"當中的\",{\"1\":{\"97\":1,\"155\":1,\"170\":1,\"191\":1,\"197\":1,\"220\":1,\"249\":1}}],[\"當中取得隨機幾筆去更新\",{\"1\":{\"97\":1}}],[\"當中取得的\",{\"1\":{\"21\":1}}],[\"當中我們可以直接獲得訊息\",{\"1\":{\"226\":1}}],[\"當中我們需要同時訓練兩個\",{\"1\":{\"153\":1}}],[\"當中我們往往仰賴對\",{\"1\":{\"149\":1}}],[\"當中我們看到了使用\",{\"1\":{\"97\":1}}],[\"當中我們會透過\",{\"1\":{\"93\":1,\"216\":1}}],[\"當中我們會預設資料之間是沒有什麼相依性的\",{\"1\":{\"91\":1}}],[\"當中呢\",{\"1\":{\"91\":1}}],[\"當中常見的\",{\"1\":{\"79\":1}}],[\"當中通常\",{\"1\":{\"50\":1}}],[\"當中獲得啟發\",{\"1\":{\"107\":1}}],[\"當中獲得比人類平均還要好的成果\",{\"1\":{\"38\":1}}],[\"當中獲得相當不錯的\",{\"1\":{\"16\":1}}],[\"當中是小許多的\",{\"1\":{\"37\":1}}],[\"當中你可以得到最好的\",{\"1\":{\"27\":1}}],[\"當中\",{\"1\":{\"7\":1,\"16\":1,\"19\":1,\"32\":1,\"53\":1,\"97\":1,\"109\":1,\"120\":1,\"187\":1,\"192\":1,\"194\":1,\"219\":1,\"220\":1,\"258\":1,\"260\":2}}],[\"我現在找到冰箱了嗎\",{\"1\":{\"245\":1}}],[\"我要完成什麼步驟\",{\"1\":{\"245\":1}}],[\"我在來筑波之前有跟學長買好了腳踏車\",{\"1\":{\"237\":1}}],[\"我跟致越向櫃台詢問了相關的資訊\",{\"1\":{\"236\":1}}],[\"我還沒有理解這一段做了什麼\",{\"1\":{\"21\":1}}],[\"我也能感受到每個員工對我們都很友善\",{\"1\":{\"11\":1}}],[\"我覺得這次到\",{\"1\":{\"11\":1}}],[\"我們期待給定\",{\"1\":{\"253\":1}}],[\"我們期待根據\",{\"1\":{\"253\":1}}],[\"我們期待代表\",{\"1\":{\"216\":1}}],[\"我們總共會有兩個模型需要訓練\",{\"1\":{\"253\":1}}],[\"我們只要知道\",{\"1\":{\"252\":1}}],[\"我們雖然知道跟隨一個步驟可以完成任務\",{\"1\":{\"251\":1}}],[\"我們的\",{\"1\":{\"249\":1}}],[\"我們的目標是要讓\",{\"1\":{\"196\":1}}],[\"我們的目標是透過這些資料去學習\",{\"1\":{\"170\":1}}],[\"我們可以定義πϕ​\",{\"1\":{\"253\":1}}],[\"我們可以定義\",{\"1\":{\"253\":1}}],[\"我們可以事先理解許多事情\",{\"1\":{\"245\":1}}],[\"我們可以想成現在\",{\"1\":{\"49\":1}}],[\"我們稱呼這個事先預想的知識為global\",{\"1\":{\"245\":1}}],[\"我們希望產出相同大小的結果\",{\"1\":{\"220\":1}}],[\"我們設定有\",{\"1\":{\"220\":1}}],[\"我們認為模型已經訓練得差不多了\",{\"1\":{\"218\":1}}],[\"我們得到簡單的\",{\"1\":{\"216\":1}}],[\"我們需要多注意\",{\"1\":{\"216\":1}}],[\"我們其實更想知道的只是數值之間的大小差異\",{\"1\":{\"216\":1}}],[\"我們就給他更高的機率\",{\"1\":{\"254\":1}}],[\"我們就可以更好地運用\",{\"1\":{\"252\":1}}],[\"我們就可以得到一個\",{\"1\":{\"216\":1}}],[\"我們就希望讓他在訓練過程當中出現的頻率可以更高\",{\"1\":{\"75\":1}}],[\"我們一樣會有\",{\"1\":{\"178\":1}}],[\"我們最後的\",{\"1\":{\"163\":1}}],[\"我們最後決定要把多個模型的輸出拿去做類似\",{\"1\":{\"8\":1}}],[\"我們知道\",{\"1\":{\"134\":1}}],[\"我們想要從中切出一塊\",{\"1\":{\"133\":1}}],[\"我們想說這裡根本沒人戴安全帽\",{\"1\":{\"7\":1}}],[\"我們將\",{\"1\":{\"116\":1}}],[\"我們是從\",{\"1\":{\"97\":1}}],[\"我們理想上會預期那些\",{\"1\":{\"76\":1}}],[\"我們會把兩者都考慮進來\",{\"1\":{\"254\":1}}],[\"我們會把每個詞跟前後文之間的關係都直接計算出來\",{\"1\":{\"219\":1}}],[\"我們會去\",{\"1\":{\"254\":1}}],[\"我們會預設產出來的\",{\"1\":{\"251\":1}}],[\"我們會需要知道正確的步驟會是什麼\",{\"1\":{\"251\":1}}],[\"我們會在\",{\"1\":{\"219\":1}}],[\"我們會在一個\",{\"1\":{\"170\":1}}],[\"我們會說元素\",{\"1\":{\"216\":1}}],[\"我們會隨著訓練過程慢慢調整\",{\"1\":{\"192\":1}}],[\"我們會先設定好最基本的兩組\",{\"1\":{\"173\":1}}],[\"我們會寫成\",{\"1\":{\"157\":1}}],[\"我們會從其中取最大的當成是最終的\",{\"1\":{\"131\":1}}],[\"我們會盡可能讓出現頻率越低的\",{\"1\":{\"75\":1}}],[\"我們會選其中最大的當成是最後的答案\",{\"1\":{\"73\":1}}],[\"我們也定義\",{\"1\":{\"249\":1}}],[\"我們也看到使用\",{\"1\":{\"210\":1}}],[\"我們也就會期待\",{\"1\":{\"163\":1}}],[\"我們也可以再加上其他的技巧去做更多的處理\",{\"1\":{\"116\":1}}],[\"我們也可以去定義當前\",{\"1\":{\"73\":1}}],[\"我們也發現到模型越來越會傾向於\",{\"1\":{\"107\":1}}],[\"我們也發現到說在回答顏色的那一題\",{\"1\":{\"7\":1}}],[\"我們對於\",{\"1\":{\"50\":1}}],[\"我們透過\",{\"1\":{\"35\":1}}],[\"我們相信那些回答分數比較高的模型可以做得比較好\",{\"1\":{\"8\":1}}],[\"我們發現其中兩份都是教室的監視器錄影畫面\",{\"1\":{\"7\":1}}],[\"我們在意的是現在執行\",{\"1\":{\"252\":1}}],[\"我們在前一天列好了購買清單\",{\"1\":{\"237\":1}}],[\"我們在\",{\"1\":{\"7\":1}}],[\"我們這一組在\",{\"1\":{\"6\":1}}],[\"我們這一組拿到的是\",{\"1\":{\"5\":1}}],[\"y<i​\",{\"1\":{\"253\":1}}],[\"y∣y∣​\",{\"1\":{\"253\":1}}],[\"y2​\",{\"1\":{\"253\":1}}],[\"y1​\",{\"1\":{\"253\":1}}],[\"y=\",{\"1\":{\"253\":1}}],[\"y=wx+b⇒y=\",{\"1\":{\"157\":1}}],[\"yv​ˉ​mix\",{\"1\":{\"173\":1}}],[\"yv​ˉ​t\",{\"1\":{\"173\":2}}],[\"y∼p\",{\"1\":{\"152\":1}}],[\"yus​\",{\"1\":{\"173\":2}}],[\"yuille\",{\"1\":{\"144\":1}}],[\"yukai\",{\"1\":{\"134\":1}}],[\"yc\",{\"1\":{\"134\":1}}],[\"yds​\",{\"1\":{\"134\":2}}],[\"ylrs​\",{\"1\":{\"131\":1}}],[\"y^​t\",{\"1\":{\"191\":2,\"192\":1,\"194\":2,\"195\":1}}],[\"y^​t​\",{\"1\":{\"190\":1,\"195\":2}}],[\"y^​c\",{\"1\":{\"134\":3}}],[\"y^​c​=fs\",{\"1\":{\"133\":1}}],[\"y^​c​\",{\"1\":{\"133\":1}}],[\"y^​dt​\",{\"1\":{\"134\":1}}],[\"y^​ds​\",{\"1\":{\"134\":2}}],[\"y^​d\",{\"1\":{\"134\":1}}],[\"y^​d​=fs\",{\"1\":{\"133\":1}}],[\"y^​d​\",{\"1\":{\"133\":1,\"134\":1}}],[\"y^​lrt​\",{\"1\":{\"131\":1}}],[\"y^​lrs​\",{\"1\":{\"131\":1}}],[\"y^​lrs​=fθ​\",{\"1\":{\"131\":1}}],[\"y^​\",{\"1\":{\"131\":3}}],[\"yhrs\",{\"1\":{\"131\":2}}],[\"ys=\",{\"1\":{\"131\":1}}],[\"ysc​\",{\"1\":{\"76\":1}}],[\"ys\",{\"1\":{\"73\":1,\"75\":1,\"76\":2}}],[\"ys​=\",{\"1\":{\"73\":1,\"190\":1}}],[\"ys​\",{\"1\":{\"54\":5,\"73\":1,\"190\":2,\"191\":1}}],[\"yong\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"your\",{\"1\":{\"257\":2}}],[\"youtube\",{\"1\":{\"131\":1,\"132\":2,\"135\":1}}],[\"you\",{\"0\":{\"207\":1},\"1\":{\"65\":1,\"257\":10}}],[\"yolo\",{\"1\":{\"10\":1}}],[\"ym​\",{\"1\":{\"54\":4}}],[\"ya​\",{\"1\":{\"51\":1}}],[\"yang\",{\"1\":{\"50\":1,\"144\":1,\"169\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3}}],[\"yi​∣u\",{\"1\":{\"253\":1}}],[\"yi​−q\",{\"1\":{\"93\":1}}],[\"yi​​=es\",{\"1\":{\"93\":1}}],[\"yi\",{\"1\":{\"49\":2,\"56\":1,\"144\":1,\"169\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3}}],[\"yiheng\",{\"1\":{\"47\":1}}],[\"y\",{\"1\":{\"35\":1,\"112\":2,\"131\":4,\"152\":2,\"153\":3,\"154\":3,\"159\":10}}],[\"yk​\",{\"1\":{\"29\":1}}],[\"yt\",{\"1\":{\"73\":1}}],[\"yt​\",{\"1\":{\"73\":1,\"190\":2,\"191\":1}}],[\"yt​=\",{\"1\":{\"73\":1,\"190\":1}}],[\"yt​^​=ξ\",{\"1\":{\"191\":1}}],[\"yt​^​=t^q\",{\"1\":{\"21\":1}}],[\"yt​^​\",{\"1\":{\"21\":1,\"54\":2}}],[\"ytp\",{\"1\":{\"2\":1}}],[\"印象中有人用了\",{\"1\":{\"10\":1}}],[\"報告期間也都會跟我們分享他們覺得在每個地方有哪些比較好的做法也許可以嘗試看看\",{\"1\":{\"10\":1}}],[\"報告\",{\"0\":{\"10\":1}}],[\"7b\",{\"1\":{\"245\":2,\"258\":2,\"260\":2}}],[\"75\",{\"1\":{\"182\":1}}],[\"75​\",{\"1\":{\"141\":2}}],[\"720x720\",{\"1\":{\"181\":1}}],[\"79\",{\"1\":{\"37\":1}}],[\"76\",{\"1\":{\"37\":1,\"63\":1}}],[\"70\",{\"1\":{\"9\":1,\"37\":2}}],[\"7\",{\"1\":{\"9\":2,\"63\":2,\"80\":1,\"99\":1,\"120\":1,\"179\":1,\"237\":1,\"254\":1,\"257\":1,\"259\":1}}],[\"上下\",{\"1\":{\"260\":1}}],[\"上限為\",{\"1\":{\"260\":1}}],[\"上得到的結果比起看過的\",{\"1\":{\"260\":1}}],[\"上同樣可以獲得更多改善\",{\"1\":{\"203\":1}}],[\"上半部分是用\",{\"1\":{\"201\":1}}],[\"上也能好好地區分不同的\",{\"1\":{\"196\":1}}],[\"上也許我們能夠對各種物件去做標記\",{\"1\":{\"46\":1}}],[\"上產生比較雜亂的特徵\",{\"1\":{\"187\":1}}],[\"上則缺乏標記\",{\"1\":{\"170\":1}}],[\"上具有標記過的資料\",{\"1\":{\"170\":1}}],[\"上適用\",{\"1\":{\"164\":1}}],[\"上較為顯著\",{\"1\":{\"162\":1}}],[\"上圖當中的藍色區塊都是只在訓練階段包含的架構\",{\"1\":{\"176\":1}}],[\"上圖就是在最後分開成兩個輸出結果\",{\"1\":{\"154\":1}}],[\"上圖展現出\",{\"1\":{\"82\":1}}],[\"上加上\",{\"1\":{\"151\":1}}],[\"上加\",{\"1\":{\"151\":1}}],[\"上增加\",{\"1\":{\"151\":2}}],[\"上增加了\",{\"1\":{\"151\":1}}],[\"上鼓勵\",{\"1\":{\"151\":1}}],[\"上可以得到較好的結果\",{\"1\":{\"84\":1}}],[\"上多加上一項去\",{\"1\":{\"76\":1}}],[\"上述的三者分數都是以\",{\"1\":{\"74\":1}}],[\"上會發生\",{\"1\":{\"53\":1}}],[\"上訓練\",{\"1\":{\"159\":1,\"160\":1}}],[\"上訓練一個模型\",{\"1\":{\"50\":1}}],[\"上訓練的模型難以直接\",{\"1\":{\"46\":1}}],[\"上\",{\"1\":{\"46\":1,\"51\":2,\"65\":1,\"70\":1,\"76\":1,\"79\":1,\"84\":2,\"91\":1,\"119\":1,\"158\":1,\"162\":1,\"257\":1,\"258\":1,\"260\":2}}],[\"上面會有更多的正面影響\",{\"1\":{\"260\":1}}],[\"上面呈現的是使用\",{\"1\":{\"216\":1}}],[\"上面的表現\",{\"1\":{\"200\":1}}],[\"上面的差異就是這裡傳入的分別是\",{\"1\":{\"25\":1}}],[\"上面基本的做法作者稱他為\",{\"1\":{\"158\":1}}],[\"上面提及的是單純的\",{\"1\":{\"21\":1}}],[\"上的資料順利地給予正確的\",{\"1\":{\"170\":1}}],[\"上的簡短說明當中給出的架構圖也許會有更加直觀的理解\",{\"1\":{\"132\":1}}],[\"上的文章\",{\"1\":{\"118\":1}}],[\"上的重要性\",{\"1\":{\"70\":1}}],[\"上的認知\",{\"1\":{\"50\":1}}],[\"上的時候\",{\"1\":{\"47\":1}}],[\"上的例子\",{\"1\":{\"46\":1}}],[\"上的\",{\"1\":{\"21\":1,\"49\":1,\"100\":1}}],[\"上的分數大概是\",{\"1\":{\"9\":1}}],[\"上看起來很棒\",{\"1\":{\"8\":1}}],[\"如上圖所示\",{\"1\":{\"220\":1}}],[\"如上圖\",{\"1\":{\"174\":1}}],[\"如上面\",{\"1\":{\"83\":1}}],[\"如同上面的描述如下\",{\"1\":{\"218\":1}}],[\"如同字面意義能夠將一個\",{\"1\":{\"215\":1}}],[\"如同\",{\"1\":{\"128\":1,\"131\":1}}],[\"如同前面我們提到\",{\"1\":{\"224\":1}}],[\"如同前面\",{\"1\":{\"221\":1,\"254\":1}}],[\"如同前面看過的\",{\"1\":{\"187\":1}}],[\"如同前面所描述\",{\"1\":{\"134\":1}}],[\"如同前面描述\",{\"1\":{\"116\":1}}],[\"如同前面提及\",{\"1\":{\"35\":1}}],[\"如同過去看過的\",{\"1\":{\"70\":1}}],[\"如\",{\"1\":{\"37\":1,\"76\":2,\"91\":2,\"128\":1,\"138\":2,\"211\":1,\"245\":1}}],[\"如此一來就可以將前面的資訊傳遞下去\",{\"1\":{\"214\":1}}],[\"如此一來就可以得到完整的輸出結果\",{\"1\":{\"8\":1}}],[\"如此一來就能在\",{\"1\":{\"76\":1}}],[\"如此一來\",{\"1\":{\"26\":1,\"54\":1,\"158\":1,\"196\":1,\"220\":1,\"252\":1}}],[\"如果連\",{\"1\":{\"260\":1}}],[\"如果更進一步觀察可以發現到\",{\"1\":{\"260\":1}}],[\"如果一個\",{\"1\":{\"254\":1}}],[\"如果找到冰箱了\",{\"1\":{\"245\":1}}],[\"如果跟人類的決策方式去比較\",{\"1\":{\"245\":1}}],[\"如果你不能接受喝水道水\",{\"1\":{\"237\":1}}],[\"如果今天要將\",{\"1\":{\"219\":1}}],[\"如果採用低解析度的圖片\",{\"1\":{\"140\":1}}],[\"如果在很少量資料的狀況下\",{\"1\":{\"121\":1}}],[\"如果搭配\",{\"1\":{\"81\":1}}],[\"如果我們直接把\",{\"1\":{\"260\":1}}],[\"如果我們有多組的\",{\"1\":{\"220\":1}}],[\"如果我們把\",{\"1\":{\"220\":1}}],[\"如果我們看過於細節\",{\"1\":{\"128\":1}}],[\"如果我們用更加強大的\",{\"1\":{\"70\":1}}],[\"如果我們想要訓練一個模型去做自駕車的街景物件偵測\",{\"1\":{\"46\":1}}],[\"如果出現道路或甚至機車\",{\"1\":{\"50\":1}}],[\"如果選擇較大\",{\"1\":{\"35\":1}}],[\"如果\",{\"1\":{\"28\":1,\"253\":1}}],[\"如果每個\",{\"1\":{\"26\":1}}],[\"如果只會問那些固定的問題的話\",{\"1\":{\"6\":1}}],[\"如下圖所示\",{\"1\":{\"74\":1,\"119\":1}}],[\"如下\",{\"1\":{\"19\":1,\"32\":1,\"75\":1,\"76\":2,\"93\":1,\"112\":1,\"134\":1,\"174\":1,\"175\":1,\"176\":1,\"224\":1,\"253\":4}}],[\"如何計算\",{\"1\":{\"195\":1}}],[\"如何對應到\",{\"1\":{\"170\":1}}],[\"如何讓\",{\"1\":{\"16\":1}}],[\"如何決定哪些\",{\"1\":{\"16\":1}}],[\"這邊的\",{\"1\":{\"254\":1}}],[\"這邊選擇只要接近\",{\"1\":{\"227\":1}}],[\"這導致過去使用\",{\"1\":{\"245\":1}}],[\"這也許會是前往\",{\"1\":{\"260\":1}}],[\"這也許導致了\",{\"1\":{\"260\":1}}],[\"這也是\",{\"1\":{\"224\":1}}],[\"這也就是所謂的\",{\"1\":{\"223\":1}}],[\"這是因為原本的\",{\"1\":{\"224\":1}}],[\"這意味著輸出的內容會根據\",{\"1\":{\"221\":1}}],[\"這會導致無法好好判斷當前的狀態\",{\"1\":{\"252\":1}}],[\"這會導致\",{\"1\":{\"218\":1}}],[\"這三個\",{\"1\":{\"178\":1,\"200\":1}}],[\"這就是在這邊加上的\",{\"1\":{\"221\":1}}],[\"這就是\",{\"1\":{\"220\":1}}],[\"這就好比我們學會對應\",{\"1\":{\"170\":1}}],[\"這就像是不同地方的人行道也許會有不同的地磚設計\",{\"1\":{\"128\":1}}],[\"這就像是平常訓練的時候你有隊友可以\",{\"1\":{\"119\":1}}],[\"這就像是同理心\",{\"1\":{\"47\":1}}],[\"這卻忽略了在同一個\",{\"1\":{\"170\":1}}],[\"這篇\",{\"1\":{\"128\":1}}],[\"這跟前面提到只使用\",{\"1\":{\"53\":1}}],[\"這種類型的模型也同樣會在模型決策時提供其他的知識\",{\"1\":{\"258\":1}}],[\"這種單純的矩陣運算可以提高運算的平行度\",{\"1\":{\"211\":1}}],[\"這種包含了\",{\"1\":{\"74\":1}}],[\"這種相似的\",{\"1\":{\"53\":1}}],[\"這種\",{\"1\":{\"51\":2,\"128\":1,\"226\":1}}],[\"這種狀況下訓練模型就被稱為半監督式學習\",{\"1\":{\"50\":1}}],[\"這種差距被描述為\",{\"1\":{\"46\":1}}],[\"這類的\",{\"1\":{\"49\":1}}],[\"這樣的結果無論是在哪個\",{\"1\":{\"260\":1}}],[\"這樣的結果有多少\",{\"1\":{\"73\":1}}],[\"這樣的\",{\"1\":{\"221\":2}}],[\"這樣的機制在許多的任務當中都看到了不錯的結果\",{\"1\":{\"212\":1}}],[\"這樣的進步有蠻多部分是來自於對較難分類的類別的提升\",{\"1\":{\"201\":1}}],[\"這樣的方法會有過多的計算量\",{\"1\":{\"107\":1}}],[\"這樣的狀況之所以會出現有可能有幾個原因\",{\"1\":{\"107\":1}}],[\"這樣的想法自然而然就出現了\",{\"1\":{\"91\":1}}],[\"這樣的問題只在\",{\"1\":{\"53\":1}}],[\"這樣的做法下每一個\",{\"1\":{\"158\":1}}],[\"這樣的做法有趣的是能夠將\",{\"1\":{\"51\":1}}],[\"這樣的做法之所以可行\",{\"1\":{\"49\":1}}],[\"這樣所需要的成本會過大\",{\"1\":{\"46\":1}}],[\"這一點其實很貼心\",{\"1\":{\"236\":1}}],[\"這一點尤其在\",{\"1\":{\"82\":1}}],[\"這一款遊戲\",{\"1\":{\"36\":1}}],[\"這一篇與過去看過的\",{\"1\":{\"170\":1}}],[\"這一篇論文成功將\",{\"1\":{\"91\":1}}],[\"這一篇同樣也是先說明了\",{\"1\":{\"70\":1}}],[\"這一篇\",{\"1\":{\"16\":1,\"74\":1,\"187\":1}}],[\"這個結果展現了單一模型可以帶來相當強大的廣泛性\",{\"1\":{\"260\":1}}],[\"這個結果如果在取得\",{\"1\":{\"35\":1}}],[\"這個部分的做法是透過\",{\"1\":{\"258\":1}}],[\"這個步驟的目標是要產出\",{\"1\":{\"252\":1}}],[\"這個步驟的目標是產出\",{\"1\":{\"251\":1}}],[\"這個向量就像是在描述一個元素的特性\",{\"1\":{\"216\":1}}],[\"這個技術能夠將一個\",{\"1\":{\"212\":1}}],[\"這個參數會漸漸趨近於\",{\"1\":{\"163\":1}}],[\"這個論文提出的做法稱為\",{\"1\":{\"90\":1}}],[\"這個類別居然會隨著訓練時間預測結果越糟糕\",{\"1\":{\"83\":1}}],[\"這個\",{\"1\":{\"63\":1,\"117\":1,\"179\":1}}],[\"這個測量標準比較強調那些\",{\"1\":{\"34\":1}}],[\"這個問題被稱為\",{\"1\":{\"119\":1}}],[\"這個問題\",{\"1\":{\"26\":1}}],[\"這裡做了相關的實驗\",{\"1\":{\"260\":1}}],[\"這裡固定\",{\"1\":{\"260\":1}}],[\"這裡以\",{\"1\":{\"260\":1}}],[\"這裡選擇了\",{\"1\":{\"258\":1}}],[\"這裡選用的\",{\"1\":{\"74\":1}}],[\"這裡會只考慮\",{\"1\":{\"252\":1}}],[\"這裡會採用\",{\"1\":{\"73\":1}}],[\"這裡分成了兩個步驟\",{\"1\":{\"251\":1}}],[\"這裡我會簡單地說明\",{\"1\":{\"216\":1}}],[\"這裡十分推薦可以去看看\",{\"1\":{\"216\":1}}],[\"這裡作者採用\",{\"1\":{\"195\":1}}],[\"這裡設為\",{\"1\":{\"193\":1}}],[\"這裡之所以是拿\",{\"1\":{\"141\":1}}],[\"這裡也可以觀察到當我們把解析度提升\",{\"1\":{\"139\":1}}],[\"這裡也加進來\",{\"1\":{\"77\":1}}],[\"這裡使用的\",{\"1\":{\"137\":1,\"196\":1}}],[\"這裡先定義一下接下來會用到的基本\",{\"1\":{\"131\":1,\"190\":1}}],[\"這裡先簡單總結一下\",{\"1\":{\"80\":1}}],[\"這裡考慮有\",{\"1\":{\"93\":1}}],[\"這裡考慮到\",{\"1\":{\"73\":1}}],[\"這裡的作法是以\",{\"1\":{\"254\":1}}],[\"這裡的兩個矩陣大小分別是\",{\"1\":{\"222\":1}}],[\"這裡的距離是投射到高維空間之後\",{\"1\":{\"192\":1}}],[\"這裡的\",{\"1\":{\"73\":1,\"76\":1,\"131\":2,\"133\":1,\"220\":1,\"249\":1}}],[\"這裡的經驗指的是一個\",{\"1\":{\"28\":1}}],[\"這裡已經預設包含了\",{\"1\":{\"73\":1}}],[\"這裡要來實驗這一個做法實際上帶來多少影響\",{\"1\":{\"35\":1}}],[\"這裡就不贅述\",{\"1\":{\"34\":1}}],[\"這些預先知道的知識讓你決定接下來解決的步驟大概會像\",{\"1\":{\"245\":1}}],[\"這些圖片會被分類成\",{\"1\":{\"116\":1}}],[\"這些普遍做得不錯的\",{\"1\":{\"61\":1}}],[\"這些\",{\"1\":{\"26\":1,\"76\":1,\"227\":1}}],[\"這兩個\",{\"1\":{\"76\":1,\"82\":1}}],[\"這兩個問題\",{\"1\":{\"29\":1}}],[\"這兩者分別會讓\",{\"1\":{\"19\":1}}],[\"這兩款遊戲中我們發現到他們都需要相當長的時間之後才會得到\",{\"1\":{\"16\":1}}],[\"這款遊戲來說\",{\"1\":{\"16\":2}}],[\"這次大概是第一次實際碰\",{\"1\":{\"11\":1}}],[\"這七個問題\",{\"1\":{\"8\":1}}],[\"這場比賽是一組四人的比賽\",{\"1\":{\"4\":1}}],[\"簡單設計成一個\",{\"1\":{\"27\":1}}],[\"簡單來說\",{\"1\":{\"8\":1,\"216\":1}}],[\"簡單的\",{\"1\":{\"4\":1}}],[\"答案產出\",{\"0\":{\"8\":1}}],[\"是否真的能避免無謂的\",{\"1\":{\"260\":1}}],[\"是否有比較好\",{\"1\":{\"7\":1}}],[\"是從\",{\"1\":{\"260\":1}}],[\"是具有足夠的一般性\",{\"1\":{\"260\":1}}],[\"是將\",{\"1\":{\"258\":1}}],[\"是合法的\",{\"1\":{\"253\":1}}],[\"是讓\",{\"1\":{\"251\":1}}],[\"是包含一些需要避免的操作\",{\"1\":{\"251\":1}}],[\"是根據\",{\"1\":{\"249\":1}}],[\"是什麼\",{\"1\":{\"231\":1}}],[\"是困惑度\",{\"1\":{\"228\":1}}],[\"是為了把未來的資訊\",{\"1\":{\"221\":1}}],[\"是為了採用底下的特性方便後續\",{\"1\":{\"157\":1}}],[\"是不是東西都帶好了等等\",{\"1\":{\"236\":1}}],[\"是不會知道未來才會輸出的內容的\",{\"1\":{\"219\":1}}],[\"是不同的\",{\"1\":{\"19\":1}}],[\"是空的狀態\",{\"1\":{\"196\":1}}],[\"是由弱增強得到\",{\"1\":{\"196\":1}}],[\"是越高越好\",{\"1\":{\"181\":1}}],[\"是最終給出\",{\"1\":{\"173\":1}}],[\"是我們的模型\",{\"1\":{\"173\":1}}],[\"是預測的\",{\"1\":{\"173\":1}}],[\"是對應的\",{\"1\":{\"173\":1}}],[\"是把一個可訓練參數拆成\",{\"1\":{\"163\":1}}],[\"是使用\",{\"1\":{\"155\":1}}],[\"是被固定的參數\",{\"1\":{\"152\":1}}],[\"是上一個\",{\"1\":{\"152\":1}}],[\"是相當重要的\",{\"1\":{\"140\":1}}],[\"是單純的\",{\"1\":{\"134\":1}}],[\"是真的有在學習特徵\",{\"1\":{\"119\":1}}],[\"是類似的\",{\"1\":{\"119\":1}}],[\"是因為\",{\"1\":{\"141\":1}}],[\"是因為不同的\",{\"1\":{\"119\":1}}],[\"是因為即便是在很多\",{\"1\":{\"16\":1}}],[\"是只有使用\",{\"1\":{\"61\":1}}],[\"是源自於即便\",{\"1\":{\"49\":1}}],[\"是每個\",{\"1\":{\"26\":1}}],[\"是在校內的兵太郎池\",{\"1\":{\"237\":1}}],[\"是在\",{\"1\":{\"25\":2,\"151\":1,\"159\":1,\"160\":1,\"258\":1}}],[\"是一篇相當值得深讀的論文\",{\"1\":{\"230\":1}}],[\"是一種評價機器翻譯品質的方法\",{\"1\":{\"228\":1}}],[\"是一種\",{\"1\":{\"51\":1}}],[\"是一樣的\",{\"1\":{\"25\":1,\"63\":1}}],[\"是一個調整兩者重要性的\",{\"1\":{\"254\":1}}],[\"是一個跨時代的傑作\",{\"1\":{\"230\":1}}],[\"是一個用於手寫辨識的資料集\",{\"1\":{\"116\":1}}],[\"是一個\",{\"1\":{\"29\":3,\"76\":1,\"191\":1}}],[\"是一個可以用來評估或是用在\",{\"1\":{\"21\":1}}],[\"是一個相當重要的\",{\"1\":{\"16\":1}}],[\"是一個讓人很喜歡的環境\",{\"1\":{\"11\":1}}],[\"是\",{\"1\":{\"19\":2,\"21\":1,\"56\":1,\"73\":1,\"74\":1,\"96\":1,\"116\":1,\"131\":2,\"135\":1,\"160\":2,\"173\":1,\"191\":1,\"193\":1,\"215\":1,\"252\":1}}],[\"是也可以都\",{\"1\":{\"6\":1}}],[\"實質上都是一連串的\",{\"1\":{\"253\":1}}],[\"實踐的過程當中我們會需要理解當前所處的狀態\",{\"1\":{\"245\":1}}],[\"實驗\",{\"1\":{\"260\":1}}],[\"實驗結果\",{\"0\":{\"228\":1,\"229\":1,\"260\":1}}],[\"實驗是做在\",{\"1\":{\"162\":1}}],[\"實驗上調整了\",{\"1\":{\"99\":1}}],[\"實驗上為了檢測拿掉兩個\",{\"1\":{\"7\":1}}],[\"實驗做在三個模擬真實世界環境的\",{\"1\":{\"257\":1}}],[\"實驗做在\",{\"1\":{\"99\":1}}],[\"實驗設定\",{\"0\":{\"56\":1,\"79\":1,\"99\":1,\"137\":1,\"178\":1,\"200\":1,\"227\":1,\"256\":1}}],[\"實作上採用了常見的\",{\"1\":{\"79\":1,\"178\":1}}],[\"實作上\",{\"1\":{\"23\":1,\"97\":1}}],[\"實際上學到了怎樣的\",{\"1\":{\"261\":1}}],[\"實際上都是\",{\"1\":{\"253\":1}}],[\"實際上他們對於物理環境是沒有任何理解的\",{\"1\":{\"245\":1}}],[\"實際上他所謂的\",{\"1\":{\"198\":1}}],[\"實際上你還是可以依照狀況決定要哪一種架構\",{\"1\":{\"226\":1}}],[\"實際上每個元素都會再加上位置的資訊\",{\"1\":{\"224\":1}}],[\"實際上是指\",{\"1\":{\"216\":1}}],[\"實際上是有幫助的\",{\"1\":{\"37\":1}}],[\"實際上\",{\"1\":{\"196\":1,\"222\":1}}],[\"實際上對於\",{\"1\":{\"195\":1}}],[\"實際上的做法是把\",{\"1\":{\"174\":1}}],[\"實際上跟\",{\"1\":{\"154\":1}}],[\"實際上包含了\",{\"1\":{\"74\":1}}],[\"實際上還會為了讓\",{\"1\":{\"21\":1}}],[\"實際上訓練的\",{\"1\":{\"21\":1}}],[\"實際上我們挑了最小的\",{\"1\":{\"6\":1}}],[\"實際上在辦公室的時間沒有想像中的還要多\",{\"1\":{\"6\":1}}],[\"吐出更多結果\",{\"1\":{\"7\":1}}],[\"但平均落在\",{\"1\":{\"260\":1}}],[\"但似乎不是這麼一回事\",{\"1\":{\"240\":1}}],[\"但似乎又勇敢了點\",{\"1\":{\"240\":1}}],[\"但其實這條是人行道\",{\"1\":{\"238\":1}}],[\"但只要還不是晚上我想應該都還行\",{\"1\":{\"238\":1}}],[\"但椅子坐久了著實不大舒服w\",{\"1\":{\"237\":1}}],[\"但基本上什麼都有\",{\"1\":{\"236\":1}}],[\"但基本上都不會太難\",{\"1\":{\"4\":1}}],[\"但還是翻來覆去睡不著\",{\"1\":{\"236\":1}}],[\"但他們往往需要手動設定\",{\"1\":{\"247\":1}}],[\"但他們發現這樣會在最後得到更好的準確度與\",{\"1\":{\"227\":1}}],[\"但他只開放到晚上\",{\"1\":{\"236\":1}}],[\"但他只是希望把多的地方設為\",{\"1\":{\"134\":1}}],[\"但結果不一定正確\",{\"1\":{\"187\":1}}],[\"但比起\",{\"1\":{\"187\":1}}],[\"但卻不知道每個步驟安排的意義是什麼\",{\"1\":{\"251\":1}}],[\"但卻還是有\",{\"1\":{\"141\":1}}],[\"但卻隨著訓練過程慢慢地變糟\",{\"1\":{\"76\":1}}],[\"但我們都會認為那就是人行道\",{\"1\":{\"128\":1}}],[\"但又會逐漸趨緩\",{\"1\":{\"121\":1}}],[\"但隨著資料量越來越大\",{\"1\":{\"121\":1}}],[\"但因為結論都是可以看到\",{\"1\":{\"118\":1}}],[\"但有包含了部分的學習過程\",{\"1\":{\"101\":1}}],[\"但在\",{\"1\":{\"91\":1}}],[\"但在邊界上往往還是難以有好的結果\",{\"1\":{\"50\":1}}],[\"但數值範圍往往很\",{\"1\":{\"91\":1}}],[\"但數學有點太難\",{\"1\":{\"21\":1}}],[\"但透過剪貼則可以造成不同環境的突兀感\",{\"1\":{\"51\":1}}],[\"但主要的問題來自於\",{\"1\":{\"50\":1}}],[\"但並不\",{\"1\":{\"38\":1}}],[\"但最後能取得更好的\",{\"1\":{\"36\":1}}],[\"但整體來說兩者都能在最後趨近於\",{\"1\":{\"35\":1}}],[\"但是反過來當\",{\"1\":{\"260\":1}}],[\"但是從頭到尾都沒丟給\",{\"1\":{\"198\":1}}],[\"但是從擷取出的特徵當中可以很明顯的看出搭配\",{\"1\":{\"119\":1}}],[\"但是實作時卻使用\",{\"1\":{\"128\":1}}],[\"但是那些事情還是要處理\",{\"1\":{\"119\":1}}],[\"但是這需要結果趨近於無限大才可能發生\",{\"1\":{\"227\":1}}],[\"但是這些問題仍然存在\",{\"1\":{\"210\":1}}],[\"但是這些架構在\",{\"1\":{\"70\":1}}],[\"但是這種做法實際上效果很糟糕\",{\"1\":{\"53\":1}}],[\"但是並不全面\",{\"1\":{\"50\":1}}],[\"但是對於真實世界\",{\"1\":{\"46\":1}}],[\"但是在訓練上的效果看起來還不賴\",{\"1\":{\"119\":1}}],[\"但是在\",{\"1\":{\"38\":1,\"84\":1,\"100\":1}}],[\"但是\",{\"1\":{\"26\":1,\"170\":1}}],[\"但是他們在跟環境互動的過程當中會慢慢發現到自己的性格怎樣調整會在這個環境當中獲得更好的\",{\"1\":{\"26\":1}}],[\"但是卻跟其他人有同樣的影響力\",{\"1\":{\"23\":1}}],[\"但\",{\"1\":{\"25\":1,\"61\":1,\"219\":1,\"224\":1,\"226\":1}}],[\"但總結來說這次有還蠻有趣的體驗\",{\"1\":{\"11\":1}}],[\"但實際上有顏色才對\",{\"1\":{\"7\":1}}],[\"份\",{\"1\":{\"7\":1}}],[\"5th\",{\"1\":{\"257\":1}}],[\"5​⋅min\",{\"1\":{\"227\":1}}],[\"5​\",{\"1\":{\"160\":1}}],[\"55\",{\"1\":{\"63\":1,\"116\":1}}],[\"5595\",{\"1\":{\"37\":1}}],[\"53\",{\"1\":{\"63\":1}}],[\"50\",{\"1\":{\"37\":3,\"118\":1}}],[\"500分\",{\"1\":{\"2\":1}}],[\"52\",{\"1\":{\"16\":1}}],[\"51\",{\"1\":{\"16\":1}}],[\"57\",{\"1\":{\"16\":2,\"162\":1}}],[\"59\",{\"1\":{\"9\":1}}],[\"5\",{\"1\":{\"7\":1,\"9\":2,\"16\":1,\"80\":2,\"116\":3,\"120\":1,\"138\":2,\"179\":1,\"227\":4,\"254\":1,\"257\":2,\"259\":1,\"261\":1}}],[\"處理預測階段的\",{\"1\":{\"142\":1}}],[\"處理成相同\",{\"1\":{\"74\":1}}],[\"處理上花了不少的時間\",{\"1\":{\"7\":1}}],[\"處理\",{\"0\":{\"7\":1},\"1\":{\"25\":1,\"47\":1,\"76\":1,\"211\":1}}],[\"活下來\",{\"1\":{\"6\":1}}],[\"活動\",{\"1\":{\"3\":1}}],[\"活動參與\",{\"0\":{\"3\":1}}],[\"ot​\",{\"1\":{\"249\":1,\"254\":1}}],[\"o0​\",{\"1\":{\"249\":1,\"253\":1,\"254\":1}}],[\"o∈o\",{\"1\":{\"249\":1}}],[\"o2​\",{\"1\":{\"175\":2}}],[\"o1​\",{\"1\":{\"175\":2,\"249\":1,\"254\":1}}],[\"olivier\",{\"1\":{\"149\":1}}],[\"olsson\",{\"1\":{\"51\":2}}],[\"ohc​​×owc​​×c\",{\"1\":{\"134\":1}}],[\"o\",{\"1\":{\"133\":1,\"226\":5,\"249\":2}}],[\"observable\",{\"1\":{\"249\":1}}],[\"observations\",{\"1\":{\"249\":1}}],[\"observation\",{\"1\":{\"32\":1,\"249\":1,\"258\":1}}],[\"object\",{\"1\":{\"101\":1,\"230\":1}}],[\"optimizer\",{\"1\":{\"227\":2,\"259\":1}}],[\"optimal\",{\"1\":{\"93\":1,\"152\":1,\"154\":1}}],[\"open\",{\"1\":{\"257\":3}}],[\"openai\",{\"1\":{\"151\":3,\"157\":1}}],[\"operations\",{\"1\":{\"226\":2}}],[\"operation\",{\"1\":{\"21\":1}}],[\"operator\",{\"1\":{\"21\":1,\"25\":1}}],[\"orientation\",{\"1\":{\"238\":1}}],[\"oracle\",{\"1\":{\"74\":2,\"84\":1}}],[\"or\",{\"0\":{\"50\":1},\"1\":{\"60\":1}}],[\"overlapped\",{\"1\":{\"181\":1}}],[\"overlapping\",{\"0\":{\"135\":1},\"1\":{\"142\":2}}],[\"overhead\",{\"1\":{\"159\":1,\"160\":1,\"261\":1}}],[\"overview\",{\"0\":{\"111\":1,\"132\":1,\"138\":1}}],[\"overfit\",{\"1\":{\"83\":1,\"119\":1,\"192\":2}}],[\"overfitting\",{\"0\":{\"105\":1},\"1\":{\"70\":2,\"74\":1,\"85\":1,\"107\":3,\"109\":1,\"121\":1,\"122\":1}}],[\"over\",{\"0\":{\"26\":1}}],[\"on​\",{\"1\":{\"253\":1}}],[\"only\",{\"1\":{\"74\":1}}],[\"online\",{\"1\":{\"21\":1,\"25\":1,\"73\":2,\"155\":1,\"187\":1}}],[\"one\",{\"1\":{\"25\":1,\"73\":1,\"173\":3,\"258\":2}}],[\"on\",{\"0\":{\"116\":1,\"117\":1,\"118\":1,\"139\":1},\"1\":{\"16\":2,\"45\":1,\"56\":2,\"64\":1,\"65\":1,\"101\":1,\"135\":1,\"155\":1,\"164\":1,\"257\":2}}],[\"ocr\",{\"1\":{\"10\":1}}],[\"ouo\",{\"1\":{\"8\":1}}],[\"outlier\",{\"1\":{\"195\":1}}],[\"output\",{\"1\":{\"65\":1,\"133\":1,\"218\":1,\"221\":2,\"223\":1,\"245\":1}}],[\"outperform\",{\"1\":{\"16\":2,\"84\":1,\"94\":1,\"101\":1}}],[\"outperforming\",{\"0\":{\"14\":1},\"1\":{\"41\":1}}],[\"out\",{\"1\":{\"6\":1}}],[\"offline\",{\"1\":{\"73\":1}}],[\"off\",{\"1\":{\"41\":1,\"95\":1,\"155\":2,\"164\":1}}],[\"of\",{\"0\":{\"26\":1,\"121\":1,\"139\":1},\"1\":{\"6\":1,\"45\":2,\"107\":2,\"186\":2,\"244\":1,\"257\":1,\"258\":1},\"2\":{\"242\":1}}],[\"看看在前進之後能到怎樣的地方看到哪些有趣的風景\",{\"1\":{\"238\":1}}],[\"看看結果如何\",{\"1\":{\"6\":1}}],[\"看作是一種解讀事物的視角\",{\"1\":{\"220\":1}}],[\"看\",{\"1\":{\"50\":1}}],[\"看圖說故事\",{\"1\":{\"5\":1}}],[\"都不會影響到最後的結果\",{\"1\":{\"224\":1}}],[\"都不太好\",{\"1\":{\"62\":1}}],[\"都去計算他們的相似程度\",{\"1\":{\"216\":1}}],[\"都依賴於前一個狀態\",{\"1\":{\"214\":1}}],[\"都值得信任\",{\"1\":{\"173\":1}}],[\"都需要\",{\"1\":{\"158\":1}}],[\"都採用的話則可以進一步降低至\",{\"1\":{\"116\":1}}],[\"都有更低的\",{\"1\":{\"260\":1}}],[\"都有正面的影響\",{\"1\":{\"162\":1}}],[\"都有\",{\"1\":{\"111\":1,\"190\":1,\"222\":1}}],[\"都有相當大的改進\",{\"1\":{\"84\":1}}],[\"都對於\",{\"1\":{\"81\":1}}],[\"都可以用更少的平均\",{\"1\":{\"260\":1}}],[\"都可以對結果帶來好的影響\",{\"1\":{\"260\":1}}],[\"都可以得到更出色的結果\",{\"1\":{\"260\":1}}],[\"都可以得到更好的結果\",{\"1\":{\"229\":1}}],[\"都可以看到替換成\",{\"1\":{\"230\":1}}],[\"都可以被稱為\",{\"1\":{\"220\":1}}],[\"都可以進一步得到更好的最終結果\",{\"1\":{\"179\":1}}],[\"都可以獲得更好的結果\",{\"1\":{\"140\":1}}],[\"都可以有獲得提升\",{\"1\":{\"80\":1}}],[\"都可以使用\",{\"1\":{\"56\":1}}],[\"都被\",{\"1\":{\"76\":1}}],[\"都被其他\",{\"1\":{\"53\":1}}],[\"都具有\",{\"1\":{\"73\":1}}],[\"都是使用相同的\",{\"1\":{\"260\":1}}],[\"都是更好的\",{\"1\":{\"260\":1}}],[\"都是同樣的輸入\",{\"1\":{\"221\":1}}],[\"都是平等對待所導致\",{\"1\":{\"201\":1}}],[\"都是平等的\",{\"1\":{\"195\":1}}],[\"都是在幫助訓練\",{\"1\":{\"176\":1}}],[\"都是會逐漸趨近於\",{\"1\":{\"163\":1}}],[\"都是要對\",{\"1\":{\"157\":1}}],[\"都是\",{\"1\":{\"61\":1,\"154\":1}}],[\"都是虛擬世界當中的影像\",{\"1\":{\"57\":1}}],[\"都是採用\",{\"1\":{\"26\":1,\"79\":1,\"259\":1}}],[\"都會相差蠻多\",{\"1\":{\"260\":1}}],[\"都會帶來相近的結果\",{\"1\":{\"181\":1}}],[\"都會帶來\",{\"1\":{\"162\":1}}],[\"都會有正面的影響\",{\"1\":{\"139\":1}}],[\"都會有機率\",{\"1\":{\"112\":1}}],[\"都會在\",{\"1\":{\"119\":1}}],[\"都會變得比較分散\",{\"1\":{\"119\":1}}],[\"都會高過於原本的狀況\",{\"1\":{\"82\":1}}],[\"都會習慣使用\",{\"1\":{\"77\":1}}],[\"都會使用\",{\"1\":{\"76\":1}}],[\"都會對到\",{\"1\":{\"60\":1}}],[\"都會特別大\",{\"1\":{\"50\":1}}],[\"都會接收同樣的\",{\"1\":{\"25\":1}}],[\"都能夠有效地提升最後的\",{\"1\":{\"138\":1}}],[\"都能夠透過\",{\"1\":{\"50\":1}}],[\"都能夠學習什麼時候該\",{\"1\":{\"26\":1}}],[\"都獲得比過去\",{\"1\":{\"138\":1}}],[\"都獲得比\",{\"1\":{\"40\":1}}],[\"都另外加上一個\",{\"1\":{\"34\":1}}],[\"都當成是工廠生產出來的機器人\",{\"1\":{\"26\":1}}],[\"都\",{\"1\":{\"16\":1}}],[\"都完成了\",{\"1\":{\"10\":1}}],[\"都覺得很好理解\",{\"1\":{\"6\":1}}],[\"都相當地好吃\",{\"1\":{\"6\":1}}],[\"其方法與這一篇可說是大同小異\",{\"1\":{\"151\":1}}],[\"其餘則為\",{\"1\":{\"99\":1}}],[\"其中又以\",{\"1\":{\"260\":1}}],[\"其中幾段植物過於茂密\",{\"1\":{\"238\":1}}],[\"其中各個矩陣的大小如下\",{\"1\":{\"220\":1}}],[\"其中左邊是\",{\"1\":{\"217\":1}}],[\"其中一種方法是採用\",{\"1\":{\"191\":1}}],[\"其中一半的\",{\"1\":{\"51\":1}}],[\"其中的一些\",{\"1\":{\"173\":1}}],[\"其中的\",{\"1\":{\"26\":1}}],[\"其中\",{\"1\":{\"21\":2,\"27\":1,\"28\":1,\"29\":1,\"34\":1,\"93\":1,\"133\":1,\"134\":1,\"158\":1,\"174\":2,\"175\":1,\"191\":1,\"196\":1,\"222\":1,\"260\":1}}],[\"其他如\",{\"1\":{\"258\":1}}],[\"其他遊戲則都是\",{\"1\":{\"99\":1}}],[\"其他絕大多都是\",{\"1\":{\"50\":1}}],[\"其他像是路燈\",{\"1\":{\"49\":1}}],[\"其他\",{\"1\":{\"34\":1,\"84\":1}}],[\"其他部分基本上都跟\",{\"1\":{\"32\":1}}],[\"其他組的報告我們也可以透過實時的直播去看\",{\"1\":{\"10\":1}}],[\"其他的研究方向則是單純考慮到整體的工作流程\",{\"1\":{\"247\":1}}],[\"其他的\",{\"1\":{\"7\":1,\"61\":1}}],[\"其他的大概就是\",{\"1\":{\"6\":1}}],[\"其實在訓練初期其實是能夠辨別這些\",{\"1\":{\"76\":1}}],[\"其實\",{\"1\":{\"10\":1,\"163\":1}}],[\"其實不太好啦xd\",{\"1\":{\"9\":1}}],[\"其實讀\",{\"1\":{\"6\":1}}],[\"說明\",{\"1\":{\"6\":1}}],[\"v100\",{\"1\":{\"259\":1}}],[\"v0\",{\"1\":{\"258\":1}}],[\"vwiv​\",{\"1\":{\"220\":1}}],[\"v3​​\",{\"1\":{\"216\":1}}],[\"v\",{\"1\":{\"154\":5,\"173\":2,\"216\":2,\"218\":4,\"220\":3,\"221\":1}}],[\"volodymyr\",{\"1\":{\"90\":1,\"97\":1,\"100\":1,\"101\":1}}],[\"volvo\",{\"1\":{\"45\":1}}],[\"vgg\",{\"1\":{\"70\":1}}],[\"vase\",{\"1\":{\"257\":5}}],[\"vaswani\",{\"1\":{\"208\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1}}],[\"variations\",{\"0\":{\"228\":1}}],[\"variational\",{\"1\":{\"165\":1}}],[\"varied\",{\"1\":{\"65\":1}}],[\"van\",{\"1\":{\"69\":2,\"74\":5,\"80\":2,\"81\":1,\"82\":1,\"83\":2,\"84\":1,\"127\":1,\"128\":1,\"132\":1,\"133\":1,\"135\":1,\"138\":3,\"139\":2,\"140\":2,\"141\":1,\"142\":2,\"153\":1}}],[\"value\",{\"0\":{\"25\":1,\"35\":1},\"1\":{\"20\":2,\"21\":1,\"25\":1,\"32\":1,\"93\":1,\"94\":1,\"97\":1,\"152\":2,\"154\":1,\"155\":2,\"165\":1,\"218\":1,\"221\":1}}],[\"validation\",{\"1\":{\"8\":1,\"63\":4,\"100\":2}}],[\"veg\",{\"1\":{\"61\":1}}],[\"vectorki​​\",{\"1\":{\"216\":1}}],[\"vectorqi​​\",{\"1\":{\"216\":1}}],[\"vector\",{\"1\":{\"25\":1,\"173\":1,\"215\":1,\"216\":4}}],[\"venture\",{\"1\":{\"16\":1,\"37\":1}}],[\"v2​​\",{\"1\":{\"216\":1}}],[\"v2\",{\"1\":{\"6\":1,\"56\":1,\"61\":1}}],[\"vi​​\",{\"1\":{\"216\":1}}],[\"view\",{\"1\":{\"116\":1}}],[\"visually\",{\"1\":{\"216\":4,\"231\":1}}],[\"visual\",{\"1\":{\"96\":1,\"204\":2}}],[\"vision\",{\"1\":{\"0\":1,\"1\":1,\"45\":1,\"65\":1,\"96\":1},\"2\":{\"67\":1,\"88\":1,\"146\":1,\"184\":1,\"206\":1,\"233\":1}}],[\"virtual\",{\"1\":{\"60\":1}}],[\"viktor\",{\"1\":{\"51\":2}}],[\"via\",{\"0\":{\"44\":1,\"54\":1},\"1\":{\"65\":2,\"165\":1}}],[\"videos\",{\"1\":{\"151\":1}}],[\"video\",{\"1\":{\"16\":2}}],[\"vime\",{\"1\":{\"165\":1}}],[\"vim\",{\"1\":{\"1\":1}}],[\"大約會比沒使用\",{\"1\":{\"261\":1}}],[\"大約訓練了\",{\"1\":{\"227\":1}}],[\"大四這個時間點交換有好多事情會打在一起\",{\"1\":{\"239\":1}}],[\"大一剛入學的我因為好奇而選了一門日文課\",{\"1\":{\"235\":1}}],[\"大力推薦我的濬屹教授\",{\"1\":{\"234\":1}}],[\"大於某個\",{\"1\":{\"173\":1}}],[\"大致的目標是要在房子當中移動\",{\"1\":{\"257\":1}}],[\"大致分成了三個主軸\",{\"1\":{\"172\":1}}],[\"大致上也會是比較好的選擇點\",{\"1\":{\"120\":1}}],[\"大致上大家看過了幾個\",{\"1\":{\"6\":1}}],[\"大物件則有降低的趨勢\",{\"1\":{\"139\":1}}],[\"大概念上\",{\"1\":{\"133\":1}}],[\"大概是人生第一次走進台積辦公室\",{\"1\":{\"4\":1}}],[\"大小\",{\"1\":{\"26\":1,\"218\":2}}],[\"大小為\",{\"1\":{\"25\":1}}],[\"大\",{\"1\":{\"20\":1}}],[\"去觀察\",{\"1\":{\"260\":1}}],[\"去做設計\",{\"1\":{\"258\":1}}],[\"去做決策\",{\"1\":{\"254\":1}}],[\"去做實驗\",{\"1\":{\"178\":1}}],[\"去決策出來的結果\",{\"1\":{\"253\":1}}],[\"去決定最終的機率分布\",{\"1\":{\"254\":1}}],[\"去決定的\",{\"1\":{\"249\":1}}],[\"去決定要加怎樣的\",{\"1\":{\"157\":1}}],[\"去紀錄\",{\"1\":{\"252\":1}}],[\"去紀錄訓練過程當中的\",{\"1\":{\"34\":1}}],[\"去產出\",{\"1\":{\"251\":2}}],[\"去提供\",{\"1\":{\"245\":1}}],[\"去提升\",{\"1\":{\"82\":1,\"258\":1}}],[\"去洗手檯把雞蛋洗乾淨\",{\"1\":{\"245\":1}}],[\"去找到冰箱\",{\"1\":{\"245\":1}}],[\"去找到藏在地圖當中的寶藏\",{\"1\":{\"16\":1}}],[\"去宿舍中心交資料\",{\"1\":{\"238\":1}}],[\"去乘上\",{\"1\":{\"223\":1}}],[\"去計算\",{\"1\":{\"196\":1}}],[\"去計算出\",{\"1\":{\"131\":1}}],[\"去改善\",{\"1\":{\"182\":1}}],[\"去把應該要相近的\",{\"1\":{\"174\":1}}],[\"去加上\",{\"1\":{\"157\":1}}],[\"去儲存\",{\"1\":{\"155\":1}}],[\"去避免訓練資料上的強關聯性\",{\"1\":{\"155\":1}}],[\"去試圖得到\",{\"1\":{\"152\":1}}],[\"去增加\",{\"1\":{\"149\":1}}],[\"去看過整體的圖片\",{\"1\":{\"135\":1}}],[\"去看最終的效能是不是確實有提升\",{\"1\":{\"115\":1}}],[\"去幫助訓練\",{\"1\":{\"131\":1}}],[\"去當\",{\"1\":{\"116\":1}}],[\"去比較相似度\",{\"1\":{\"221\":1}}],[\"去比較\",{\"1\":{\"101\":1}}],[\"去選擇\",{\"1\":{\"97\":1}}],[\"去學\",{\"1\":{\"95\":1}}],[\"去學習\",{\"1\":{\"26\":1,\"35\":1,\"73\":1,\"97\":1,\"131\":1,\"152\":1,\"154\":1}}],[\"去學習導致\",{\"1\":{\"23\":1}}],[\"去預測\",{\"1\":{\"94\":1,\"134\":1,\"191\":1}}],[\"去初始化權重\",{\"1\":{\"76\":1}}],[\"去定義如下\",{\"1\":{\"75\":1}}],[\"去處理即可\",{\"1\":{\"135\":1}}],[\"去處理\",{\"1\":{\"74\":1}}],[\"去評估取得\",{\"1\":{\"74\":1}}],[\"去更新\",{\"1\":{\"73\":1,\"131\":1}}],[\"去更新參數學習\",{\"1\":{\"22\":1}}],[\"去訓練模型\",{\"1\":{\"259\":1}}],[\"去訓練基本上不會得到太好的結果\",{\"1\":{\"131\":1}}],[\"去訓練\",{\"1\":{\"70\":1,\"96\":1,\"131\":1,\"157\":1}}],[\"去訓練顯然很糟糕\",{\"1\":{\"61\":1,\"62\":1}}],[\"去\",{\"1\":{\"54\":1,\"61\":1,\"93\":1,\"131\":1}}],[\"去轉移出來\",{\"1\":{\"50\":1}}],[\"去判別現在給我的究竟是\",{\"1\":{\"49\":1}}],[\"去拉近\",{\"1\":{\"49\":1}}],[\"去跟環境互動\",{\"1\":{\"32\":1}}],[\"去調整\",{\"1\":{\"195\":1}}],[\"去調整選擇不同\",{\"1\":{\"26\":1}}],[\"去調查看看有哪些其他還不錯的\",{\"1\":{\"6\":1}}],[\"去得到目標\",{\"1\":{\"21\":1}}],[\"去逼近\",{\"1\":{\"21\":1}}],[\"去近似\",{\"1\":{\"20\":1,\"157\":1}}],[\"去解決也許會比\",{\"1\":{\"6\":1}}],[\"去解決這個問題\",{\"1\":{\"5\":1}}],[\"飲料\",{\"1\":{\"6\":1}}],[\"超過執行時間上限\",{\"1\":{\"249\":1}}],[\"超級懷疑這個\",{\"1\":{\"7\":1}}],[\"超級舒服的人體工學椅\",{\"1\":{\"6\":1}}],[\"超大的雙螢幕\",{\"1\":{\"6\":1}}],[\"電動的升降桌\",{\"1\":{\"6\":1}}],[\"點擊按鈕等操作去達成任務\",{\"1\":{\"257\":1}}],[\"點\",{\"1\":{\"236\":1}}],[\"點心\",{\"1\":{\"6\":1}}],[\"點半再來報到\",{\"1\":{\"6\":1}}],[\"點就要回家\",{\"1\":{\"6\":1}}],[\"95\",{\"1\":{\"116\":1}}],[\"98\",{\"1\":{\"63\":1,\"227\":1}}],[\"9400\",{\"1\":{\"60\":1}}],[\"90\",{\"1\":{\"37\":1}}],[\"96\",{\"1\":{\"37\":1}}],[\"999\",{\"1\":{\"83\":1}}],[\"9999\",{\"1\":{\"34\":1,\"37\":1,\"194\":1}}],[\"99\",{\"1\":{\"34\":1,\"37\":1}}],[\"9\",{\"1\":{\"6\":1,\"138\":1,\"142\":1,\"227\":2,\"236\":1,\"257\":1}}],[\"隔天早上\",{\"1\":{\"6\":1}}],[\"6uc\",{\"1\":{\"234\":1}}],[\"62\",{\"1\":{\"118\":1}}],[\"680\",{\"1\":{\"117\":1}}],[\"68\",{\"1\":{\"63\":1,\"181\":1,\"182\":1}}],[\"69571\",{\"1\":{\"37\":1}}],[\"6463\",{\"1\":{\"37\":1}}],[\"601\",{\"1\":{\"37\":1}}],[\"60\",{\"1\":{\"37\":2,\"116\":1,\"141\":1}}],[\"600分\",{\"1\":{\"2\":1}}],[\"6\",{\"1\":{\"6\":1,\"37\":1,\"79\":1,\"80\":2,\"120\":1,\"182\":1,\"216\":4,\"231\":1,\"236\":1,\"254\":1,\"257\":5}}],[\"一方面又因為大學成績普通以至於需要考慮更多校系\",{\"1\":{\"239\":1}}],[\"一般來說我們會期待正確的輸出要越接近\",{\"1\":{\"227\":1}}],[\"一次\",{\"1\":{\"162\":1}}],[\"一如既往\",{\"1\":{\"137\":1}}],[\"一塊\",{\"1\":{\"133\":1}}],[\"一\",{\"1\":{\"123\":1}}],[\"一起跟著跑\",{\"1\":{\"236\":1}}],[\"一起練習日文的致越也申請到了筑波的交換\",{\"1\":{\"236\":1}}],[\"一起\",{\"1\":{\"54\":1}}],[\"一開始都不太相同\",{\"1\":{\"99\":1}}],[\"一開始都是一樣的\",{\"1\":{\"26\":1}}],[\"一開始我們一樣先看一下這一篇論文當中會用到的\",{\"1\":{\"73\":1}}],[\"一開始進去到辦公室的感覺就很舒服\",{\"1\":{\"6\":1}}],[\"一個模擬在網購的網頁虛擬環境\",{\"1\":{\"257\":1}}],[\"一個在房子當中的環境\",{\"1\":{\"257\":1}}],[\"一個句子當中的內容如果調換\",{\"1\":{\"224\":1}}],[\"一個單詞決定要去看哪些\",{\"1\":{\"220\":1}}],[\"一個假的\",{\"1\":{\"187\":1}}],[\"一個理想上必定能夠避免\",{\"1\":{\"107\":1}}],[\"一個\",{\"1\":{\"94\":1,\"170\":1,\"249\":2,\"257\":1}}],[\"一個常見的問題是產出的結果通常會傾向去預測結果為常見的\",{\"1\":{\"50\":1}}],[\"一個簡單的方法是想辦法給這些\",{\"1\":{\"50\":1}}],[\"一個是\",{\"1\":{\"21\":2,\"226\":1}}],[\"一個喜愛資訊領域的人\",{\"1\":{\"0\":1}}],[\"一樣表示\",{\"1\":{\"175\":1}}],[\"一樣好\",{\"1\":{\"162\":1}}],[\"一樣糟\",{\"1\":{\"162\":1}}],[\"一樣\",{\"1\":{\"21\":1,\"56\":1}}],[\"一些\",{\"1\":{\"5\":1,\"32\":1,\"50\":1}}],[\"有一種踏實感\",{\"1\":{\"238\":1}}],[\"有一些常見的\",{\"1\":{\"57\":1}}],[\"有\",{\"1\":{\"237\":1}}],[\"有不少的提升\",{\"1\":{\"202\":1}}],[\"有不少人最後給的結果之所以那麼好看是因為\",{\"1\":{\"63\":1}}],[\"有多好多壞\",{\"1\":{\"154\":1}}],[\"有多少影響呢\",{\"1\":{\"36\":1}}],[\"有多少人沒戴安全帽\",{\"1\":{\"5\":1}}],[\"有多少人有戴安全帽\",{\"1\":{\"5\":1}}],[\"有更好的一般性\",{\"1\":{\"261\":1}}],[\"有更好的效果\",{\"1\":{\"153\":1}}],[\"有更好的處理\",{\"1\":{\"138\":1}}],[\"有更多的\",{\"1\":{\"151\":1}}],[\"有提及一個使用\",{\"1\":{\"149\":1}}],[\"有正向的幫助\",{\"1\":{\"131\":1}}],[\"有機率\",{\"1\":{\"111\":1}}],[\"有效地避免\",{\"1\":{\"107\":1}}],[\"有性生殖能夠讓子代有部分雙親的優勢\",{\"1\":{\"107\":1}}],[\"有大的影響\",{\"1\":{\"100\":1}}],[\"有所提升\",{\"1\":{\"81\":1}}],[\"有較高的機會被\",{\"1\":{\"75\":1}}],[\"有較大的影響\",{\"1\":{\"35\":1}}],[\"有另一個有趣的好處是\",{\"1\":{\"74\":1}}],[\"有許多相像的地方\",{\"1\":{\"74\":1}}],[\"有許多新的架構可以得到更高的\",{\"1\":{\"70\":1}}],[\"有許多的\",{\"1\":{\"22\":1}}],[\"有兩列分別表示\",{\"1\":{\"62\":1,\"179\":1}}],[\"有點偏以及\",{\"1\":{\"61\":1}}],[\"有還不錯的成效\",{\"1\":{\"97\":1}}],[\"有還不錯的\",{\"1\":{\"61\":1}}],[\"有可能就被誤判成人行道\",{\"1\":{\"50\":1}}],[\"有部分的認知\",{\"1\":{\"50\":1}}],[\"有相當大的差異\",{\"1\":{\"49\":1}}],[\"有最大的\",{\"1\":{\"38\":1}}],[\"有最好的結果\",{\"1\":{\"38\":1}}],[\"有了評斷信心水平的標準\",{\"1\":{\"73\":1}}],[\"有了\",{\"1\":{\"37\":1,\"251\":1}}],[\"有了目標\",{\"1\":{\"21\":1}}],[\"有人天生愛保險\",{\"1\":{\"26\":1}}],[\"有人天生愛探險\",{\"1\":{\"26\":1}}],[\"有時會很不穩定\",{\"1\":{\"23\":1}}],[\"有些趨近於\",{\"1\":{\"218\":1}}],[\"有些甚至是遞增的\",{\"1\":{\"163\":1}}],[\"有些是\",{\"1\":{\"62\":1}}],[\"有些傾向\",{\"1\":{\"26\":2}}],[\"有些則不需要\",{\"1\":{\"23\":1}}],[\"有些環境需要更多的\",{\"1\":{\"23\":1}}],[\"有些\",{\"1\":{\"23\":1,\"62\":1}}],[\"有些組別完成度很高\",{\"1\":{\"10\":1}}],[\"有些圖片上面會有\",{\"1\":{\"5\":1}}],[\"有沒有任何人違反了\",{\"1\":{\"5\":1}}],[\"安全帽是甚麼顏色\",{\"1\":{\"5\":1}}],[\"希望接下來可以有更多有趣的事件跟個人的成長\",{\"1\":{\"240\":1}}],[\"希望他們在\",{\"1\":{\"76\":1}}],[\"希望改善這兩個對\",{\"1\":{\"16\":1}}],[\"希望我們可以去找到\",{\"1\":{\"5\":1}}],[\"希望透過這個\",{\"1\":{\"0\":1}}],[\"的發生\",{\"1\":{\"261\":1}}],[\"的發生機率\",{\"1\":{\"260\":1}}],[\"的理由是因為會使模型感到困惑\",{\"1\":{\"260\":1}}],[\"的關鍵\",{\"1\":{\"260\":1}}],[\"的關聯性就能被連結起來\",{\"1\":{\"54\":1}}],[\"的頻率\",{\"1\":{\"260\":1}}],[\"的定義與使用方式等\",{\"1\":{\"258\":1}}],[\"的定義如下\",{\"1\":{\"93\":1,\"159\":1}}],[\"的過程當中引入不同的\",{\"1\":{\"258\":1}}],[\"的模擬環境\",{\"1\":{\"257\":1}}],[\"的模型都跟\",{\"1\":{\"162\":1}}],[\"的模型都是採用如\",{\"1\":{\"70\":1}}],[\"的模型在訓練過程當中那些被暫時移除的\",{\"1\":{\"113\":1}}],[\"的模型萃取出圖片的特徵\",{\"1\":{\"96\":1}}],[\"的模型得出來的結果\",{\"1\":{\"93\":1}}],[\"的模型\",{\"1\":{\"61\":1}}],[\"的模型對於\",{\"1\":{\"47\":1}}],[\"的模型雖然有許多\",{\"1\":{\"47\":1}}],[\"的模型會盡可能避開\",{\"1\":{\"35\":1}}],[\"的模型變成底下的樣子\",{\"1\":{\"25\":1}}],[\"的模型當成最後的結果\",{\"1\":{\"22\":1}}],[\"的次數\",{\"1\":{\"254\":1}}],[\"的行為\",{\"1\":{\"253\":1}}],[\"的溝通也出了問題\",{\"1\":{\"236\":1}}],[\"的身影\",{\"1\":{\"230\":1}}],[\"的兩個重大缺點\",{\"1\":{\"230\":1}}],[\"的兩倍\",{\"1\":{\"133\":1}}],[\"的維度越大時\",{\"1\":{\"223\":1}}],[\"的最後輸出了\",{\"1\":{\"221\":1}}],[\"的矩陣\",{\"1\":{\"220\":1}}],[\"的當下\",{\"1\":{\"219\":1}}],[\"的向量\",{\"1\":{\"216\":1}}],[\"的機制\",{\"1\":{\"216\":1}}],[\"的機率分布\",{\"1\":{\"254\":4}}],[\"的機率要越大越好\",{\"1\":{\"253\":1}}],[\"的機率有多高\",{\"1\":{\"253\":1}}],[\"的機率也就可以描述成\",{\"1\":{\"249\":1}}],[\"的機率不被使用到\",{\"1\":{\"111\":1}}],[\"的機率\",{\"1\":{\"26\":1,\"82\":1,\"191\":1}}],[\"的解說影片\",{\"1\":{\"216\":1}}],[\"的解決方案\",{\"1\":{\"201\":1}}],[\"的意思\",{\"1\":{\"215\":1}}],[\"的每個狀態\",{\"1\":{\"214\":1}}],[\"的紀錄成為新的\",{\"1\":{\"212\":1}}],[\"的基礎上去改善上述兩個缺點\",{\"1\":{\"210\":1}}],[\"的知識提取出來\",{\"1\":{\"258\":1}}],[\"的知識\",{\"1\":{\"197\":1}}],[\"的不平衡導致預測錯誤\",{\"1\":{\"195\":1}}],[\"的不同\",{\"1\":{\"49\":1}}],[\"的容忍程度\",{\"1\":{\"195\":1}}],[\"的所有\",{\"1\":{\"194\":1}}],[\"的中心點\",{\"1\":{\"193\":1}}],[\"的中心點就如同這裡的\",{\"1\":{\"192\":1}}],[\"的特徵擷取起來\",{\"1\":{\"221\":1}}],[\"的特徵中心點\",{\"1\":{\"193\":1}}],[\"的特徵拉近\",{\"1\":{\"175\":1}}],[\"的更新\",{\"1\":{\"192\":1}}],[\"的前提下\",{\"1\":{\"191\":1}}],[\"的區域過大\",{\"1\":{\"181\":1}}],[\"的提升\",{\"1\":{\"181\":1}}],[\"的通用性\",{\"1\":{\"178\":1}}],[\"的左下角\",{\"1\":{\"175\":1}}],[\"的右上角還是\",{\"1\":{\"175\":1}}],[\"的相似性\",{\"1\":{\"174\":1}}],[\"的曲線也可以發現到在不同的遊戲當中他們的更新曲線相當地不同\",{\"1\":{\"163\":1}}],[\"的數量\",{\"1\":{\"160\":2}}],[\"的形式\",{\"1\":{\"159\":2,\"173\":2}}],[\"的計算結果趨近於\",{\"1\":{\"219\":1}}],[\"的計算中間加上\",{\"1\":{\"219\":1}}],[\"的計算本質上就是找中心點\",{\"1\":{\"194\":1}}],[\"的計算方式如下\",{\"1\":{\"194\":1}}],[\"的計算\",{\"1\":{\"157\":1}}],[\"的想法是今天同一個\",{\"1\":{\"175\":1}}],[\"的想法就是希望具有相同\",{\"1\":{\"174\":1}}],[\"的想法基本上是相同方向\",{\"1\":{\"157\":1}}],[\"的想法跟\",{\"1\":{\"157\":1}}],[\"的算式\",{\"1\":{\"155\":1}}],[\"的論文\",{\"1\":{\"155\":2}}],[\"的概念就如同火影忍者的影分身之術\",{\"1\":{\"155\":1}}],[\"的概念仍然是透過\",{\"1\":{\"154\":1}}],[\"的總和就能夠得到\",{\"1\":{\"154\":1}}],[\"的決定上採用了\",{\"1\":{\"152\":1,\"154\":1}}],[\"的限制\",{\"1\":{\"151\":1,\"154\":1}}],[\"的效果\",{\"1\":{\"151\":1}}],[\"的效益參差不齊\",{\"1\":{\"119\":1}}],[\"的亂度越高越好\",{\"1\":{\"151\":1}}],[\"的時候都是透過增加\",{\"1\":{\"151\":1}}],[\"的各種\",{\"1\":{\"150\":1}}],[\"的記憶體\",{\"1\":{\"141\":1}}],[\"的記憶體空間足夠\",{\"1\":{\"128\":1}}],[\"的顯卡了\",{\"1\":{\"141\":1}}],[\"的有無對於最終結果的影響會是更大的\",{\"1\":{\"140\":1}}],[\"的預測有改善\",{\"1\":{\"195\":1}}],[\"的預測可以有更進一步的提升\",{\"1\":{\"139\":1}}],[\"的預測都有提升\",{\"1\":{\"139\":1}}],[\"的預測結果要與對應的\",{\"1\":{\"173\":1}}],[\"的預測結果要接近\",{\"1\":{\"54\":1}}],[\"的預測結果\",{\"1\":{\"49\":1,\"132\":1}}],[\"的部份會多上一個\",{\"1\":{\"226\":1}}],[\"的部份雖然我並沒有深入去理解\",{\"1\":{\"226\":1}}],[\"的部份由於\",{\"1\":{\"226\":1}}],[\"的部份一個是\",{\"1\":{\"226\":1}}],[\"的部份作為\",{\"1\":{\"221\":1}}],[\"的部份作者嘗試了解\",{\"1\":{\"181\":1}}],[\"的部份所提及\",{\"1\":{\"221\":1}}],[\"的部份\",{\"1\":{\"173\":1,\"221\":1}}],[\"的部份有怎樣的關聯\",{\"1\":{\"170\":1}}],[\"的部份使用的是\",{\"1\":{\"137\":1}}],[\"的部分與\",{\"1\":{\"221\":1}}],[\"的部分如下\",{\"1\":{\"202\":1}}],[\"的部分明顯可以看到最後的\",{\"1\":{\"201\":1}}],[\"的部分一如既往採用\",{\"1\":{\"200\":1}}],[\"的部分採用了\",{\"1\":{\"200\":1}}],[\"的部分在退步也是有幾項退步蠻多\",{\"1\":{\"162\":1}}],[\"的部分改用\",{\"1\":{\"159\":1,\"160\":1}}],[\"的部分作者另外的調整時選用的\",{\"1\":{\"79\":1}}],[\"的部分\",{\"1\":{\"74\":1}}],[\"的部分只能取得\",{\"1\":{\"74\":1}}],[\"的部分也可以發現到兩者的發展方向會稍有不同\",{\"1\":{\"35\":1}}],[\"的部分是分別給\",{\"1\":{\"25\":1}}],[\"的部分目的也是希望能夠促使\",{\"1\":{\"19\":1}}],[\"的部分主要是參考各個\",{\"1\":{\"6\":1}}],[\"的項\",{\"1\":{\"134\":1}}],[\"的值相乘\",{\"1\":{\"134\":1}}],[\"的交互考量\",{\"1\":{\"134\":1}}],[\"的一部分\",{\"1\":{\"134\":1,\"252\":1}}],[\"的一種\",{\"1\":{\"51\":1}}],[\"的邊長都會是\",{\"1\":{\"133\":1}}],[\"的小圖片\",{\"1\":{\"133\":1}}],[\"的優點帶走\",{\"1\":{\"143\":1}}],[\"的優勢\",{\"1\":{\"132\":1}}],[\"的優劣\",{\"1\":{\"38\":1}}],[\"的圖片去做混合\",{\"1\":{\"173\":1}}],[\"的圖片\",{\"1\":{\"133\":1}}],[\"的圖片解析度有\",{\"1\":{\"128\":1}}],[\"的圖片由於是透過車子上裝設攝影鏡頭去蒐集的\",{\"1\":{\"83\":1}}],[\"的資訊\",{\"1\":{\"128\":1,\"131\":1,\"135\":2,\"174\":1,\"221\":1,\"260\":1}}],[\"的資料拿進來使用\",{\"1\":{\"197\":1}}],[\"的資料去訓練\",{\"1\":{\"179\":1}}],[\"的資料分佈會隨著\",{\"1\":{\"91\":1,\"93\":1,\"97\":1}}],[\"的資料通常都會先\",{\"1\":{\"91\":1}}],[\"的資料\",{\"1\":{\"73\":2,\"131\":2,\"174\":1,\"198\":1}}],[\"的資料數量\",{\"1\":{\"73\":2,\"131\":2}}],[\"的資料不存在任何\",{\"1\":{\"50\":1}}],[\"的資料上只有一些\",{\"1\":{\"50\":1}}],[\"的彩色圖片\",{\"1\":{\"116\":1}}],[\"的門牌號碼彩色照片\",{\"1\":{\"116\":1}}],[\"的架構了\",{\"1\":{\"221\":1}}],[\"的架構圖如上所示\",{\"1\":{\"217\":1}}],[\"的架構由於\",{\"1\":{\"178\":1}}],[\"的架構\",{\"1\":{\"170\":1,\"226\":1}}],[\"的架構上採用\",{\"1\":{\"137\":1}}],[\"的架構都可以得到更好的結果\",{\"1\":{\"116\":1}}],[\"的架構如同前面所述\",{\"1\":{\"79\":1}}],[\"的手寫數字\",{\"1\":{\"116\":1}}],[\"的性能以及通用性\",{\"1\":{\"115\":1}}],[\"的編號\",{\"1\":{\"112\":1}}],[\"的輸出作為\",{\"1\":{\"221\":1}}],[\"的輸出\",{\"1\":{\"112\":1}}],[\"的輸出結果都乘上機率\",{\"1\":{\"111\":1}}],[\"的輸入當中就可以作為參考輸出下一個\",{\"1\":{\"252\":1}}],[\"的輸入輸出就會變成底下的樣子\",{\"1\":{\"112\":1}}],[\"的輸入去學習一直是一個很大的挑戰\",{\"1\":{\"91\":1}}],[\"的輸入\",{\"1\":{\"10\":1,\"112\":1}}],[\"的類別超級多\",{\"1\":{\"116\":1}}],[\"的類別\",{\"1\":{\"109\":1}}],[\"的位置以及類型\",{\"1\":{\"101\":1}}],[\"的平均去評估\",{\"1\":{\"100\":1}}],[\"的好壞就相對困難\",{\"1\":{\"100\":1}}],[\"的好壞\",{\"1\":{\"100\":1}}],[\"的目的是要讓整體的\",{\"1\":{\"93\":1}}],[\"的目標就是要讓整體的\",{\"1\":{\"93\":1}}],[\"的目標是希望結合\",{\"1\":{\"128\":1}}],[\"的目標是在改採用更佳的\",{\"1\":{\"70\":1}}],[\"的目標是把兩個不同分佈的\",{\"1\":{\"46\":1}}],[\"的改變而有巨大幅度的變化\",{\"1\":{\"91\":1,\"93\":1,\"97\":1}}],[\"的訓練資料具有高度相關性\",{\"1\":{\"91\":1,\"97\":1}}],[\"的訓練資料\",{\"1\":{\"91\":1}}],[\"的訓練上也是使用\",{\"1\":{\"73\":1}}],[\"的角度來看\",{\"1\":{\"91\":1}}],[\"的感官資料\",{\"1\":{\"91\":1}}],[\"的成果\",{\"1\":{\"182\":1}}],[\"的成長\",{\"1\":{\"138\":2,\"141\":2}}],[\"的成功\",{\"1\":{\"101\":1}}],[\"的成功也放進\",{\"1\":{\"91\":1}}],[\"的成功帶進\",{\"1\":{\"85\":1}}],[\"的成本過高\",{\"1\":{\"70\":1}}],[\"的影響力\",{\"1\":{\"155\":1}}],[\"的影響\",{\"1\":{\"85\":1,\"139\":1,\"143\":1,\"196\":1}}],[\"的影響程度\",{\"1\":{\"19\":1,\"155\":1}}],[\"的表達能力也有所侷限\",{\"1\":{\"216\":1}}],[\"的表達能力可以更強\",{\"1\":{\"83\":1}}],[\"的表現都比起其他架構來得好許多\",{\"1\":{\"74\":1}}],[\"的變化就比較不與\",{\"1\":{\"82\":1}}],[\"的變化很大程度跟\",{\"1\":{\"82\":1}}],[\"的情況下\",{\"1\":{\"82\":1,\"140\":1}}],[\"的情況下有很大的不同\",{\"1\":{\"75\":1}}],[\"的使用上如同過去我們看過的\",{\"1\":{\"79\":1}}],[\"的第\",{\"1\":{\"76\":1,\"173\":2,\"174\":1}}],[\"的產生方式可以是\",{\"1\":{\"73\":1}}],[\"的符號\",{\"1\":{\"73\":1,\"131\":1}}],[\"的描述\",{\"1\":{\"73\":1,\"112\":1}}],[\"的同時\",{\"1\":{\"70\":1}}],[\"的核心做法是不單只是跟\",{\"1\":{\"54\":1}}],[\"的步驟\",{\"1\":{\"51\":1}}],[\"的技巧\",{\"1\":{\"51\":1,\"99\":1}}],[\"的例子\",{\"1\":{\"50\":1}}],[\"的認識嚴重缺乏\",{\"1\":{\"196\":1}}],[\"的認識\",{\"1\":{\"50\":1}}],[\"的方式來處理\",{\"1\":{\"187\":1}}],[\"的方式解決\",{\"1\":{\"170\":1}}],[\"的方式\",{\"1\":{\"155\":1}}],[\"的方式是採用\",{\"1\":{\"21\":1}}],[\"的方法多\",{\"1\":{\"261\":1}}],[\"的方法上\",{\"1\":{\"138\":1}}],[\"的方法上雖然任何\",{\"1\":{\"56\":1}}],[\"的方法對於辨識細節相當不利\",{\"1\":{\"128\":1}}],[\"的方法會使用\",{\"1\":{\"73\":1}}],[\"的方法是把套上\",{\"1\":{\"73\":1}}],[\"的方法\",{\"1\":{\"53\":1,\"74\":1,\"95\":1,\"122\":1,\"131\":1,\"143\":1,\"149\":1,\"163\":1,\"203\":1}}],[\"的方法來降低這種問題\",{\"1\":{\"50\":1}}],[\"的方法解決了\",{\"1\":{\"50\":1}}],[\"的四個缺陷\",{\"1\":{\"38\":1}}],[\"的普遍性\",{\"1\":{\"38\":1}}],[\"的實作比較簡單\",{\"1\":{\"73\":1}}],[\"的實驗\",{\"1\":{\"37\":1}}],[\"的實用程度\",{\"1\":{\"7\":1}}],[\"的重要性\",{\"1\":{\"35\":1}}],[\"的趨勢仍然是隨著\",{\"1\":{\"35\":1}}],[\"的狀況缺乏認知\",{\"1\":{\"47\":1}}],[\"的狀況下\",{\"1\":{\"35\":1}}],[\"的狀況\",{\"1\":{\"35\":2}}],[\"的做法呢\",{\"1\":{\"226\":1}}],[\"的做法簡單來說就是兩件事情\",{\"1\":{\"111\":1}}],[\"的做法是對\",{\"1\":{\"109\":1}}],[\"的做法就是照著\",{\"1\":{\"53\":1}}],[\"的做法之所以能夠成功\",{\"1\":{\"49\":1}}],[\"的做法\",{\"1\":{\"35\":1,\"158\":1,\"211\":1}}],[\"的缺陷\",{\"1\":{\"35\":1}}],[\"的設計方式\",{\"1\":{\"223\":1}}],[\"的設計如下\",{\"1\":{\"222\":1}}],[\"的設計中會重複\",{\"1\":{\"221\":1}}],[\"的設計上也相當直覺\",{\"1\":{\"54\":1}}],[\"的設計是採用\",{\"1\":{\"30\":1}}],[\"的設定上會是\",{\"1\":{\"257\":1}}],[\"的設定上每個\",{\"1\":{\"220\":1}}],[\"的設定上對於目標被發現存在高估的問題\",{\"1\":{\"153\":1}}],[\"的設定上參考了許多過去的研究\",{\"1\":{\"56\":1}}],[\"的設定基本上跟\",{\"1\":{\"56\":1}}],[\"的設定下會大程度影響到最終\",{\"1\":{\"35\":1}}],[\"的設定取得的\",{\"1\":{\"35\":1}}],[\"的設定會透過\",{\"1\":{\"35\":1}}],[\"的設定詳閱論文的\",{\"1\":{\"34\":1}}],[\"的話大致上會在\",{\"1\":{\"120\":1}}],[\"的話會導致\",{\"1\":{\"95\":1}}],[\"的話\",{\"1\":{\"30\":1,\"91\":1}}],[\"的選用有關\",{\"1\":{\"82\":1}}],[\"的選項有更高機率被選擇到\",{\"1\":{\"27\":1}}],[\"的選擇上\",{\"1\":{\"181\":1}}],[\"的選擇相關\",{\"1\":{\"82\":1}}],[\"的選擇根據\",{\"1\":{\"32\":1}}],[\"的選擇應遠比\",{\"1\":{\"28\":1}}],[\"的選擇\",{\"1\":{\"23\":1}}],[\"的比較當中明顯看到在所有的成績都有所提升\",{\"1\":{\"38\":1}}],[\"的比例改變\",{\"1\":{\"26\":1}}],[\"的比賽經驗很不錯\",{\"1\":{\"11\":1}}],[\"的傾向\",{\"1\":{\"26\":1}}],[\"的存在\",{\"1\":{\"26\":1,\"173\":1}}],[\"的版本去訓練\",{\"1\":{\"134\":1}}],[\"的版本是少了\",{\"1\":{\"60\":1}}],[\"的版本\",{\"1\":{\"25\":1,\"62\":1,\"93\":1,\"216\":1,\"218\":1}}],[\"的參數更新\",{\"1\":{\"192\":1}}],[\"的參數加上\",{\"1\":{\"157\":1}}],[\"的參數\",{\"1\":{\"25\":2,\"155\":1,\"249\":1}}],[\"的作者認為是因為\",{\"1\":{\"23\":1}}],[\"的大圖片\",{\"1\":{\"133\":1}}],[\"的大小會比\",{\"1\":{\"220\":1}}],[\"的大小會是最恰當的\",{\"1\":{\"181\":1}}],[\"的大小平分\",{\"1\":{\"220\":1}}],[\"的大小是\",{\"1\":{\"220\":1}}],[\"的大小相差越來越懸殊\",{\"1\":{\"35\":1}}],[\"的大小下\",{\"1\":{\"35\":1}}],[\"的大小\",{\"1\":{\"23\":1,\"35\":1,\"99\":1,\"128\":1,\"190\":1,\"223\":1,\"252\":1}}],[\"的大家都很友善\",{\"1\":{\"10\":1}}],[\"的問題似乎能夠得到改善\",{\"1\":{\"30\":1}}],[\"的問題\",{\"0\":{\"23\":1},\"1\":{\"23\":1,\"30\":1,\"70\":1,\"85\":1,\"119\":1,\"187\":1}}],[\"的分布往往分散\",{\"1\":{\"195\":1}}],[\"的分布上有做了一點調整\",{\"1\":{\"34\":1}}],[\"的分布也會變動\",{\"1\":{\"28\":1}}],[\"的分布會變動的話\",{\"1\":{\"28\":1}}],[\"的分布是固定的狀況下會使用\",{\"1\":{\"27\":1}}],[\"的分布\",{\"1\":{\"20\":1}}],[\"的範圍\",{\"1\":{\"19\":1}}],[\"的環境當中有更好的成效\",{\"1\":{\"18\":1}}],[\"的地方都會是用相同的權重\",{\"1\":{\"223\":1}}],[\"的地方只能參考\",{\"1\":{\"134\":1}}],[\"的地方\",{\"1\":{\"16\":1}}],[\"的結果相加\",{\"1\":{\"221\":1}}],[\"的結果在細節上會有缺失\",{\"1\":{\"142\":1}}],[\"的結果後相加就會是\",{\"1\":{\"134\":1}}],[\"的結果以外\",{\"1\":{\"134\":1}}],[\"的結果要接近\",{\"1\":{\"54\":1}}],[\"的結果就當作是他的\",{\"1\":{\"50\":1}}],[\"的結果\",{\"1\":{\"16\":1,\"35\":1,\"178\":1,\"179\":2,\"220\":1}}],[\"的操作\",{\"1\":{\"8\":1}}],[\"的研究\",{\"1\":{\"6\":1,\"19\":1}}],[\"的回覆\",{\"1\":{\"6\":1}}],[\"的\",{\"1\":{\"6\":1,\"11\":1,\"21\":2,\"23\":1,\"25\":7,\"26\":2,\"29\":1,\"35\":2,\"37\":1,\"49\":2,\"54\":1,\"56\":1,\"59\":1,\"60\":1,\"62\":1,\"70\":1,\"74\":3,\"75\":1,\"76\":4,\"83\":1,\"91\":1,\"97\":2,\"109\":1,\"112\":1,\"116\":1,\"118\":1,\"119\":2,\"137\":1,\"149\":1,\"151\":1,\"152\":2,\"155\":1,\"157\":2,\"163\":2,\"170\":2,\"173\":2,\"174\":1,\"181\":1,\"187\":1,\"191\":1,\"193\":1,\"194\":2,\"197\":1,\"203\":1,\"219\":1,\"221\":2,\"238\":1,\"240\":1,\"245\":1,\"247\":1,\"251\":1,\"253\":4,\"260\":1}}],[\"的敘述\",{\"1\":{\"5\":1}}],[\"的題目\",{\"1\":{\"5\":1}}],[\"的共同創辦人之一\",{\"1\":{\"0\":1}}],[\"d=\",{\"1\":{\"253\":1}}],[\"d=e1​\",{\"1\":{\"97\":1}}],[\"dynamic\",{\"1\":{\"249\":1}}],[\"dm​​\",{\"1\":{\"223\":2}}],[\"dm​\",{\"1\":{\"220\":2,\"223\":1,\"224\":2}}],[\"dm​=512\",{\"1\":{\"220\":1,\"222\":1}}],[\"dv​\",{\"1\":{\"218\":1}}],[\"dk​=dv​=dm​\",{\"1\":{\"220\":1}}],[\"dk​​qkt​\",{\"1\":{\"218\":1}}],[\"dk​​\",{\"1\":{\"218\":1}}],[\"dk​\",{\"1\":{\"218\":3}}],[\"dθv​←dθv​+∂\",{\"1\":{\"155\":1}}],[\"dθ←dθ+∇θ\",{\"1\":{\"155\":1}}],[\"dueling\",{\"0\":{\"154\":1,\"159\":1},\"1\":{\"150\":1,\"154\":3,\"159\":2,\"162\":2}}],[\"dresser\",{\"1\":{\"257\":1}}],[\"drawer\",{\"1\":{\"257\":5}}],[\"dropout\",{\"0\":{\"105\":1,\"113\":1,\"119\":1},\"1\":{\"107\":1,\"109\":2,\"111\":1,\"112\":1,\"113\":4,\"115\":2,\"116\":8,\"117\":1,\"118\":3,\"119\":5,\"120\":2,\"121\":3,\"122\":1,\"227\":1}}],[\"driven\",{\"1\":{\"19\":1}}],[\"dl\",{\"1\":{\"91\":3}}],[\"decision\",{\"1\":{\"249\":1}}],[\"decoder\",{\"0\":{\"84\":1,\"221\":1},\"1\":{\"74\":3,\"79\":1,\"84\":1,\"109\":2,\"133\":1,\"134\":1,\"137\":2,\"210\":1,\"212\":1,\"215\":3,\"217\":1,\"221\":2,\"222\":1}}],[\"degeneration\",{\"1\":{\"196\":1}}],[\"detection\",{\"1\":{\"230\":1}}],[\"deterministic\",{\"1\":{\"163\":2}}],[\"details\",{\"1\":{\"160\":1}}],[\"detail\",{\"0\":{\"133\":1},\"1\":{\"133\":6,\"134\":8,\"135\":3,\"140\":2,\"142\":8}}],[\"description\",{\"0\":{\"112\":1}}],[\"denoising\",{\"0\":{\"109\":1,\"185\":1,\"192\":1},\"1\":{\"187\":1,\"198\":1,\"204\":1}}],[\"dengxin\",{\"1\":{\"69\":2,\"74\":5,\"80\":2,\"81\":1,\"82\":1,\"83\":2,\"84\":1,\"127\":1,\"128\":1,\"132\":1,\"133\":1,\"135\":1,\"138\":3,\"139\":2,\"140\":2,\"141\":1,\"142\":2}}],[\"depthwise\",{\"1\":{\"84\":1}}],[\"deeplabv2\",{\"1\":{\"74\":1,\"81\":1,\"137\":1,\"200\":1}}],[\"deeplabv3+\",{\"1\":{\"74\":1}}],[\"deeplab\",{\"1\":{\"56\":1,\"61\":1,\"70\":1}}],[\"deepmind\",{\"1\":{\"15\":1,\"148\":1}}],[\"deep\",{\"0\":{\"89\":1},\"1\":{\"0\":1,\"90\":1,\"91\":4,\"100\":1,\"102\":2,\"165\":2,\"216\":4,\"231\":1}}],[\"dt​\",{\"1\":{\"54\":1}}],[\"ds​\",{\"1\":{\"54\":1}}],[\"dot\",{\"0\":{\"218\":1},\"1\":{\"216\":2,\"220\":1}}],[\"dong\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"downsampling\",{\"1\":{\"131\":1}}],[\"downsample\",{\"1\":{\"76\":1,\"133\":1}}],[\"domain\",{\"0\":{\"44\":2,\"46\":1,\"49\":1,\"54\":2,\"68\":1,\"126\":1,\"168\":1,\"185\":1},\"1\":{\"46\":14,\"47\":5,\"49\":7,\"50\":10,\"53\":2,\"54\":6,\"61\":2,\"62\":1,\"65\":5,\"70\":1,\"71\":1,\"73\":6,\"74\":3,\"75\":1,\"83\":1,\"86\":2,\"128\":1,\"129\":1,\"131\":7,\"134\":1,\"144\":2,\"170\":11,\"171\":1,\"173\":11,\"174\":1,\"182\":1,\"187\":5,\"192\":2,\"196\":5,\"197\":2,\"198\":2,\"201\":1,\"204\":1,\"230\":1},\"2\":{\"67\":1,\"88\":1,\"146\":1,\"184\":1,\"206\":1}}],[\"double\",{\"0\":{\"153\":1},\"1\":{\"21\":2,\"150\":1,\"153\":4,\"154\":1}}],[\"diary\",{\"2\":{\"241\":1,\"242\":1}}],[\"diffusion\",{\"1\":{\"230\":1}}],[\"differences\",{\"1\":{\"165\":1}}],[\"dimension\",{\"1\":{\"218\":1}}],[\"dimensional\",{\"1\":{\"91\":1}}],[\"dimention\",{\"1\":{\"218\":1}}],[\"divergence代表什麼意義嗎\",{\"1\":{\"204\":1}}],[\"divergence\",{\"1\":{\"196\":1}}],[\"dilation\",{\"1\":{\"74\":1,\"79\":1}}],[\"directed\",{\"1\":{\"41\":1}}],[\"distilation\",{\"1\":{\"128\":1}}],[\"distillation\",{\"0\":{\"197\":1},\"1\":{\"73\":1,\"131\":1,\"173\":1,\"197\":1,\"198\":1,\"200\":1,\"203\":1}}],[\"distance\",{\"0\":{\"76\":1,\"83\":1},\"1\":{\"74\":1,\"76\":1,\"131\":1}}],[\"distribution\",{\"1\":{\"47\":1,\"100\":1,\"133\":1,\"152\":1}}],[\"distributed\",{\"0\":{\"160\":1},\"1\":{\"41\":2}}],[\"discriminator\",{\"1\":{\"49\":1}}],[\"discussion\",{\"0\":{\"39\":1}}],[\"discount\",{\"1\":{\"37\":1,\"93\":2}}],[\"disk\",{\"1\":{\"5\":1}}],[\"dae\",{\"1\":{\"109\":2}}],[\"daes\",{\"0\":{\"109\":1},\"1\":{\"109\":2}}],[\"dai\",{\"1\":{\"69\":2,\"74\":5,\"80\":2,\"81\":1,\"82\":1,\"83\":2,\"84\":1,\"127\":1,\"128\":1,\"132\":1,\"133\":1,\"135\":1,\"138\":3,\"139\":2,\"140\":2,\"141\":1,\"142\":2}}],[\"daformer\",{\"0\":{\"68\":1,\"74\":1,\"84\":1},\"1\":{\"69\":1,\"73\":1,\"74\":1,\"75\":1,\"80\":1,\"84\":3,\"86\":5,\"128\":2,\"129\":1,\"131\":5,\"135\":1,\"137\":3,\"138\":2,\"139\":1,\"141\":1,\"170\":1,\"173\":1,\"178\":1,\"179\":1,\"181\":1}}],[\"day\",{\"1\":{\"65\":2}}],[\"datatset\",{\"1\":{\"119\":1}}],[\"data\",{\"0\":{\"121\":1},\"1\":{\"46\":1,\"47\":2,\"49\":2,\"50\":7,\"51\":2,\"65\":1,\"73\":1,\"121\":2,\"187\":1,\"193\":1,\"196\":2,\"198\":1,\"229\":1,\"260\":3}}],[\"dataset\",{\"0\":{\"57\":1,\"118\":1,\"257\":1},\"1\":{\"7\":3,\"53\":6,\"74\":5,\"79\":1,\"107\":1,\"116\":1,\"117\":1,\"119\":1,\"179\":1,\"190\":3,\"191\":1,\"194\":1,\"200\":1,\"227\":3,\"253\":2,\"260\":3,\"261\":1}}],[\"datasets\",{\"0\":{\"7\":1,\"115\":1,\"116\":1},\"1\":{\"5\":1,\"7\":2,\"79\":1,\"115\":1,\"137\":1,\"178\":1,\"200\":2,\"227\":1,\"257\":1,\"260\":1}}],[\"dacs\",{\"0\":{\"44\":1,\"54\":1},\"1\":{\"49\":1,\"51\":1,\"54\":1,\"61\":1,\"62\":1,\"63\":3,\"65\":1,\"79\":1,\"170\":1,\"173\":1,\"187\":1,\"200\":1}}],[\"david\",{\"1\":{\"32\":1,\"90\":1,\"97\":1,\"100\":1,\"101\":1,\"153\":1}}],[\"dan\",{\"1\":{\"32\":1}}],[\"d\",{\"1\":{\"25\":2,\"76\":2,\"97\":1,\"152\":1,\"226\":1,\"253\":3}}],[\"dqn\",{\"0\":{\"152\":1,\"153\":1,\"154\":1,\"159\":2},\"1\":{\"21\":1,\"90\":1,\"96\":2,\"97\":2,\"101\":3,\"150\":3,\"152\":1,\"153\":8,\"154\":6,\"155\":1,\"159\":4,\"162\":3}}],[\"dpo\",{\"1\":{\"258\":1}}],[\"dp\",{\"1\":{\"4\":1}}],[\"前進有些不舒服\",{\"1\":{\"238\":1}}],[\"前進的速度也相當緩慢\",{\"1\":{\"236\":1}}],[\"前面提到\",{\"1\":{\"218\":1}}],[\"前面所提及的\",{\"1\":{\"215\":1}}],[\"前面的狀態在傳遞過程中也會不斷被稀釋\",{\"1\":{\"214\":1}}],[\"前面序列的資訊隨著長度越長會逐漸被稀釋\",{\"1\":{\"210\":1}}],[\"前面都加上一組\",{\"1\":{\"26\":1}}],[\"前面有一個預賽\",{\"1\":{\"4\":1}}],[\"前就已經做了不少\",{\"1\":{\"6\":1}}],[\"前幾天去參加了\",{\"1\":{\"4\":1}}],[\"台積電的黑客松\",{\"1\":{\"4\":1}}],[\"臺灣好厲駭\",{\"1\":{\"3\":1}}],[\"iclr\",{\"1\":{\"148\":1,\"151\":1},\"2\":{\"167\":1}}],[\"icml\",{\"1\":{\"15\":1,\"247\":1},\"2\":{\"43\":1}}],[\"i−obi\",{\"1\":{\"134\":1}}],[\"ijc\",{\"1\":{\"131\":1}}],[\"ijct​=\",{\"1\":{\"131\":1}}],[\"ijc​​\",{\"1\":{\"131\":1}}],[\"ilya\",{\"1\":{\"106\":1}}],[\"iverson\",{\"1\":{\"73\":1,\"131\":1}}],[\"i=1∑∣au​∣​pagent​\",{\"1\":{\"254\":1}}],[\"i=1∑∣au​∣​pknow​\",{\"1\":{\"254\":1}}],[\"i=1∣d\",{\"1\":{\"253\":1}}],[\"i=1∣d∣​\",{\"1\":{\"253\":1}}],[\"i=1∣b∣​\",{\"1\":{\"252\":1}}],[\"i=1nt​​\",{\"1\":{\"73\":2}}],[\"i=1ns​​\",{\"1\":{\"73\":2}}],[\"i=0∑k​eπ\",{\"1\":{\"160\":1}}],[\"i=0∑k​∇ζπ​​log\",{\"1\":{\"160\":1}}],[\"i=0∑k​∇θπ​​log\",{\"1\":{\"155\":1,\"160\":1}}],[\"i=t+1∏s​ci​\",{\"1\":{\"21\":1}}],[\"i\",{\"1\":{\"73\":17,\"75\":1,\"76\":14,\"112\":2,\"134\":3,\"155\":1,\"174\":5,\"175\":2,\"191\":7,\"192\":7,\"193\":6,\"194\":3,\"195\":3,\"196\":7,\"216\":1,\"249\":1,\"252\":1,\"253\":2,\"254\":9}}],[\"issue\",{\"1\":{\"196\":1}}],[\"issues\",{\"0\":{\"63\":1}}],[\"is\",{\"0\":{\"46\":1,\"207\":1},\"1\":{\"257\":4}}],[\"isip\",{\"1\":{\"3\":1}}],[\"improving\",{\"0\":{\"68\":1},\"1\":{\"74\":1,\"86\":2}}],[\"improvement\",{\"1\":{\"37\":1,\"162\":2}}],[\"images\",{\"1\":{\"58\":1,\"59\":1,\"60\":1}}],[\"imagenet\",{\"0\":{\"76\":1,\"83\":1},\"1\":{\"56\":1,\"74\":1,\"76\":4,\"79\":1,\"116\":3,\"131\":1}}],[\"image\",{\"0\":{\"116\":1},\"1\":{\"19\":1,\"20\":1,\"22\":1,\"23\":2,\"25\":1,\"31\":1,\"34\":2,\"35\":3,\"36\":2,\"37\":2,\"38\":2,\"46\":2,\"47\":2,\"49\":2,\"50\":2,\"51\":5,\"53\":2,\"54\":2,\"58\":1,\"59\":1,\"60\":1,\"61\":2,\"62\":1,\"69\":1,\"71\":1,\"74\":5,\"80\":2,\"81\":1,\"82\":1,\"83\":3,\"84\":2,\"97\":2,\"100\":2,\"101\":2,\"109\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1,\"128\":1,\"129\":1,\"131\":1,\"132\":2,\"133\":1,\"134\":1,\"135\":2,\"138\":3,\"139\":2,\"140\":2,\"141\":1,\"142\":2,\"151\":1,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"162\":3,\"163\":1,\"174\":1,\"175\":2,\"176\":2,\"179\":4,\"180\":2,\"181\":3,\"187\":1,\"192\":1,\"201\":2,\"202\":1,\"214\":2,\"215\":1,\"216\":4,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1,\"247\":2,\"250\":1,\"257\":3,\"260\":7}}],[\"instruct\",{\"1\":{\"258\":2}}],[\"instructblip\",{\"1\":{\"6\":1}}],[\"insturction\",{\"1\":{\"249\":1}}],[\"interaction\",{\"1\":{\"257\":1}}],[\"interactive\",{\"1\":{\"257\":1}}],[\"internet\",{\"1\":{\"247\":1}}],[\"intra\",{\"1\":{\"182\":1}}],[\"introduce\",{\"1\":{\"64\":1}}],[\"intrinsic\",{\"0\":{\"19\":1},\"1\":{\"18\":1,\"19\":4,\"22\":1,\"25\":3,\"32\":1,\"35\":3}}],[\"initialize\",{\"1\":{\"160\":1}}],[\"independent\",{\"1\":{\"158\":1,\"160\":2}}],[\"inference\",{\"0\":{\"259\":1},\"1\":{\"252\":2,\"254\":1,\"261\":1}}],[\"influence\",{\"0\":{\"139\":1}}],[\"informatics\",{\"1\":{\"69\":1,\"127\":1}}],[\"information\",{\"0\":{\"15\":1,\"45\":1,\"69\":1,\"90\":1,\"106\":1,\"127\":1,\"148\":1,\"169\":1,\"186\":1,\"208\":1,\"244\":1},\"1\":{\"74\":1,\"165\":1}}],[\"info\",{\"1\":{\"16\":1,\"47\":1,\"49\":1,\"50\":4,\"53\":1,\"61\":1,\"62\":1,\"73\":1,\"97\":1,\"118\":1,\"119\":1,\"128\":1,\"131\":1,\"149\":1,\"151\":1,\"154\":1,\"155\":1,\"170\":1,\"179\":1,\"216\":1,\"224\":1,\"226\":1,\"245\":1,\"258\":3}}],[\"invaders\",{\"1\":{\"99\":2}}],[\"input\",{\"1\":{\"96\":1,\"109\":1,\"112\":1,\"160\":2,\"211\":1,\"216\":1,\"221\":3,\"223\":1}}],[\"in\",{\"1\":{\"41\":2,\"64\":1,\"65\":1,\"107\":1,\"215\":1,\"216\":4,\"231\":2,\"257\":4}}],[\"it\",{\"1\":{\"11\":1,\"257\":1,\"258\":1}}],[\"iou\",{\"1\":{\"82\":4,\"83\":1}}],[\"ioicamp\",{\"1\":{\"3\":1}}],[\"ioncamp\",{\"1\":{\"3\":1}}],[\"奧義科技參訪\",{\"1\":{\"3\":1}}],[\"課程講師\",{\"1\":{\"3\":2}}],[\"索拉教育\",{\"1\":{\"3\":2}}],[\"班講師\",{\"1\":{\"3\":1}}],[\"營長\",{\"1\":{\"3\":1}}],[\"講師\",{\"1\":{\"3\":1}}],[\"日本的交通違規還是有\",{\"1\":{\"240\":1}}],[\"日本人好像沒有像過去印象中總是視客人為上帝的感覺了\",{\"1\":{\"240\":1}}],[\"日本喝水道水的習慣一開始真的不大適應\",{\"1\":{\"237\":1}}],[\"日\",{\"1\":{\"236\":1}}],[\"日文課程當中不厭其煩地協助我的川越老師\",{\"1\":{\"234\":1}}],[\"日語學習小組\",{\"1\":{\"3\":1}}],[\"日期\",{\"1\":{\"2\":1,\"3\":1}}],[\"成一個\",{\"1\":{\"227\":1}}],[\"成功結合\",{\"1\":{\"102\":1}}],[\"成功在雙陸棋上面\",{\"1\":{\"94\":1}}],[\"成功在幾乎所有的\",{\"1\":{\"84\":1}}],[\"成功將\",{\"1\":{\"85\":1}}],[\"成功避免了預測結果變差的狀況\",{\"1\":{\"83\":1}}],[\"成新的\",{\"1\":{\"53\":1}}],[\"成員\",{\"1\":{\"3\":2}}],[\"成績\",{\"1\":{\"2\":1}}],[\"8b\",{\"1\":{\"245\":1,\"258\":1}}],[\"8\",{\"1\":{\"80\":2,\"101\":1,\"117\":1,\"120\":1,\"141\":1,\"165\":1,\"227\":1,\"257\":1,\"259\":1}}],[\"83\",{\"1\":{\"37\":1,\"63\":1}}],[\"89\",{\"1\":{\"37\":2}}],[\"84\",{\"1\":{\"37\":1,\"63\":1}}],[\"80\",{\"1\":{\"30\":2}}],[\"800000\",{\"1\":{\"118\":1}}],[\"800\",{\"1\":{\"2\":1}}],[\"81\",{\"1\":{\"1\":1}}],[\"1e\",{\"1\":{\"259\":1}}],[\"1≤i≤∣au​∣argmax​\",{\"1\":{\"254\":1}}],[\"1m\",{\"1\":{\"162\":1}}],[\"15\",{\"1\":{\"83\":1}}],[\"15×15\",{\"1\":{\"35\":1}}],[\"1k\",{\"1\":{\"79\":1}}],[\"18\",{\"1\":{\"79\":1,\"236\":1}}],[\"1−γ\",{\"1\":{\"254\":1}}],[\"1−λ\",{\"1\":{\"194\":1}}],[\"1−λd​\",{\"1\":{\"134\":2}}],[\"1−ac\",{\"1\":{\"134\":1}}],[\"1−fc​\",{\"1\":{\"75\":1}}],[\"1−α\",{\"1\":{\"73\":1}}],[\"17\",{\"1\":{\"63\":1}}],[\"1774\",{\"1\":{\"37\":1}}],[\"13\",{\"1\":{\"60\":2,\"62\":2,\"63\":1,\"179\":2,\"244\":1}}],[\"13b\",{\"1\":{\"6\":1}}],[\"19\",{\"1\":{\"58\":1,\"59\":1}}],[\"120\",{\"1\":{\"83\":1}}],[\"12\",{\"1\":{\"79\":1,\"227\":1,\"257\":1,\"259\":1}}],[\"12326\",{\"1\":{\"37\":1}}],[\"128\",{\"1\":{\"2\":1}}],[\"11\",{\"1\":{\"257\":1}}],[\"1187\",{\"1\":{\"37\":1}}],[\"11361\",{\"1\":{\"37\":1}}],[\"1177\",{\"1\":{\"37\":1}}],[\"14814\",{\"1\":{\"37\":1}}],[\"14\",{\"1\":{\"37\":1,\"80\":1}}],[\"16\",{\"1\":{\"37\":1,\"60\":1,\"62\":2,\"63\":1,\"85\":1,\"179\":2}}],[\"1664\",{\"1\":{\"37\":1}}],[\"16926\",{\"1\":{\"37\":1}}],[\"160\",{\"1\":{\"30\":1}}],[\"1​​≤i<obd\",{\"1\":{\"134\":1}}],[\"1​​≤i<s⋅obd\",{\"1\":{\"134\":1}}],[\"1​​\",{\"1\":{\"134\":1}}],[\"1​​yk​​∀0≤k≤n−1∀n≤k≤k−1\",{\"1\":{\"29\":1}}],[\"1​+hd​bd\",{\"1\":{\"133\":1}}],[\"1​+shc​bc\",{\"1\":{\"133\":1}}],[\"1​∼u\",{\"1\":{\"133\":2}}],[\"1​\",{\"1\":{\"133\":2}}],[\"1​m=max\",{\"1\":{\"28\":1}}],[\"1​m=0∑k−1​rk​\",{\"1\":{\"27\":1}}],[\"1\",{\"1\":{\"19\":1,\"21\":1,\"27\":1,\"28\":1,\"29\":1,\"34\":1,\"35\":2,\"63\":1,\"73\":1,\"79\":1,\"80\":2,\"99\":1,\"112\":1,\"113\":1,\"116\":3,\"131\":4,\"133\":1,\"134\":4,\"139\":2,\"141\":1,\"142\":2,\"178\":1,\"181\":1,\"191\":1,\"193\":1,\"195\":1,\"196\":1,\"216\":1,\"218\":2,\"226\":1,\"227\":3,\"249\":1,\"253\":4,\"254\":5,\"257\":18,\"258\":2}}],[\"108k\",{\"1\":{\"162\":1}}],[\"1024x512\",{\"1\":{\"128\":1}}],[\"10​ifpistrueotherwise​\",{\"1\":{\"73\":1,\"131\":1}}],[\"101\",{\"1\":{\"61\":1,\"200\":2}}],[\"10362\",{\"1\":{\"37\":1}}],[\"100002i\",{\"1\":{\"224\":2}}],[\"100×max\",{\"1\":{\"162\":1}}],[\"100×scorehuman​−scorerandom​scoreagent​−scorerandom​​\",{\"1\":{\"162\":1}}],[\"100\",{\"1\":{\"5\":3,\"38\":1,\"116\":3,\"162\":1}}],[\"100分\",{\"1\":{\"2\":3}}],[\"10\",{\"1\":{\"2\":1,\"36\":1,\"37\":3,\"63\":1,\"116\":3,\"257\":1,\"260\":1}}],[\"h=64\",{\"1\":{\"220\":1}}],[\"h=8\",{\"1\":{\"220\":1}}],[\"h=f∘g\",{\"1\":{\"191\":1}}],[\"ht​=\",{\"1\":{\"249\":1}}],[\"ht​\",{\"1\":{\"210\":1,\"214\":3,\"249\":1,\"252\":2,\"254\":1}}],[\"ht−1​\",{\"1\":{\"32\":1,\"210\":1,\"214\":1}}],[\"hpatch​\",{\"1\":{\"175\":2}}],[\"hpixel​\",{\"1\":{\"174\":1}}],[\"hcls​\",{\"1\":{\"173\":1}}],[\"hc​=hd​\",{\"1\":{\"133\":1}}],[\"h−shc​\",{\"1\":{\"133\":1}}],[\"h−1\",{\"1\":{\"21\":3}}],[\"h×w\",{\"1\":{\"133\":1}}],[\"hello\",{\"1\":{\"219\":1}}],[\"headh​\",{\"1\":{\"220\":1}}],[\"head1​\",{\"1\":{\"220\":1}}],[\"headi​​=concat\",{\"1\":{\"220\":1}}],[\"headed\",{\"1\":{\"212\":1}}],[\"head\",{\"0\":{\"220\":1},\"1\":{\"132\":1,\"174\":1,\"175\":1,\"220\":12,\"221\":4}}],[\"hexo\",{\"1\":{\"1\":1}}],[\"hs​×ws​×c\",{\"1\":{\"131\":1}}],[\"hsuan\",{\"1\":{\"49\":2,\"56\":1}}],[\"hr1​\",{\"1\":{\"141\":1}}],[\"hr0\",{\"1\":{\"141\":2}}],[\"hrs​\",{\"1\":{\"134\":1}}],[\"hr​=xhr​\",{\"1\":{\"133\":1}}],[\"hr​\",{\"1\":{\"133\":3}}],[\"hr\",{\"1\":{\"128\":1,\"132\":3,\"133\":5,\"134\":9,\"140\":1,\"141\":2,\"142\":1,\"143\":1}}],[\"hrda\",{\"0\":{\"126\":1},\"1\":{\"128\":2,\"131\":1,\"132\":1,\"133\":1,\"135\":1,\"138\":3,\"139\":2,\"140\":1,\"141\":2,\"144\":2,\"170\":1,\"178\":1,\"179\":3,\"182\":1}}],[\"hneat\",{\"1\":{\"101\":2}}],[\"hns=humanscore​−randomscore​agentscore​−randomscore​​\",{\"1\":{\"34\":1}}],[\"hns\",{\"1\":{\"34\":3}}],[\"hf​\",{\"1\":{\"76\":1}}],[\"hϕ​\",{\"1\":{\"73\":1}}],[\"historical\",{\"1\":{\"249\":1,\"252\":1}}],[\"history\",{\"1\":{\"97\":1}}],[\"hinton\",{\"1\":{\"106\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1}}],[\"hidden\",{\"1\":{\"94\":1,\"109\":2,\"111\":3,\"112\":6,\"113\":1,\"119\":3,\"120\":1}}],[\"high\",{\"0\":{\"31\":1,\"126\":1},\"1\":{\"37\":2,\"64\":1,\"91\":1,\"128\":1,\"132\":2,\"133\":3,\"134\":1,\"141\":1,\"144\":2}}],[\"hitcon\",{\"1\":{\"3\":2}}],[\"hyperparameters\",{\"1\":{\"63\":1,\"116\":1}}],[\"hyperparameter\",{\"0\":{\"120\":1},\"1\":{\"29\":1,\"34\":1,\"56\":1,\"76\":1,\"79\":1,\"181\":1,\"254\":1}}],[\"how\",{\"0\":{\"119\":1}}],[\"house\",{\"1\":{\"116\":1}}],[\"hoyer\",{\"1\":{\"69\":2,\"74\":5,\"80\":2,\"81\":1,\"82\":1,\"83\":2,\"84\":1,\"127\":1,\"128\":1,\"131\":1,\"132\":2,\"133\":1,\"135\":2,\"138\":3,\"139\":2,\"140\":2,\"141\":1,\"142\":2}}],[\"horgan\",{\"1\":{\"32\":1}}],[\"horizon\",{\"1\":{\"27\":1}}],[\"hot\",{\"1\":{\"25\":1,\"73\":1,\"173\":3}}],[\"hμ\",{\"1\":{\"25\":1}}],[\"h\",{\"1\":{\"21\":2,\"25\":4,\"54\":2,\"73\":1,\"76\":1,\"131\":2,\"155\":2,\"191\":1,\"197\":2,\"220\":3}}],[\"hallucinatory\",{\"1\":{\"260\":1}}],[\"hackmd\",{\"1\":{\"204\":1}}],[\"hackday\",{\"1\":{\"6\":3}}],[\"hackdoor\",{\"1\":{\"3\":1}}],[\"hasselt\",{\"1\":{\"153\":1}}],[\"hado\",{\"1\":{\"153\":1}}],[\"hardware\",{\"1\":{\"227\":1}}],[\"hard\",{\"1\":{\"18\":1,\"23\":1,\"191\":2,\"195\":1}}],[\"human\",{\"0\":{\"14\":1},\"1\":{\"2\":1,\"34\":2,\"37\":2,\"40\":1,\"41\":1,\"101\":1,\"162\":2}}],[\"金盾獎\",{\"1\":{\"2\":1}}],[\"全國第六名\",{\"1\":{\"2\":1}}],[\"3~7\",{\"1\":{\"254\":1}}],[\"36\",{\"1\":{\"227\":1}}],[\"37000\",{\"1\":{\"227\":1}}],[\"3blue1brown\",{\"1\":{\"216\":5,\"231\":1}}],[\"3​​≤j<obd\",{\"1\":{\"134\":1}}],[\"3​​≤j<s⋅obd\",{\"1\":{\"134\":1}}],[\"3​​\",{\"1\":{\"134\":1}}],[\"3​+wd​​\",{\"1\":{\"133\":1}}],[\"3​+swc​​\",{\"1\":{\"133\":1}}],[\"3​∼u\",{\"1\":{\"133\":2}}],[\"3​\",{\"1\":{\"133\":2}}],[\"3\",{\"1\":{\"80\":1,\"116\":2,\"139\":1,\"141\":2,\"142\":2,\"179\":1,\"181\":3,\"227\":1,\"245\":1,\"254\":1,\"257\":5,\"258\":1,\"259\":1}}],[\"35\",{\"1\":{\"63\":1,\"116\":1}}],[\"31\",{\"1\":{\"37\":1,\"118\":1}}],[\"314\",{\"1\":{\"2\":1}}],[\"32g\",{\"1\":{\"259\":1}}],[\"32000\",{\"1\":{\"227\":1}}],[\"32×32\",{\"1\":{\"116\":2}}],[\"32\",{\"1\":{\"37\":1,\"259\":1}}],[\"38\",{\"1\":{\"37\":1}}],[\"349971\",{\"1\":{\"37\":1}}],[\"3000\",{\"1\":{\"259\":1}}],[\"300分\",{\"1\":{\"2\":1}}],[\"300\",{\"1\":{\"2\":1}}],[\"30\",{\"1\":{\"2\":1,\"37\":2,\"139\":1}}],[\"2d\",{\"1\":{\"257\":1}}],[\"2i+1\",{\"1\":{\"224\":1}}],[\"2i\",{\"1\":{\"224\":1}}],[\"2∣st+i​\",{\"1\":{\"155\":1,\"160\":1}}],[\"2​​∧obd\",{\"1\":{\"134\":1}}],[\"2​​∧s⋅obd\",{\"1\":{\"134\":1}}],[\"2​=bd\",{\"1\":{\"133\":1}}],[\"2​=bc\",{\"1\":{\"133\":1}}],[\"2​\",{\"1\":{\"133\":2,\"160\":1}}],[\"22\",{\"1\":{\"116\":1}}],[\"22480\",{\"1\":{\"37\":1}}],[\"28×28\",{\"1\":{\"116\":1}}],[\"2n\",{\"1\":{\"111\":1}}],[\"25\",{\"1\":{\"65\":1}}],[\"255\",{\"1\":{\"16\":1}}],[\"26\",{\"1\":{\"37\":2}}],[\"2600\",{\"1\":{\"16\":2}}],[\"2975\",{\"1\":{\"58\":1}}],[\"29\",{\"1\":{\"37\":1,\"118\":1}}],[\"2ϵ1+4ϵ\",{\"1\":{\"21\":1}}],[\"2\",{\"1\":{\"9\":1,\"21\":1,\"25\":1,\"32\":1,\"80\":3,\"93\":1,\"116\":1,\"139\":1,\"142\":2,\"152\":1,\"154\":1,\"155\":1,\"159\":4,\"178\":1,\"181\":1,\"182\":1,\"204\":1,\"254\":4,\"257\":7,\"258\":1,\"261\":1}}],[\"24gb\",{\"1\":{\"141\":1}}],[\"24966\",{\"1\":{\"59\":1}}],[\"24\",{\"1\":{\"5\":1,\"65\":1}}],[\"2048\",{\"1\":{\"259\":1}}],[\"2048x1024\",{\"1\":{\"128\":1}}],[\"2080\",{\"1\":{\"85\":1}}],[\"200\",{\"1\":{\"35\":1}}],[\"2017\",{\"1\":{\"151\":1,\"208\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1}}],[\"2014\",{\"1\":{\"106\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1,\"149\":1,\"212\":2,\"227\":2}}],[\"2013\",{\"1\":{\"90\":1}}],[\"2016\",{\"1\":{\"58\":1,\"60\":1,\"144\":1}}],[\"2015\",{\"1\":{\"47\":1,\"97\":1,\"100\":1,\"101\":1,\"152\":1,\"153\":1,\"154\":1}}],[\"2018\",{\"1\":{\"32\":1,\"47\":1,\"49\":2,\"50\":1,\"56\":1,\"148\":1,\"162\":3,\"163\":1}}],[\"2019年03月\",{\"1\":{\"3\":1}}],[\"2019年08月\",{\"1\":{\"3\":2}}],[\"2019年06月\",{\"1\":{\"2\":1}}],[\"2019年07月\",{\"1\":{\"2\":1,\"3\":2}}],[\"2019年10月\",{\"1\":{\"2\":1}}],[\"2019年11月\",{\"1\":{\"2\":1,\"3\":1}}],[\"2019年12月\",{\"1\":{\"2\":1}}],[\"2019\",{\"1\":{\"2\":1,\"3\":3}}],[\"20\",{\"1\":{\"16\":1,\"35\":1}}],[\"2022\",{\"1\":{\"69\":2,\"74\":5,\"80\":2,\"81\":1,\"82\":1,\"83\":2,\"84\":1,\"127\":1,\"128\":1,\"132\":1,\"133\":1,\"135\":1,\"138\":3,\"139\":2,\"140\":2,\"141\":1,\"142\":2,\"169\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3,\"247\":1}}],[\"2022年08月\",{\"1\":{\"3\":1}}],[\"2022年07月\",{\"1\":{\"2\":1,\"3\":2}}],[\"2022年\",{\"1\":{\"3\":1}}],[\"2022年10月\",{\"1\":{\"2\":1,\"3\":1}}],[\"2021\",{\"1\":{\"45\":1,\"50\":1,\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"2021年\",{\"1\":{\"3\":1}}],[\"2023年09月\",{\"1\":{\"3\":1}}],[\"2023年04月\",{\"1\":{\"3\":1}}],[\"2020年\",{\"1\":{\"3\":1}}],[\"2020年08月\",{\"1\":{\"3\":2}}],[\"2020年09月\",{\"1\":{\"3\":1}}],[\"2020年01月\",{\"1\":{\"2\":1,\"3\":1}}],[\"2020年02月\",{\"1\":{\"2\":1}}],[\"2020年03月\",{\"1\":{\"2\":2}}],[\"2020年04月\",{\"1\":{\"2\":1}}],[\"2020年06月\",{\"1\":{\"2\":2}}],[\"2020年07月\",{\"1\":{\"2\":1,\"3\":2}}],[\"2020\",{\"1\":{\"2\":1,\"3\":3,\"15\":1,\"19\":1,\"20\":1,\"22\":1,\"23\":2,\"25\":1,\"31\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2,\"45\":1,\"51\":2}}],[\"2024\",{\"0\":{\"4\":1},\"1\":{\"2\":1,\"4\":1,\"236\":1,\"244\":1,\"250\":1,\"260\":7}}],[\"2024年01月\",{\"1\":{\"2\":1}}],[\"212\",{\"1\":{\"2\":1}}],[\"21\",{\"1\":{\"2\":1,\"37\":1}}],[\"runnan\",{\"1\":{\"244\":1,\"250\":1,\"260\":7}}],[\"ruslan\",{\"1\":{\"106\":1}}],[\"rdm​×hdv​×rhdv​×dm​=rdm​×dm​\",{\"1\":{\"220\":1}}],[\"rnn\",{\"0\":{\"210\":1,\"214\":1},\"1\":{\"210\":3,\"211\":2,\"213\":1,\"214\":3,\"215\":1,\"216\":1,\"224\":2,\"226\":5,\"230\":1}}],[\"r−v\",{\"1\":{\"155\":3}}],[\"r=r\",{\"1\":{\"152\":1}}],[\"r+γq\",{\"1\":{\"153\":1,\"154\":1,\"159\":2}}],[\"r+γmaxb∈a​q\",{\"1\":{\"153\":1}}],[\"r+γb∈amax​q\",{\"1\":{\"152\":1,\"159\":2}}],[\"r+γa\",{\"1\":{\"93\":1}}],[\"rcv1\",{\"1\":{\"118\":1}}],[\"rcs\",{\"0\":{\"75\":1,\"82\":1},\"1\":{\"74\":1,\"80\":2,\"82\":3}}],[\"rj\",{\"1\":{\"112\":1}}],[\"rgb\",{\"1\":{\"102\":1}}],[\"room\",{\"1\":{\"257\":1}}],[\"rokuten\",{\"1\":{\"240\":1}}],[\"role\",{\"1\":{\"107\":1}}],[\"row\",{\"1\":{\"80\":4,\"139\":4,\"142\":4}}],[\"road\",{\"1\":{\"53\":1,\"61\":1,\"62\":1,\"76\":1}}],[\"round\",{\"1\":{\"2\":3}}],[\"raw\",{\"1\":{\"102\":1}}],[\"rate\",{\"0\":{\"77\":1,\"81\":1},\"1\":{\"74\":1,\"77\":1,\"79\":1,\"81\":1,\"116\":3,\"117\":1,\"118\":1,\"120\":1,\"131\":1,\"259\":1}}],[\"rates\",{\"1\":{\"74\":1,\"260\":1}}],[\"rare\",{\"0\":{\"75\":1,\"82\":1},\"1\":{\"74\":1,\"75\":3,\"80\":1,\"82\":1,\"131\":1}}],[\"randomized\",{\"1\":{\"165\":1}}],[\"randomness\",{\"1\":{\"149\":1}}],[\"random\",{\"0\":{\"158\":1},\"1\":{\"35\":1,\"75\":1,\"82\":3,\"107\":1,\"158\":4,\"159\":1,\"160\":1,\"162\":1}}],[\"ram\",{\"1\":{\"5\":2}}],[\"r\",{\"1\":{\"25\":2,\"59\":1,\"152\":1,\"154\":1,\"159\":4,\"174\":4,\"175\":2,\"249\":1}}],[\"rightarrow\",{\"0\":{\"201\":1,\"202\":1}}],[\"richter\",{\"1\":{\"59\":1}}],[\"rider\",{\"1\":{\"37\":1,\"53\":1,\"82\":1,\"99\":1}}],[\"ri\",{\"1\":{\"25\":3}}],[\"rtx\",{\"1\":{\"85\":1}}],[\"rt−1i​\",{\"1\":{\"22\":1,\"32\":1}}],[\"rt−1e​\",{\"1\":{\"22\":1,\"32\":1}}],[\"rt​=t\",{\"1\":{\"93\":1}}],[\"rt​∣st​=s\",{\"1\":{\"93\":1}}],[\"rt​\",{\"1\":{\"21\":1,\"93\":1,\"97\":1}}],[\"rt+1βi​​+γi​rt+2βi​​+γ2rt+3βi​​+\",{\"1\":{\"20\":1}}],[\"rti​\",{\"1\":{\"19\":1,\"23\":1,\"32\":1}}],[\"rti​=rtepisodic​⋅min\",{\"1\":{\"19\":1}}],[\"rte​\",{\"1\":{\"19\":1,\"23\":1,\"32\":1}}],[\"rtβi​​=rte​+βi​rti​\",{\"1\":{\"19\":1}}],[\"r2d2+sep\",{\"1\":{\"37\":1}}],[\"r2d2\",{\"1\":{\"16\":3,\"22\":1,\"30\":1,\"36\":1,\"37\":2,\"38\":3}}],[\"rl\",{\"0\":{\"21\":1},\"1\":{\"16\":4,\"18\":2,\"19\":1,\"21\":1,\"91\":8,\"93\":4,\"94\":1,\"96\":1,\"97\":2,\"100\":1,\"102\":1,\"149\":1,\"150\":1,\"155\":1,\"164\":1}}],[\"reflection\",{\"1\":{\"258\":2}}],[\"rejected\",{\"1\":{\"251\":3,\"252\":1,\"258\":5}}],[\"review\",{\"1\":{\"151\":1}}],[\"revenge\",{\"1\":{\"16\":2,\"37\":1}}],[\"reverse\",{\"1\":{\"1\":1}}],[\"reuters\",{\"1\":{\"118\":1}}],[\"recognition\",{\"0\":{\"117\":1}}],[\"recurrent\",{\"0\":{\"214\":1},\"1\":{\"41\":2,\"210\":4,\"213\":1}}],[\"regularization\",{\"1\":{\"113\":1,\"149\":1,\"150\":1,\"151\":1,\"155\":1,\"227\":1},\"2\":{\"125\":1}}],[\"regularize\",{\"1\":{\"76\":1}}],[\"reply\",{\"1\":{\"155\":1}}],[\"replay\",{\"1\":{\"22\":1,\"30\":3,\"32\":3,\"36\":1,\"41\":2,\"97\":6,\"102\":1,\"152\":3,\"155\":1}}],[\"representation\",{\"1\":{\"101\":1,\"188\":1,\"204\":2,\"212\":1,\"214\":1}}],[\"rel=oracleuda​\",{\"1\":{\"74\":1}}],[\"rel\",{\"1\":{\"74\":1}}],[\"release\",{\"1\":{\"45\":1}}],[\"related\",{\"0\":{\"17\":1,\"48\":1,\"71\":1,\"92\":1,\"108\":1,\"129\":1,\"150\":1,\"171\":1,\"188\":1,\"213\":1,\"246\":1}}],[\"real\",{\"1\":{\"47\":1,\"49\":1,\"57\":1,\"257\":1}}],[\"read\",{\"2\":{\"43\":1,\"67\":1,\"88\":1,\"104\":1,\"125\":1,\"146\":1,\"167\":1,\"184\":1,\"206\":1,\"233\":1,\"263\":1}}],[\"react\",{\"1\":{\"1\":1,\"258\":3}}],[\"resolution\",{\"0\":{\"126\":1,\"134\":1,\"139\":1},\"1\":{\"128\":3,\"132\":4,\"133\":7,\"134\":2,\"139\":1,\"140\":3,\"141\":1,\"143\":1,\"144\":2}}],[\"result\",{\"0\":{\"116\":1,\"117\":1,\"118\":1}}],[\"results\",{\"0\":{\"33\":1,\"55\":1,\"78\":1,\"98\":1,\"114\":1,\"136\":1,\"161\":1,\"177\":1,\"180\":1,\"199\":1,\"225\":1,\"255\":1}}],[\"residual\",{\"1\":{\"92\":1}}],[\"resnet\",{\"1\":{\"61\":1,\"70\":1,\"200\":2}}],[\"resnet101\",{\"1\":{\"56\":2,\"137\":1}}],[\"researchgate\",{\"1\":{\"83\":1,\"84\":1}}],[\"research\",{\"1\":{\"6\":1,\"186\":1,\"208\":1}}],[\"returns\",{\"1\":{\"34\":1}}],[\"return\",{\"1\":{\"27\":8,\"28\":1,\"35\":2,\"37\":1,\"93\":3,\"154\":1,\"155\":1}}],[\"retrace\",{\"1\":{\"21\":6,\"25\":2,\"37\":1}}],[\"re\",{\"1\":{\"25\":3,\"75\":1}}],[\"reward\",{\"0\":{\"19\":1},\"1\":{\"16\":7,\"18\":1,\"19\":10,\"21\":1,\"22\":2,\"25\":7,\"26\":2,\"27\":1,\"28\":2,\"32\":2,\"35\":6,\"91\":2,\"93\":2,\"97\":1,\"99\":4,\"100\":2,\"152\":1,\"249\":1,\"253\":1,\"257\":3}}],[\"reinforcement\",{\"0\":{\"89\":1},\"1\":{\"0\":1,\"1\":1,\"41\":3,\"92\":1,\"102\":1,\"165\":3},\"2\":{\"43\":1,\"104\":1,\"167\":1,\"263\":1}}],[\"gpt4\",{\"1\":{\"260\":1}}],[\"gpus\",{\"1\":{\"259\":1}}],[\"gpu\",{\"1\":{\"5\":1,\"85\":1,\"128\":2,\"132\":1,\"211\":1,\"227\":1}}],[\"global\",{\"1\":{\"245\":3,\"249\":1,\"251\":3,\"253\":6,\"254\":3,\"260\":1,\"261\":1}}],[\"gθˉ​\",{\"1\":{\"173\":1}}],[\"gθ​\",{\"1\":{\"73\":1,\"173\":4,\"174\":1}}],[\"guez\",{\"1\":{\"153\":1}}],[\"go\",{\"1\":{\"149\":1,\"257\":2}}],[\"gool\",{\"1\":{\"69\":2,\"74\":5,\"80\":2,\"81\":1,\"82\":1,\"83\":2,\"84\":1,\"127\":1,\"128\":1,\"132\":1,\"133\":1,\"135\":1,\"138\":3,\"139\":2,\"140\":2,\"141\":1,\"142\":2}}],[\"google\",{\"1\":{\"2\":1,\"15\":1,\"148\":1,\"208\":2}}],[\"gheshlaghi\",{\"1\":{\"148\":1,\"162\":3,\"163\":1}}],[\"gϕ​\",{\"1\":{\"131\":2}}],[\"gta\",{\"1\":{\"74\":2}}],[\"gta5\",{\"0\":{\"59\":1,\"61\":1,\"201\":1},\"1\":{\"57\":2,\"59\":1,\"63\":1,\"64\":1,\"79\":1,\"137\":1,\"138\":1,\"139\":1,\"178\":2,\"179\":3,\"181\":1,\"182\":1,\"200\":1,\"201\":1}}],[\"gemma\",{\"1\":{\"245\":1,\"258\":1}}],[\"german\",{\"1\":{\"212\":1,\"227\":1}}],[\"germanros\",{\"1\":{\"60\":1}}],[\"geist\",{\"1\":{\"149\":1}}],[\"get\",{\"1\":{\"133\":1}}],[\"geoffrey\",{\"1\":{\"106\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1}}],[\"generation\",{\"0\":{\"135\":1}}],[\"generator\",{\"1\":{\"49\":1}}],[\"general\",{\"1\":{\"23\":1,\"29\":1,\"34\":1,\"38\":1}}],[\"give\",{\"0\":{\"18\":1},\"1\":{\"18\":1,\"41\":1}}],[\"github\",{\"1\":{\"86\":1}}],[\"git\",{\"1\":{\"1\":1}}],[\"garbagecan\",{\"1\":{\"257\":1}}],[\"gap\",{\"1\":{\"173\":1,\"187\":2}}],[\"gaps\",{\"1\":{\"170\":2}}],[\"gaussian\",{\"1\":{\"73\":1,\"151\":1,\"157\":1,\"158\":2,\"159\":1,\"160\":3}}],[\"gan\",{\"1\":{\"49\":1}}],[\"gammon\",{\"0\":{\"94\":1},\"1\":{\"92\":1,\"97\":1}}],[\"gamma\",{\"1\":{\"37\":2}}],[\"gamer\",{\"1\":{\"16\":1}}],[\"game\",{\"1\":{\"16\":2}}],[\"games\",{\"1\":{\"16\":3,\"37\":1,\"38\":3,\"40\":1,\"99\":1,\"162\":1}}],[\"gated\",{\"1\":{\"210\":1}}],[\"gate\",{\"1\":{\"16\":1}}],[\"gates\",{\"1\":{\"16\":1}}],[\"gb\",{\"1\":{\"5\":4}}],[\"gcp\",{\"1\":{\"5\":1}}],[\"grounded\",{\"1\":{\"257\":1}}],[\"group\",{\"1\":{\"244\":1}}],[\"grader\",{\"1\":{\"257\":1}}],[\"gradient\",{\"1\":{\"95\":1}}],[\"graph\",{\"1\":{\"4\":1}}],[\"gridworld\",{\"1\":{\"35\":1}}],[\"greedy\",{\"1\":{\"4\":1,\"26\":1,\"29\":1,\"32\":1,\"97\":1,\"149\":1,\"150\":1,\"151\":2,\"152\":1,\"154\":1,\"159\":1}}],[\"g\",{\"1\":{\"2\":1,\"34\":1,\"191\":1}}],[\"第二與第三階段都是\",{\"1\":{\"198\":1}}],[\"第一階段包含了\",{\"1\":{\"198\":1}}],[\"第一個能夠在所有\",{\"1\":{\"40\":1}}],[\"第八屆成功大學大學生活體驗營\",{\"1\":{\"3\":1}}],[\"第七屆高一生程式設計排名賽\",{\"1\":{\"2\":1}}],[\"第7名\",{\"1\":{\"2\":1}}],[\"第62名\",{\"1\":{\"2\":1}}],[\"第5695名\",{\"1\":{\"2\":1}}],[\"第2427名\",{\"1\":{\"2\":1}}],[\"第29755名\",{\"1\":{\"2\":1}}],[\"第22名\",{\"1\":{\"2\":1}}],[\"第96名\",{\"1\":{\"2\":1}}],[\"epoch\",{\"1\":{\"259\":1}}],[\"episode\",{\"1\":{\"19\":2,\"26\":1,\"32\":2,\"34\":1,\"35\":3,\"91\":1,\"157\":2,\"158\":1,\"162\":1}}],[\"esim\",{\"1\":{\"240\":1}}],[\"e4​​\",{\"1\":{\"216\":1}}],[\"ej​\",{\"1\":{\"174\":6}}],[\"ei​\",{\"1\":{\"174\":7}}],[\"eπ\",{\"1\":{\"160\":1}}],[\"eπ​\",{\"1\":{\"27\":1}}],[\"e\",{\"1\":{\"159\":2,\"174\":2}}],[\"embodied\",{\"1\":{\"247\":3,\"257\":1}}],[\"embeddings\",{\"0\":{\"223\":1}}],[\"embedding\",{\"1\":{\"135\":1,\"221\":4,\"223\":4}}],[\"ema\",{\"1\":{\"73\":1,\"131\":1,\"194\":1}}],[\"eccv22\",{\"1\":{\"144\":1}}],[\"eccv\",{\"1\":{\"127\":1},\"2\":{\"146\":1}}],[\"effect\",{\"0\":{\"119\":1,\"121\":1}}],[\"efficient\",{\"1\":{\"41\":1}}],[\"error\",{\"1\":{\"116\":3,\"117\":1,\"118\":1,\"119\":1,\"245\":1,\"251\":1,\"260\":2,\"261\":1}}],[\"e2​\",{\"1\":{\"97\":1}}],[\"evolution\",{\"1\":{\"107\":1,\"165\":1}}],[\"everything\",{\"1\":{\"65\":1}}],[\"evaluation\",{\"0\":{\"63\":1}}],[\"evaluator\",{\"1\":{\"34\":1}}],[\"early\",{\"1\":{\"63\":1}}],[\"environments\",{\"1\":{\"257\":1}}],[\"encoding\",{\"0\":{\"224\":1},\"1\":{\"224\":1,\"227\":1}}],[\"encode\",{\"1\":{\"196\":1,\"227\":1}}],[\"encoder\",{\"0\":{\"221\":1},\"1\":{\"74\":6,\"79\":3,\"109\":2,\"133\":1,\"137\":1,\"193\":2,\"196\":2,\"210\":1,\"212\":1,\"215\":3,\"216\":1,\"217\":1,\"221\":7,\"222\":1}}],[\"enforcing\",{\"0\":{\"196\":1}}],[\"en​\",{\"1\":{\"97\":1}}],[\"enduro\",{\"1\":{\"99\":1}}],[\"end\",{\"1\":{\"96\":2}}],[\"entropy和kl\",{\"1\":{\"204\":1}}],[\"entropy\",{\"1\":{\"54\":1,\"73\":1,\"131\":1,\"134\":1,\"149\":1,\"150\":1,\"151\":1,\"155\":3,\"160\":1,\"191\":1,\"195\":2,\"204\":1}}],[\"english\",{\"0\":{\"229\":1},\"1\":{\"1\":1,\"212\":2,\"227\":2}}],[\"engineering\",{\"1\":{\"1\":1,\"258\":1}}],[\"exchange\",{\"2\":{\"242\":1}}],[\"extended\",{\"1\":{\"211\":1}}],[\"extension\",{\"1\":{\"86\":1}}],[\"extracting\",{\"1\":{\"247\":3}}],[\"extractor\",{\"1\":{\"191\":1,\"196\":1}}],[\"extrinsic\",{\"1\":{\"19\":1,\"22\":1,\"25\":3,\"32\":1,\"35\":4}}],[\"expert\",{\"1\":{\"251\":6,\"252\":1,\"253\":4,\"258\":1}}],[\"experiments\",{\"0\":{\"162\":1}}],[\"experienced\",{\"1\":{\"251\":2}}],[\"experience\",{\"1\":{\"22\":2,\"23\":1,\"26\":4,\"29\":1,\"32\":2,\"41\":2,\"97\":9,\"102\":1,\"152\":2,\"155\":1}}],[\"explicit\",{\"1\":{\"260\":1}}],[\"explained\",{\"1\":{\"216\":4,\"231\":1}}],[\"explore\",{\"1\":{\"26\":1}}],[\"exploration\",{\"0\":{\"26\":1,\"37\":1,\"147\":1,\"151\":1},\"1\":{\"16\":1,\"18\":1,\"19\":2,\"20\":1,\"23\":2,\"26\":1,\"27\":1,\"35\":5,\"41\":1,\"149\":2,\"150\":1,\"151\":5,\"154\":1,\"155\":1,\"157\":1,\"163\":1,\"164\":1,\"165\":4,\"251\":1}}],[\"exploit\",{\"1\":{\"26\":1}}],[\"exploitation\",{\"1\":{\"19\":1,\"20\":1,\"22\":1,\"26\":1,\"27\":1,\"35\":5}}],[\"exp\",{\"1\":{\"193\":1,\"196\":2}}],[\"exponential\",{\"1\":{\"73\":1,\"131\":1,\"174\":1,\"175\":1,\"194\":1}}],[\"exam\",{\"1\":{\"2\":1}}],[\"eye\",{\"1\":{\"16\":2,\"37\":1}}],[\"eto\",{\"1\":{\"258\":2}}],[\"et​=\",{\"1\":{\"97\":1}}],[\"eth\",{\"1\":{\"69\":1,\"127\":1}}],[\"et\",{\"1\":{\"15\":1,\"19\":1,\"20\":1,\"22\":1,\"23\":2,\"25\":1,\"31\":1,\"32\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2,\"47\":2,\"49\":2,\"50\":2,\"51\":2,\"56\":1,\"58\":1,\"59\":1,\"60\":1,\"90\":1,\"97\":1,\"100\":1,\"101\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1,\"148\":1,\"152\":1,\"154\":1,\"162\":3,\"163\":1,\"208\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1,\"244\":1,\"250\":1,\"260\":7}}],[\"agi\",{\"1\":{\"260\":1}}],[\"agents\",{\"1\":{\"165\":1,\"247\":3,\"257\":1}}],[\"agent\",{\"0\":{\"22\":1,\"243\":1,\"247\":1,\"254\":1},\"1\":{\"16\":1,\"18\":2,\"19\":3,\"28\":1,\"34\":1,\"35\":4,\"91\":1,\"97\":2,\"99\":1,\"100\":2,\"149\":1,\"151\":1,\"155\":1,\"245\":3,\"247\":2,\"251\":4,\"252\":3,\"253\":4,\"254\":5,\"257\":2,\"258\":3,\"260\":5,\"261\":3}}],[\"agent57\",{\"0\":{\"14\":1},\"1\":{\"23\":1,\"25\":2,\"26\":3,\"27\":1,\"29\":1,\"34\":1,\"35\":1,\"36\":1,\"38\":5,\"41\":2}}],[\"apre​=at​\",{\"1\":{\"252\":1,\"254\":1}}],[\"apre​\",{\"1\":{\"252\":4}}],[\"approximation\",{\"1\":{\"92\":1,\"157\":1}}],[\"approximators\",{\"1\":{\"95\":1}}],[\"approximator\",{\"1\":{\"20\":1,\"95\":2,\"101\":1}}],[\"apply\",{\"1\":{\"46\":1,\"51\":2,\"64\":1}}],[\"applications\",{\"1\":{\"45\":1}}],[\"appendix\",{\"1\":{\"34\":1,\"198\":1}}],[\"a1​\",{\"1\":{\"249\":1,\"254\":1}}],[\"a0​∣u\",{\"1\":{\"249\":1}}],[\"a0​∼πθ​\",{\"1\":{\"249\":1}}],[\"a0​\",{\"1\":{\"249\":2,\"253\":1,\"254\":1}}],[\"a∈a\",{\"1\":{\"249\":1}}],[\"anamw\",{\"1\":{\"259\":1}}],[\"analysis\",{\"0\":{\"163\":1}}],[\"an​\",{\"1\":{\"253\":1}}],[\"anext​=at+1​\",{\"1\":{\"252\":1}}],[\"anext​\",{\"1\":{\"252\":3,\"254\":1}}],[\"and\",{\"0\":{\"68\":1,\"133\":1,\"139\":1,\"168\":1,\"185\":1,\"221\":1,\"223\":1,\"259\":1},\"1\":{\"41\":1,\"74\":1,\"86\":1,\"186\":1,\"204\":1,\"208\":1,\"245\":1,\"251\":1,\"257\":2,\"260\":2,\"261\":1}}],[\"a3c\",{\"0\":{\"155\":1,\"160\":1},\"1\":{\"150\":1,\"151\":1,\"155\":7,\"160\":3,\"162\":2,\"165\":1}}],[\"azar\",{\"1\":{\"148\":1,\"162\":3,\"163\":1}}],[\"ablation\",{\"0\":{\"142\":1,\"181\":1},\"1\":{\"260\":1}}],[\"about\",{\"0\":{\"0\":1,\"63\":1}}],[\"acm\",{\"1\":{\"169\":1},\"2\":{\"184\":1}}],[\"ac\",{\"1\":{\"134\":3}}],[\"ac​=σ\",{\"1\":{\"134\":1}}],[\"ac​\",{\"1\":{\"134\":2}}],[\"activation\",{\"1\":{\"112\":1,\"119\":2}}],[\"actions\",{\"1\":{\"249\":1,\"254\":2}}],[\"actionable\",{\"1\":{\"247\":3}}],[\"action\",{\"0\":{\"25\":1,\"35\":1},\"1\":{\"16\":1,\"20\":1,\"21\":1,\"22\":1,\"25\":4,\"26\":1,\"27\":3,\"28\":1,\"29\":1,\"32\":2,\"91\":1,\"93\":2,\"97\":2,\"99\":2,\"151\":5,\"152\":4,\"154\":3,\"247\":2,\"249\":3,\"252\":2,\"253\":1,\"254\":15,\"258\":3,\"260\":4,\"261\":1}}],[\"actors\",{\"0\":{\"32\":1},\"1\":{\"32\":1}}],[\"actor\",{\"1\":{\"22\":1,\"26\":13,\"29\":1,\"37\":1,\"40\":1,\"155\":1,\"165\":1}}],[\"aware\",{\"0\":{\"126\":1},\"1\":{\"144\":2}}],[\"au​\",{\"1\":{\"254\":3}}],[\"au​⊆a\",{\"1\":{\"254\":1}}],[\"autoregressive\",{\"1\":{\"245\":1}}],[\"autoencoder\",{\"1\":{\"109\":1,\"119\":1,\"123\":1}}],[\"autoencoders\",{\"0\":{\"109\":1}}],[\"augumented\",{\"0\":{\"247\":1},\"1\":{\"73\":1,\"258\":2}}],[\"augumentation\",{\"1\":{\"51\":2,\"196\":2}}],[\"augmentation\",{\"1\":{\"65\":1}}],[\"a∼ρ\",{\"1\":{\"93\":1}}],[\"avgpool\",{\"1\":{\"76\":1}}],[\"average\",{\"1\":{\"37\":2,\"40\":1,\"73\":1,\"76\":1,\"131\":1,\"142\":1,\"194\":2}}],[\"arrive\",{\"1\":{\"257\":2}}],[\"around\",{\"1\":{\"257\":1}}],[\"are\",{\"1\":{\"257\":1}}],[\"argb∈amax​q\",{\"1\":{\"154\":1,\"159\":2}}],[\"arthur\",{\"1\":{\"153\":1,\"155\":1}}],[\"architectures\",{\"0\":{\"68\":1},\"1\":{\"74\":1,\"86\":2}}],[\"architecture\",{\"0\":{\"31\":1,\"74\":1}}],[\"armchair\",{\"1\":{\"257\":1}}],[\"arm\",{\"1\":{\"27\":1}}],[\"am​=a\",{\"1\":{\"27\":2,\"28\":2}}],[\"ak​=⎩⎨⎧​kargmax1≤a≤n​μ^​k−1​\",{\"1\":{\"29\":1}}],[\"ak​=\",{\"1\":{\"27\":1,\"28\":1}}],[\"ak​\",{\"1\":{\"27\":2}}],[\"add\",{\"1\":{\"227\":1}}],[\"adam\",{\"1\":{\"227\":1}}],[\"adaptsegnet\",{\"1\":{\"200\":1}}],[\"adaptative\",{\"0\":{\"168\":1}}],[\"adaptation問題\",{\"1\":{\"170\":1}}],[\"adaptations\",{\"1\":{\"119\":1}}],[\"adaptation\",{\"0\":{\"44\":1},\"1\":{\"65\":4,\"71\":1,\"119\":1,\"129\":1,\"171\":1,\"230\":1},\"2\":{\"67\":1,\"88\":1,\"146\":1,\"184\":1,\"206\":1}}],[\"adapt\",{\"1\":{\"65\":1,\"74\":1,\"131\":1}}],[\"adaption\",{\"0\":{\"46\":1,\"54\":1},\"1\":{\"46\":2,\"50\":1,\"187\":1,\"196\":1}}],[\"adapting\",{\"1\":{\"41\":1}}],[\"adaptive\",{\"0\":{\"26\":1,\"37\":1,\"68\":1,\"126\":1,\"185\":1},\"1\":{\"74\":1,\"86\":2,\"144\":2,\"204\":1}}],[\"advantage\",{\"1\":{\"154\":2,\"155\":2}}],[\"adversarial\",{\"1\":{\"49\":1,\"50\":1,\"65\":1}}],[\"adrià\",{\"1\":{\"15\":1,\"19\":1,\"20\":1,\"22\":1,\"23\":2,\"25\":1,\"31\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2}}],[\"ashish\",{\"1\":{\"208\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1}}],[\"asia\",{\"1\":{\"186\":1}}],[\"as\",{\"1\":{\"165\":1,\"247\":3}}],[\"asynchronous\",{\"1\":{\"165\":2}}],[\"asb​\",{\"1\":{\"25\":3}}],[\"as​∣xs​\",{\"1\":{\"21\":2}}],[\"assignment\",{\"1\":{\"16\":1,\"23\":1,\"30\":2,\"196\":2}}],[\"a∣xt+1​\",{\"1\":{\"21\":2}}],[\"at+1​=αu\",{\"1\":{\"254\":1}}],[\"at+1​∣ht​\",{\"1\":{\"249\":1}}],[\"at+1​∼πθ​\",{\"1\":{\"249\":1}}],[\"at+i​\",{\"1\":{\"155\":1,\"160\":2}}],[\"at+i​∣st+i​\",{\"1\":{\"155\":1,\"160\":2}}],[\"attention\",{\"0\":{\"207\":1,\"212\":1,\"216\":1,\"218\":1,\"220\":1,\"226\":1},\"1\":{\"74\":1,\"132\":1,\"134\":10,\"137\":1,\"142\":2,\"143\":1,\"212\":3,\"213\":1,\"215\":1,\"216\":11,\"218\":4,\"219\":3,\"220\":8,\"221\":7,\"222\":1,\"224\":3,\"226\":6,\"231\":2}}],[\"at\",{\"1\":{\"53\":2,\"54\":2,\"61\":2,\"62\":1,\"257\":2}}],[\"at−1​\",{\"1\":{\"22\":1,\"32\":1}}],[\"at​=a\",{\"1\":{\"93\":1}}],[\"at​\",{\"1\":{\"20\":2,\"21\":8,\"97\":1,\"249\":1,\"254\":1}}],[\"atari\",{\"0\":{\"14\":1,\"89\":1},\"1\":{\"16\":5,\"38\":3,\"40\":1,\"41\":1,\"99\":1,\"162\":1}}],[\"alfworld\",{\"1\":{\"245\":1,\"257\":4,\"260\":2}}],[\"aligning\",{\"1\":{\"257\":1}}],[\"alignment\",{\"0\":{\"49\":1},\"1\":{\"49\":1,\"201\":1}}],[\"alibaba\",{\"1\":{\"244\":1}}],[\"all\",{\"0\":{\"207\":1}}],[\"alternative\",{\"1\":{\"165\":1}}],[\"alogorithm\",{\"1\":{\"155\":1}}],[\"alpha\",{\"1\":{\"149\":1}}],[\"alan\",{\"1\":{\"144\":1}}],[\"alex\",{\"1\":{\"106\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1}}],[\"algorithms\",{\"1\":{\"92\":1}}],[\"algorithm\",{\"0\":{\"27\":1},\"1\":{\"198\":1}}],[\"al\",{\"1\":{\"15\":1,\"19\":1,\"20\":1,\"22\":1,\"23\":2,\"25\":1,\"31\":1,\"32\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2,\"47\":2,\"49\":2,\"50\":2,\"51\":2,\"53\":2,\"54\":2,\"56\":1,\"58\":1,\"59\":1,\"60\":1,\"61\":2,\"62\":1,\"90\":1,\"97\":1,\"100\":1,\"101\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1,\"148\":1,\"152\":1,\"154\":1,\"162\":3,\"163\":1,\"208\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1,\"244\":1,\"250\":1,\"260\":7}}],[\"ai​∣si​\",{\"1\":{\"155\":1}}],[\"ai\",{\"1\":{\"5\":1}}],[\"ais3\",{\"1\":{\"2\":1,\"3\":2}}],[\"a\",{\"0\":{\"26\":1,\"105\":1},\"1\":{\"2\":1,\"21\":4,\"25\":5,\"27\":10,\"28\":8,\"29\":2,\"51\":2,\"64\":1,\"93\":4,\"152\":5,\"154\":18,\"155\":3,\"159\":8,\"160\":2,\"165\":1,\"187\":1,\"192\":2,\"249\":2,\"257\":31}}],[\"網頁組第3名\",{\"1\":{\"2\":1}}],[\"網路管理等領域\",{\"1\":{\"0\":1}}],[\"青年黑克松\",{\"1\":{\"2\":1}}],[\"專題第三名\",{\"1\":{\"2\":1}}],[\"fff​=2048\",{\"1\":{\"222\":1}}],[\"ffn\",{\"1\":{\"222\":4}}],[\"fluffy\",{\"1\":{\"216\":5}}],[\"flamingo\",{\"1\":{\"6\":1}}],[\"f~​\",{\"1\":{\"193\":1,\"196\":1}}],[\"fj​\",{\"1\":{\"175\":1}}],[\"fk​\",{\"1\":{\"175\":1}}],[\"ft​\",{\"1\":{\"134\":3}}],[\"f​=ζ\",{\"1\":{\"134\":1}}],[\"fs​\",{\"1\":{\"134\":1}}],[\"fs\",{\"1\":{\"133\":1}}],[\"full\",{\"1\":{\"253\":1}}],[\"fully\",{\"1\":{\"116\":1}}],[\"fusion\",{\"0\":{\"134\":1},\"1\":{\"143\":1}}],[\"fused\",{\"1\":{\"134\":2}}],[\"fuse\",{\"1\":{\"132\":1,\"134\":1}}],[\"functions\",{\"1\":{\"165\":1}}],[\"functiona\",{\"1\":{\"154\":1}}],[\"function\",{\"0\":{\"25\":1,\"35\":1},\"1\":{\"20\":2,\"21\":1,\"25\":1,\"76\":2,\"92\":1,\"93\":1,\"94\":1,\"95\":1,\"97\":1,\"112\":1,\"131\":2,\"152\":3,\"154\":2,\"155\":4,\"160\":1,\"193\":1,\"216\":1,\"249\":1}}],[\"f\",{\"1\":{\"112\":1,\"158\":2,\"191\":1,\"193\":1,\"196\":1}}],[\"fc​=ns​⋅h⋅w∑i=1ns​​∑j=1h×w​\",{\"1\":{\"75\":1}}],[\"fc​\",{\"1\":{\"75\":1}}],[\"fd\",{\"0\":{\"76\":1,\"83\":1},\"1\":{\"74\":1,\"80\":2,\"83\":1}}],[\"fθ​\",{\"1\":{\"54\":3,\"131\":3}}],[\"feed\",{\"0\":{\"222\":1},\"1\":{\"221\":1}}],[\"feedbacks\",{\"2\":{\"12\":1}}],[\"feedback\",{\"1\":{\"11\":1,\"258\":2,\"261\":1}}],[\"fe\",{\"1\":{\"133\":3,\"134\":1}}],[\"fence\",{\"1\":{\"60\":1}}],[\"features\",{\"1\":{\"76\":3,\"101\":1,\"194\":1,\"196\":2}}],[\"feature\",{\"0\":{\"76\":1,\"83\":1},\"1\":{\"49\":1,\"74\":3,\"76\":3,\"131\":1,\"133\":1,\"170\":1,\"174\":6,\"191\":1,\"192\":1,\"193\":2,\"194\":2,\"196\":4}}],[\"fang\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1,\"244\":1,\"250\":1,\"260\":7}}],[\"fa\",{\"1\":{\"134\":2}}],[\"factorised\",{\"1\":{\"158\":1,\"159\":1,\"160\":1}}],[\"factor\",{\"1\":{\"37\":1,\"93\":1}}],[\"family\",{\"0\":{\"26\":1}}],[\"follow\",{\"1\":{\"21\":1,\"25\":1,\"93\":1}}],[\"format\",{\"1\":{\"258\":1}}],[\"former\",{\"1\":{\"208\":1}}],[\"forward\",{\"0\":{\"222\":1},\"1\":{\"221\":1}}],[\"fortunato\",{\"1\":{\"148\":1,\"162\":3,\"163\":1}}],[\"for\",{\"0\":{\"68\":1,\"73\":1,\"77\":1,\"147\":1,\"151\":1,\"168\":1,\"185\":1},\"1\":{\"2\":1,\"41\":1,\"50\":1,\"65\":4,\"69\":1,\"74\":3,\"86\":2,\"127\":1,\"128\":1,\"131\":1,\"150\":1,\"157\":1,\"165\":2,\"170\":1,\"203\":1,\"204\":3,\"247\":3,\"257\":1}}],[\"frederick\",{\"1\":{\"215\":1,\"231\":1}}],[\"french\",{\"1\":{\"212\":1,\"227\":1}}],[\"free\",{\"1\":{\"94\":1,\"95\":1}}],[\"freeway\",{\"1\":{\"37\":1}}],[\"frame\",{\"1\":{\"99\":1}}],[\"framework\",{\"1\":{\"64\":1,\"79\":1,\"143\":1,\"178\":1}}],[\"frameworks\",{\"1\":{\"1\":1}}],[\"frames\",{\"1\":{\"32\":1,\"97\":1,\"99\":2,\"162\":2}}],[\"from\",{\"0\":{\"105\":1},\"1\":{\"16\":2,\"19\":1,\"20\":1,\"22\":1,\"23\":2,\"25\":1,\"31\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2,\"46\":2,\"47\":2,\"49\":2,\"50\":2,\"51\":2,\"53\":2,\"54\":2,\"58\":1,\"59\":1,\"60\":1,\"61\":2,\"62\":1,\"69\":1,\"74\":5,\"80\":2,\"81\":1,\"82\":1,\"83\":3,\"84\":2,\"97\":1,\"100\":1,\"101\":1,\"109\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1,\"128\":1,\"131\":1,\"132\":2,\"133\":1,\"134\":1,\"135\":2,\"138\":3,\"139\":2,\"140\":2,\"141\":1,\"142\":2,\"151\":2,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"162\":3,\"163\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3,\"187\":1,\"188\":1,\"192\":1,\"201\":1,\"208\":1,\"214\":2,\"215\":1,\"216\":4,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1,\"247\":2,\"250\":1,\"257\":5,\"260\":7}}],[\"frontend\",{\"1\":{\"10\":1}}],[\"fi​\",{\"1\":{\"175\":3}}],[\"fitted\",{\"1\":{\"92\":1}}],[\"fintune\",{\"1\":{\"7\":1}}],[\"fine\",{\"1\":{\"6\":3,\"50\":1,\"113\":1,\"258\":2}}],[\"finetune\",{\"1\":{\"5\":1}}],[\"final\",{\"1\":{\"2\":1}}],[\"firstsecurity\",{\"1\":{\"3\":1}}],[\"fml\",{\"1\":{\"2\":1}}],[\"梅竹黑客松\",{\"1\":{\"2\":1}}],[\"無關的\",{\"1\":{\"253\":2}}],[\"無論在哪個\",{\"1\":{\"260\":1}}],[\"無論現在的\",{\"1\":{\"224\":1}}],[\"無論是不是有看過的\",{\"1\":{\"260\":1}}],[\"無論是\",{\"1\":{\"223\":1}}],[\"無論是出現在\",{\"1\":{\"175\":1}}],[\"無論是對\",{\"1\":{\"139\":1}}],[\"無論是單純加上\",{\"1\":{\"117\":1}}],[\"無論是否有使用\",{\"1\":{\"25\":1}}],[\"無法及時針對環境與\",{\"1\":{\"261\":1}}],[\"無法收斂的問題可以透過\",{\"1\":{\"95\":1}}],[\"無法收斂\",{\"1\":{\"95\":1}}],[\"無法好好處理\",{\"1\":{\"23\":1,\"30\":1}}],[\"無\",{\"1\":{\"2\":1}}],[\"t≥0\",{\"1\":{\"254\":1}}],[\"tutor\",{\"1\":{\"236\":6,\"237\":2}}],[\"tuning\",{\"1\":{\"6\":1,\"113\":1}}],[\"tune\",{\"1\":{\"6\":3,\"50\":1,\"198\":2,\"258\":2}}],[\"t−1\",{\"1\":{\"219\":1}}],[\"trial\",{\"1\":{\"245\":1,\"251\":1,\"260\":1,\"261\":1}}],[\"truncate\",{\"1\":{\"162\":1}}],[\"trail\",{\"1\":{\"260\":1}}],[\"training\",{\"0\":{\"50\":1,\"68\":1,\"73\":1,\"253\":1,\"259\":1},\"1\":{\"51\":2,\"58\":1,\"59\":1,\"60\":1,\"65\":1,\"71\":1,\"73\":2,\"74\":2,\"86\":1,\"129\":1,\"131\":1,\"187\":2,\"188\":1,\"192\":2,\"201\":1,\"229\":1,\"253\":3,\"259\":1,\"260\":1,\"261\":1}}],[\"train\",{\"1\":{\"6\":1,\"61\":2,\"76\":1,\"83\":1}}],[\"transition\",{\"1\":{\"152\":1,\"249\":1}}],[\"transformer论文逐段精读\",{\"1\":{\"231\":1}}],[\"transformers\",{\"1\":{\"216\":4,\"231\":1}}],[\"transformer\",{\"1\":{\"71\":1,\"74\":7,\"77\":1,\"83\":1,\"85\":1,\"131\":1,\"135\":1,\"212\":1,\"217\":2,\"220\":1,\"221\":4,\"224\":1,\"226\":1,\"227\":1,\"229\":1,\"230\":4}}],[\"transformed\",{\"1\":{\"21\":2,\"25\":3}}],[\"tranheden\",{\"1\":{\"53\":2,\"54\":2,\"61\":2,\"62\":1}}],[\"trajectory\",{\"1\":{\"32\":1,\"249\":4,\"251\":12,\"252\":3,\"253\":4,\"258\":1,\"260\":1}}],[\"trajectories\",{\"1\":{\"21\":1,\"25\":1,\"32\":1,\"253\":2,\"258\":6}}],[\"trace\",{\"1\":{\"30\":4,\"36\":4,\"38\":1}}],[\"take\",{\"1\":{\"257\":1}}],[\"tasks\",{\"1\":{\"257\":2}}],[\"task\",{\"0\":{\"251\":1},\"1\":{\"245\":5,\"247\":1,\"249\":7,\"251\":6,\"253\":10,\"254\":7,\"257\":2,\"260\":9,\"261\":2}}],[\"tat\",{\"1\":{\"169\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3}}],[\"table\",{\"1\":{\"152\":1}}],[\"target\",{\"0\":{\"185\":1,\"191\":1},\"1\":{\"21\":3,\"25\":1,\"46\":3,\"47\":1,\"49\":2,\"50\":7,\"53\":2,\"54\":4,\"73\":3,\"93\":1,\"97\":1,\"102\":1,\"131\":5,\"134\":1,\"152\":1,\"170\":3,\"173\":6,\"187\":3,\"190\":3,\"191\":1,\"193\":1,\"194\":1,\"196\":4,\"198\":1,\"204\":1}}],[\"t\",{\"1\":{\"93\":1,\"97\":1,\"155\":1,\"173\":1,\"196\":8,\"219\":2,\"249\":2}}],[\"td\",{\"0\":{\"94\":1},\"1\":{\"92\":1,\"95\":1,\"97\":1}}],[\"twarm​\",{\"1\":{\"77\":1}}],[\"t​\",{\"1\":{\"75\":1}}],[\"tsukuba\",{\"2\":{\"242\":1}}],[\"tsu\",{\"1\":{\"231\":1}}],[\"tsai\",{\"1\":{\"49\":2,\"56\":1}}],[\"ts\",{\"1\":{\"41\":1}}],[\"tsmc\",{\"0\":{\"4\":1},\"1\":{\"2\":1,\"5\":1,\"10\":1,\"11\":2},\"2\":{\"13\":1}}],[\"ting\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"timit\",{\"1\":{\"117\":1}}],[\"time\",{\"0\":{\"30\":1,\"36\":1}}],[\"ti\",{\"1\":{\"85\":1}}],[\"tips\",{\"1\":{\"21\":1,\"25\":1,\"26\":1,\"29\":1,\"30\":1,\"35\":1,\"49\":1,\"53\":1,\"151\":1,\"152\":1,\"153\":1,\"154\":1,\"155\":1,\"157\":1,\"179\":1,\"192\":1,\"195\":1,\"196\":1,\"216\":1,\"221\":2,\"252\":1,\"253\":3}}],[\"thought\",{\"1\":{\"258\":2}}],[\"than\",{\"1\":{\"257\":1}}],[\"threshold\",{\"1\":{\"173\":1}}],[\"thread\",{\"1\":{\"159\":1,\"160\":1}}],[\"through\",{\"0\":{\"30\":1,\"36\":1}}],[\"thing\",{\"0\":{\"76\":1,\"83\":1},\"1\":{\"74\":1,\"76\":2,\"131\":1}}],[\"thq\",{\"1\":{\"21\":1}}],[\"theory\",{\"1\":{\"107\":1}}],[\"thelimeydragon\",{\"1\":{\"16\":1}}],[\"the\",{\"0\":{\"14\":1},\"1\":{\"16\":1,\"41\":1,\"107\":1,\"257\":7}}],[\"t^q\",{\"1\":{\"21\":1}}],[\"t∈n​\",{\"1\":{\"21\":1}}],[\"template\",{\"1\":{\"247\":1}}],[\"temporal\",{\"1\":{\"165\":1}}],[\"temperature\",{\"1\":{\"75\":1,\"193\":1}}],[\"tensorflow\",{\"1\":{\"165\":1}}],[\"text\",{\"0\":{\"118\":1},\"1\":{\"215\":1,\"231\":1,\"257\":1}}],[\"te\",{\"1\":{\"75\":1}}],[\"test\",{\"1\":{\"116\":1,\"119\":1}}],[\"testing\",{\"1\":{\"74\":1}}],[\"testset\",{\"1\":{\"63\":1}}],[\"teacher\",{\"1\":{\"73\":2,\"128\":1,\"131\":2,\"173\":2,\"197\":1,\"198\":2}}],[\"team\",{\"1\":{\"11\":1}}],[\"technology\",{\"1\":{\"45\":1,\"186\":1}}],[\"term\",{\"1\":{\"16\":1,\"23\":1,\"30\":2,\"210\":1}}],[\"towards\",{\"1\":{\"257\":1}}],[\"token\",{\"1\":{\"227\":4,\"245\":1,\"253\":9}}],[\"top\",{\"1\":{\"116\":3}}],[\"toronto\",{\"1\":{\"106\":1}}],[\"to\",{\"0\":{\"53\":1,\"105\":1,\"197\":1},\"1\":{\"53\":1,\"57\":1,\"63\":2,\"64\":2,\"65\":2,\"96\":1,\"165\":1,\"212\":2,\"215\":1,\"257\":3}}],[\"toi海選\",{\"1\":{\"2\":1}}],[\"toi校內賽\",{\"1\":{\"2\":1}}],[\"toi初選\",{\"1\":{\"2\":1}}],[\"toefl\",{\"1\":{\"1\":1}}],[\"競賽名稱\",{\"1\":{\"2\":1}}],[\"競賽成績\",{\"0\":{\"2\":1}}],[\"n2\",{\"1\":{\"235\":1}}],[\"n2⋅d\",{\"1\":{\"226\":1}}],[\"num⋅warmup\",{\"1\":{\"227\":1}}],[\"num−0\",{\"1\":{\"227\":1}}],[\"number\",{\"0\":{\"158\":1},\"1\":{\"116\":1,\"158\":2}}],[\"nvidia\",{\"1\":{\"227\":1,\"259\":1}}],[\"n⋅d2\",{\"1\":{\"226\":1}}],[\"n=6\",{\"1\":{\"221\":2}}],[\"nlp\",{\"1\":{\"210\":1,\"214\":2,\"231\":1},\"2\":{\"233\":1}}],[\"ni​\",{\"1\":{\"254\":1}}],[\"ningyu\",{\"1\":{\"244\":1,\"250\":1,\"260\":7}}],[\"niki\",{\"1\":{\"208\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1}}],[\"nips\",{\"1\":{\"208\":1}}],[\"nitish\",{\"1\":{\"106\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1}}],[\"nfq\",{\"0\":{\"96\":1},\"1\":{\"92\":1,\"96\":2}}],[\"nt​\",{\"1\":{\"73\":1,\"131\":1,\"190\":1}}],[\"nthu\",{\"1\":{\"3\":1}}],[\"ns​\",{\"1\":{\"73\":1,\"131\":1,\"190\":1}}],[\"nat\",{\"1\":{\"258\":2}}],[\"national\",{\"1\":{\"244\":1}}],[\"native\",{\"1\":{\"1\":1}}],[\"natural\",{\"1\":{\"210\":1}}],[\"naive\",{\"0\":{\"53\":1},\"1\":{\"53\":2,\"73\":1}}],[\"nk​\",{\"1\":{\"27\":2,\"28\":2}}],[\"n−1\",{\"1\":{\"27\":1,\"29\":1}}],[\"n\",{\"1\":{\"27\":1,\"97\":1,\"111\":1,\"120\":1,\"218\":1,\"219\":1,\"226\":3,\"254\":2,\"259\":1}}],[\"nn\",{\"1\":{\"21\":3,\"23\":1,\"25\":3,\"35\":1}}],[\"ngu+sep\",{\"1\":{\"37\":1}}],[\"ngu\",{\"0\":{\"22\":1,\"23\":1},\"1\":{\"18\":3,\"20\":2,\"21\":1,\"22\":2,\"23\":2,\"25\":2,\"26\":1,\"29\":1,\"30\":1,\"32\":1,\"35\":5,\"37\":1,\"38\":2}}],[\"newswire\",{\"1\":{\"118\":1}}],[\"net\",{\"1\":{\"116\":2,\"150\":1,\"157\":1,\"159\":2}}],[\"nets\",{\"0\":{\"113\":1}}],[\"networks\",{\"0\":{\"93\":1,\"105\":1,\"147\":1,\"222\":1},\"1\":{\"37\":1,\"38\":1,\"40\":1,\"41\":1,\"90\":1,\"92\":1}}],[\"network\",{\"0\":{\"68\":1,\"74\":1,\"119\":1,\"214\":1},\"1\":{\"21\":4,\"25\":3,\"35\":6,\"37\":2,\"49\":2,\"54\":1,\"56\":1,\"74\":2,\"76\":1,\"79\":1,\"86\":2,\"93\":4,\"95\":2,\"97\":4,\"100\":1,\"102\":1,\"107\":2,\"109\":1,\"112\":2,\"119\":2,\"128\":1,\"131\":4,\"133\":2,\"135\":1,\"137\":1,\"149\":1,\"152\":4,\"154\":1,\"157\":2,\"159\":2,\"160\":2,\"173\":4,\"178\":1,\"187\":1,\"191\":1,\"192\":3,\"210\":2,\"211\":1,\"213\":1}}],[\"next\",{\"1\":{\"97\":1,\"254\":12}}],[\"neurips\",{\"1\":{\"90\":1},\"2\":{\"104\":1,\"233\":1}}],[\"neuralps\",{\"1\":{\"208\":1}}],[\"neural\",{\"0\":{\"105\":1,\"214\":1},\"1\":{\"41\":1,\"92\":1,\"93\":1,\"97\":1,\"107\":1,\"109\":1,\"112\":2,\"149\":1,\"152\":3,\"154\":1,\"210\":2,\"211\":2,\"213\":1}}],[\"needs\",{\"1\":{\"65\":1}}],[\"need\",{\"0\":{\"207\":1},\"1\":{\"65\":1}}],[\"never\",{\"0\":{\"18\":1},\"1\":{\"18\":1,\"41\":1}}],[\"negative\",{\"1\":{\"16\":3,\"99\":1}}],[\"noam\",{\"1\":{\"208\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1}}],[\"noisynet\",{\"1\":{\"160\":1,\"162\":6,\"163\":1}}],[\"noisy\",{\"0\":{\"147\":1},\"1\":{\"150\":1,\"157\":1,\"159\":3,\"160\":1,\"188\":1}}],[\"noise\",{\"0\":{\"151\":1},\"1\":{\"109\":3,\"150\":1,\"151\":12,\"157\":8,\"158\":3,\"159\":1,\"160\":4,\"163\":2,\"165\":2,\"173\":1,\"195\":1,\"196\":1}}],[\"normalize\",{\"1\":{\"227\":1}}],[\"normalized\",{\"1\":{\"34\":2}}],[\"normalization\",{\"1\":{\"221\":2}}],[\"normal\",{\"1\":{\"133\":1}}],[\"non\",{\"1\":{\"95\":1}}],[\"none\",{\"1\":{\"7\":1}}],[\"notations\",{\"1\":{\"173\":1}}],[\"notation\",{\"1\":{\"73\":1,\"112\":1,\"131\":1,\"190\":1}}],[\"notes\",{\"1\":{\"65\":1}}],[\"note\",{\"1\":{\"25\":1,\"155\":2,\"162\":1},\"2\":{\"42\":1,\"66\":1,\"87\":1,\"103\":1,\"124\":1,\"145\":1,\"166\":1,\"183\":1,\"205\":1,\"232\":1,\"262\":1}}],[\"novelty\",{\"1\":{\"19\":1}}],[\"noveltyαt​\",{\"1\":{\"19\":1}}],[\"noveltyrtepisodic​\",{\"1\":{\"19\":1}}],[\"no\",{\"1\":{\"16\":1}}],[\"npsc決賽\",{\"1\":{\"2\":1}}],[\"n1\",{\"1\":{\"1\":1,\"235\":1}}],[\"j=1nt​​\",{\"1\":{\"190\":2}}],[\"j=1ns​​\",{\"1\":{\"190\":2}}],[\"j​=p​0\",{\"1\":{\"160\":1}}],[\"j​=0\",{\"1\":{\"160\":1}}],[\"j​∼u\",{\"1\":{\"160\":2}}],[\"jw​ϵjb​​=f\",{\"1\":{\"158\":1}}],[\"juliani\",{\"1\":{\"155\":1}}],[\"jiang\",{\"1\":{\"144\":1}}],[\"jitter\",{\"1\":{\"73\":1}}],[\"j−obi\",{\"1\":{\"134\":1}}],[\"jmlr\",{\"1\":{\"106\":1},\"2\":{\"125\":1}}],[\"john\",{\"1\":{\"32\":1}}],[\"j\",{\"1\":{\"25\":5,\"26\":1,\"32\":1,\"73\":8,\"75\":1,\"76\":13,\"112\":1,\"134\":3,\"174\":1,\"175\":1,\"216\":1}}],[\"jlpt\",{\"1\":{\"1\":1}}],[\"jaxa\",{\"1\":{\"239\":1}}],[\"jam\",{\"1\":{\"2\":1}}],[\"japanese\",{\"1\":{\"1\":1}}],[\"javascripts\",{\"1\":{\"1\":1}}],[\"b=\",{\"1\":{\"252\":1}}],[\"byte\",{\"1\":{\"227\":1}}],[\"bytenet\",{\"1\":{\"211\":1}}],[\"by\",{\"0\":{\"196\":1}}],[\"b∈rq\",{\"1\":{\"158\":1}}],[\"bd\",{\"1\":{\"133\":6}}],[\"bc​\",{\"1\":{\"133\":1}}],[\"bc\",{\"1\":{\"133\":6}}],[\"browse\",{\"1\":{\"247\":1}}],[\"brain\",{\"1\":{\"208\":1}}],[\"bracket\",{\"1\":{\"73\":1,\"131\":1}}],[\"breakout\",{\"1\":{\"99\":1}}],[\"b5\",{\"1\":{\"74\":2,\"79\":1,\"83\":1,\"137\":1}}],[\"bilinearly\",{\"1\":{\"131\":1}}],[\"bilal\",{\"1\":{\"15\":1,\"25\":1,\"31\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2,\"148\":1,\"162\":3,\"163\":1}}],[\"bicycle\",{\"1\":{\"82\":2}}],[\"biases\",{\"1\":{\"112\":1}}],[\"bias\",{\"1\":{\"75\":1,\"158\":1}}],[\"binary\",{\"1\":{\"51\":1,\"56\":1}}],[\"better\",{\"1\":{\"151\":2,\"165\":1}}],[\"bert\",{\"1\":{\"99\":1}}],[\"beat\",{\"1\":{\"64\":1}}],[\"beam\",{\"1\":{\"37\":1,\"99\":1}}],[\"best\",{\"1\":{\"63\":3,\"101\":1}}],[\"behaviour\",{\"1\":{\"41\":1}}],[\"benifit\",{\"1\":{\"37\":1}}],[\"benchmarks\",{\"1\":{\"57\":1,\"200\":1}}],[\"benchmark\",{\"0\":{\"14\":1},\"1\":{\"16\":1,\"41\":1}}],[\"b\",{\"1\":{\"25\":3,\"51\":2,\"112\":1,\"152\":1,\"153\":2,\"154\":1,\"159\":4,\"252\":1,\"254\":3}}],[\"bus\",{\"1\":{\"76\":1}}],[\"build\",{\"1\":{\"61\":1}}],[\"budden\",{\"1\":{\"32\":1}}],[\"buffer\",{\"1\":{\"22\":1,\"30\":1,\"32\":3,\"152\":2,\"155\":1}}],[\"bucket\",{\"1\":{\"5\":1}}],[\"bo\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"box\",{\"1\":{\"133\":1}}],[\"bounding\",{\"1\":{\"133\":1}}],[\"bound\",{\"0\":{\"27\":1}}],[\"bonus\",{\"1\":{\"9\":1}}],[\"bottleneck\",{\"1\":{\"76\":1}}],[\"bot\",{\"1\":{\"1\":1}}],[\"balanced\",{\"1\":{\"65\":1}}],[\"backbone\",{\"1\":{\"56\":1,\"61\":1,\"70\":3,\"74\":5,\"76\":1,\"85\":1,\"137\":1,\"173\":1,\"200\":2,\"253\":1,\"260\":4}}],[\"backpropagation\",{\"1\":{\"113\":1,\"135\":1}}],[\"backprop\",{\"0\":{\"30\":1,\"36\":1}}],[\"bandit\",{\"0\":{\"26\":1},\"1\":{\"27\":1,\"29\":1,\"37\":3,\"38\":1}}],[\"batch\",{\"1\":{\"25\":4,\"194\":2,\"259\":1}}],[\"badia\",{\"1\":{\"15\":1,\"19\":1,\"20\":1,\"22\":1,\"23\":2,\"25\":1,\"31\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2}}],[\"base\",{\"1\":{\"227\":2,\"230\":1,\"252\":2,\"254\":2,\"260\":2}}],[\"baseline\",{\"0\":{\"258\":1},\"1\":{\"63\":1,\"101\":1,\"116\":3,\"118\":1,\"141\":1,\"154\":1,\"162\":3,\"181\":1,\"258\":1}}],[\"based\",{\"1\":{\"2\":1,\"56\":1,\"65\":1,\"74\":3,\"135\":1,\"165\":1,\"258\":2,\"260\":1}}],[\"basic\",{\"0\":{\"15\":1,\"45\":1,\"69\":1,\"90\":1,\"106\":1,\"127\":1,\"148\":1,\"169\":1,\"186\":1,\"208\":1,\"244\":1}}],[\"bagging\",{\"1\":{\"8\":1}}],[\"bleu評估方法\",{\"1\":{\"231\":1}}],[\"bleu\",{\"1\":{\"227\":1,\"228\":1}}],[\"block\",{\"1\":{\"221\":2}}],[\"blog\",{\"1\":{\"0\":1}}],[\"blue\",{\"1\":{\"216\":5}}],[\"blur\",{\"1\":{\"73\":1}}],[\"blip\",{\"1\":{\"6\":1}}],[\"pmodel​\",{\"1\":{\"254\":1}}],[\"pknow​\",{\"1\":{\"254\":7}}],[\"ppl\",{\"1\":{\"228\":1}}],[\"pdrop​=0\",{\"1\":{\"227\":1}}],[\"pdt​\",{\"1\":{\"134\":1}}],[\"p100\",{\"1\":{\"227\":1}}],[\"py​\",{\"1\":{\"197\":1,\"198\":1}}],[\"pytorch\",{\"1\":{\"1\":1}}],[\"python\",{\"1\":{\"1\":1,\"3\":1}}],[\"psuedo\",{\"1\":{\"203\":1}}],[\"ps​\",{\"1\":{\"197\":1,\"198\":1}}],[\"pseudo\",{\"0\":{\"50\":1,\"135\":1,\"185\":1,\"192\":1},\"1\":{\"50\":3,\"51\":1,\"53\":1,\"54\":1,\"73\":5,\"83\":1,\"131\":2,\"134\":1,\"135\":1,\"173\":5,\"187\":3,\"190\":1,\"191\":1,\"192\":5,\"195\":3,\"196\":2,\"198\":1,\"204\":1}}],[\"pv​ˉ​mix\",{\"1\":{\"173\":1}}],[\"pv​ˉ​t\",{\"1\":{\"173\":1}}],[\"put\",{\"1\":{\"257\":2}}],[\"pus​\",{\"1\":{\"173\":1}}],[\"puigdomènech\",{\"1\":{\"15\":1,\"19\":1,\"20\":1,\"22\":1,\"23\":2,\"25\":1,\"31\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2}}],[\"p+q\",{\"1\":{\"158\":1}}],[\"pq+q\",{\"1\":{\"158\":1}}],[\"pc\",{\"1\":{\"134\":1}}],[\"planners\",{\"1\":{\"247\":3}}],[\"planning\",{\"0\":{\"243\":1,\"247\":1,\"254\":1},\"1\":{\"258\":4,\"261\":1}}],[\"playing\",{\"0\":{\"89\":1}}],[\"plr\",{\"1\":{\"131\":1}}],[\"plrt​\",{\"1\":{\"131\":2}}],[\"p=0\",{\"1\":{\"120\":1}}],[\"pn\",{\"1\":{\"120\":2}}],[\"p\",{\"1\":{\"73\":1,\"75\":2,\"111\":3,\"112\":4,\"113\":2,\"116\":1,\"120\":1,\"131\":1,\"160\":2}}],[\"pt​∥pt\",{\"1\":{\"197\":1}}],[\"pt​\",{\"1\":{\"191\":1,\"195\":2,\"197\":1}}],[\"pt\",{\"1\":{\"73\":2,\"191\":2,\"192\":2,\"197\":1}}],[\"ptifall\",{\"1\":{\"16\":1}}],[\"pomdp\",{\"1\":{\"249\":1}}],[\"posts\",{\"0\":{\"264\":1}}],[\"pos\",{\"1\":{\"224\":4}}],[\"position\",{\"0\":{\"222\":1},\"1\":{\"216\":1,\"221\":2}}],[\"positional\",{\"0\":{\"224\":1},\"1\":{\"135\":1,\"224\":1}}],[\"positive\",{\"1\":{\"16\":3,\"99\":1}}],[\"pooling\",{\"1\":{\"76\":1}}],[\"pole\",{\"1\":{\"60\":1}}],[\"policies\",{\"0\":{\"26\":1}}],[\"policy有什么区别\",{\"1\":{\"165\":1}}],[\"policy\",{\"1\":{\"21\":3,\"23\":2,\"25\":4,\"26\":5,\"29\":1,\"37\":1,\"40\":1,\"41\":1,\"91\":1,\"93\":2,\"95\":2,\"97\":1,\"100\":1,\"101\":1,\"149\":1,\"151\":1,\"155\":6,\"164\":2,\"165\":1,\"249\":1}}],[\"pong\",{\"1\":{\"37\":1,\"99\":1}}],[\"pagent​\",{\"1\":{\"254\":3}}],[\"pair\",{\"1\":{\"227\":1}}],[\"pairs\",{\"1\":{\"227\":2}}],[\"path\",{\"1\":{\"226\":1,\"258\":2}}],[\"patch\",{\"0\":{\"168\":1,\"175\":1},\"1\":{\"170\":1,\"172\":1,\"175\":4,\"176\":1,\"181\":2}}],[\"pan\",{\"1\":{\"186\":1,\"187\":1,\"192\":1,\"201\":1}}],[\"parsing\",{\"0\":{\"229\":1}}],[\"parmar\",{\"1\":{\"208\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1}}],[\"partially\",{\"1\":{\"249\":1}}],[\"part\",{\"1\":{\"165\":1}}],[\"parameters\",{\"1\":{\"107\":1,\"220\":1}}],[\"parameterization\",{\"0\":{\"25\":1,\"35\":1}}],[\"parameter\",{\"0\":{\"151\":1},\"1\":{\"21\":1,\"150\":1,\"151\":7,\"157\":2,\"165\":2}}],[\"pablo\",{\"1\":{\"19\":1,\"20\":1,\"22\":1,\"23\":2}}],[\"paper\",{\"1\":{\"6\":3,\"11\":1,\"16\":2,\"50\":1,\"62\":1,\"63\":1,\"70\":1,\"74\":1,\"79\":1,\"86\":1,\"96\":1,\"112\":1,\"128\":1,\"142\":1,\"151\":1,\"179\":1,\"187\":1,\"258\":4,\"260\":1},\"2\":{\"43\":1,\"67\":1,\"88\":1,\"104\":1,\"125\":1,\"146\":1,\"167\":1,\"184\":1,\"206\":1,\"233\":1,\"263\":1}}],[\"pei\",{\"1\":{\"231\":1}}],[\"peng\",{\"1\":{\"231\":1}}],[\"penalty\",{\"1\":{\"16\":1}}],[\"pe\",{\"1\":{\"224\":2}}],[\"perplexity\",{\"1\":{\"227\":1,\"231\":1}}],[\"perceptron\",{\"1\":{\"94\":1}}],[\"perturbations\",{\"1\":{\"65\":1}}],[\"person\",{\"1\":{\"53\":1,\"61\":1}}],[\"period\",{\"1\":{\"30\":2,\"36\":1}}],[\"per\",{\"1\":{\"19\":1,\"226\":1}}],[\"performance\",{\"1\":{\"16\":2,\"34\":1,\"35\":1,\"36\":1,\"61\":2,\"62\":1,\"64\":1,\"73\":1,\"74\":1,\"75\":1,\"80\":4,\"81\":1,\"84\":1,\"85\":1,\"100\":1,\"138\":1,\"141\":1,\"142\":1,\"143\":1,\"187\":1}}],[\"pick\",{\"1\":{\"257\":1}}],[\"pipa\",{\"0\":{\"168\":1},\"1\":{\"170\":1,\"178\":2,\"179\":2,\"181\":1}}],[\"pietquin\",{\"1\":{\"149\":1}}],[\"pixels\",{\"1\":{\"83\":2}}],[\"pixel\",{\"0\":{\"168\":1,\"174\":1},\"1\":{\"49\":1,\"76\":1,\"101\":1,\"170\":2,\"172\":1,\"174\":4,\"175\":2,\"176\":1,\"181\":1,\"191\":1}}],[\"pitfall\",{\"1\":{\"16\":3,\"37\":1}}],[\"piot\",{\"1\":{\"15\":1,\"25\":1,\"31\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2,\"148\":1,\"162\":3,\"163\":1}}],[\"prior\",{\"1\":{\"249\":1}}],[\"prioritized\",{\"1\":{\"41\":1}}],[\"private\",{\"1\":{\"9\":1,\"16\":2,\"37\":1}}],[\"preliminaries\",{\"0\":{\"249\":1}}],[\"preliminary\",{\"0\":{\"131\":1,\"190\":1}}],[\"prevent\",{\"0\":{\"105\":1}}],[\"preprocess\",{\"1\":{\"97\":1}}],[\"pretraining\",{\"1\":{\"113\":1}}],[\"pretrain\",{\"1\":{\"79\":1,\"197\":1}}],[\"pretrained\",{\"1\":{\"5\":1,\"56\":1,\"76\":1,\"113\":2,\"200\":1}}],[\"prediction\",{\"1\":{\"50\":2,\"170\":1}}],[\"prefix\",{\"1\":{\"7\":2}}],[\"pre\",{\"1\":{\"2\":1}}],[\"process\",{\"1\":{\"249\":1}}],[\"processing\",{\"1\":{\"210\":1}}],[\"procedures\",{\"1\":{\"247\":1}}],[\"prompting\",{\"1\":{\"258\":2}}],[\"prompt\",{\"1\":{\"247\":1,\"251\":1,\"252\":2,\"257\":2,\"258\":8,\"260\":3}}],[\"product\",{\"0\":{\"218\":1},\"1\":{\"216\":2,\"220\":1}}],[\"proda\",{\"1\":{\"73\":1,\"79\":1,\"170\":1,\"196\":1,\"198\":2,\"200\":1,\"203\":1,\"204\":1}}],[\"prototypes\",{\"1\":{\"203\":1}}],[\"prototype\",{\"0\":{\"194\":1},\"1\":{\"192\":4,\"194\":3,\"195\":1,\"196\":1}}],[\"prototypical\",{\"0\":{\"185\":1,\"192\":1},\"1\":{\"196\":1,\"198\":1,\"204\":1}}],[\"probability\",{\"1\":{\"152\":1,\"191\":1}}],[\"program\",{\"2\":{\"242\":1}}],[\"programming\",{\"1\":{\"1\":2}}],[\"progress\",{\"1\":{\"41\":1}}],[\"proxy\",{\"1\":{\"6\":1}}],[\"projection\",{\"1\":{\"174\":1,\"175\":1}}],[\"project\",{\"1\":{\"2\":1}}],[\"ce\",{\"1\":{\"195\":1}}],[\"cia\",{\"1\":{\"238\":1}}],[\"cisyscapes\",{\"1\":{\"181\":1}}],[\"cifar\",{\"1\":{\"116\":4}}],[\"cityscape\",{\"1\":{\"64\":1,\"74\":3}}],[\"cityscapes\",{\"0\":{\"58\":1,\"61\":1,\"62\":1,\"201\":1,\"202\":1},\"1\":{\"57\":3,\"59\":1,\"60\":1,\"63\":1,\"79\":1,\"83\":1,\"128\":1,\"137\":1,\"138\":2,\"139\":1,\"178\":3,\"179\":5,\"182\":2,\"200\":1,\"201\":1,\"202\":1}}],[\"city\",{\"1\":{\"60\":1}}],[\"cthings​\",{\"1\":{\"76\":1}}],[\"c=argmaxc\",{\"1\":{\"73\":1,\"131\":1}}],[\"cce\",{\"1\":{\"73\":1,\"131\":1}}],[\"cbst\",{\"1\":{\"50\":1}}],[\"closed\",{\"1\":{\"257\":1}}],[\"cluster\",{\"1\":{\"192\":3,\"196\":1}}],[\"clustering\",{\"1\":{\"192\":1}}],[\"club\",{\"1\":{\"3\":1}}],[\"classifier\",{\"1\":{\"191\":1}}],[\"classes\",{\"1\":{\"51\":1,\"58\":1,\"59\":2,\"60\":3,\"62\":1,\"63\":2,\"73\":1,\"74\":2,\"75\":4,\"76\":4,\"80\":2,\"179\":2,\"190\":1,\"201\":1}}],[\"classmix\",{\"1\":{\"51\":2,\"53\":1,\"54\":1,\"56\":1,\"64\":1,\"65\":1,\"73\":1}}],[\"class\",{\"0\":{\"75\":1,\"76\":1,\"82\":1,\"83\":1},\"1\":{\"50\":2,\"53\":5,\"61\":2,\"62\":1,\"65\":1,\"74\":2,\"75\":3,\"76\":2,\"80\":1,\"82\":2,\"83\":1,\"84\":1,\"131\":2,\"138\":2,\"139\":3,\"174\":3,\"179\":1,\"191\":2,\"192\":1,\"193\":2,\"194\":2,\"195\":4,\"196\":1}}],[\"cnn\",{\"0\":{\"211\":1},\"1\":{\"47\":2,\"74\":1,\"77\":1,\"91\":1,\"211\":2,\"226\":2}}],[\"china\",{\"1\":{\"186\":1}}],[\"chieh\",{\"1\":{\"144\":1}}],[\"chiehchen\",{\"1\":{\"47\":1}}],[\"chua\",{\"1\":{\"169\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3}}],[\"chen\",{\"1\":{\"134\":1,\"144\":1,\"169\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3,\"186\":1,\"187\":1,\"192\":1,\"201\":1,\"231\":1}}],[\"chain\",{\"1\":{\"258\":1}}],[\"chapter\",{\"1\":{\"216\":4,\"231\":1}}],[\"channel\",{\"1\":{\"101\":1,\"131\":1,\"132\":1,\"135\":1}}],[\"channels\",{\"1\":{\"74\":1}}],[\"chalmers\",{\"1\":{\"45\":1}}],[\"chns=max\",{\"1\":{\"34\":1}}],[\"chns\",{\"1\":{\"34\":1}}],[\"curiosity\",{\"1\":{\"19\":1}}],[\"cuda\",{\"1\":{\"6\":1}}],[\"creature\",{\"1\":{\"216\":6}}],[\"credit\",{\"1\":{\"16\":1,\"23\":1,\"30\":2}}],[\"critic\",{\"1\":{\"155\":1,\"165\":1}}],[\"crop\",{\"0\":{\"133\":1,\"139\":1,\"140\":1},\"1\":{\"133\":9,\"134\":6,\"135\":3,\"139\":4,\"140\":8,\"141\":1,\"142\":2,\"143\":1,\"175\":2,\"181\":4}}],[\"cross\",{\"0\":{\"44\":1},\"1\":{\"54\":1,\"65\":1,\"73\":1,\"131\":1,\"134\":1,\"191\":1,\"195\":2,\"204\":1}}],[\"cryptography\",{\"1\":{\"1\":1}}],[\"cvpr2022\",{\"1\":{\"86\":1}}],[\"cvpr22\",{\"1\":{\"86\":1}}],[\"cvpr\",{\"1\":{\"69\":1,\"186\":1},\"2\":{\"88\":1,\"206\":1}}],[\"cv\",{\"1\":{\"6\":1,\"46\":1}}],[\"cpu\",{\"1\":{\"5\":1}}],[\"cabinet\",{\"1\":{\"257\":4}}],[\"caution\",{\"1\":{\"116\":1,\"198\":1}}],[\"categorical\",{\"1\":{\"73\":1,\"131\":1}}],[\"carlo\",{\"1\":{\"157\":1}}],[\"car\",{\"1\":{\"61\":1}}],[\"cars\",{\"1\":{\"45\":1}}],[\"careerhack\",{\"0\":{\"4\":1},\"1\":{\"2\":1},\"2\":{\"13\":1}}],[\"capped\",{\"1\":{\"34\":1,\"38\":2}}],[\"camp\",{\"1\":{\"3\":2}}],[\"cs\",{\"1\":{\"3\":1}}],[\"cot\",{\"1\":{\"258\":1}}],[\"coe\",{\"1\":{\"236\":1}}],[\"cosine\",{\"1\":{\"174\":2,\"175\":1}}],[\"count\",{\"1\":{\"165\":1}}],[\"cover\",{\"1\":{\"119\":1}}],[\"co\",{\"1\":{\"119\":2}}],[\"common\",{\"1\":{\"75\":1}}],[\"complexity\",{\"1\":{\"226\":2}}],[\"compact\",{\"1\":{\"187\":1}}],[\"comparison\",{\"0\":{\"141\":1,\"179\":1}}],[\"competition\",{\"1\":{\"2\":1}}],[\"computer\",{\"1\":{\"0\":1,\"1\":1,\"45\":1,\"65\":1,\"96\":1},\"2\":{\"67\":1,\"88\":1,\"146\":1,\"184\":1,\"206\":1,\"233\":1}}],[\"color\",{\"1\":{\"73\":1,\"101\":1}}],[\"column\",{\"1\":{\"50\":1}}],[\"cordts\",{\"1\":{\"58\":1}}],[\"corss\",{\"0\":{\"54\":1}}],[\"coin\",{\"1\":{\"35\":6}}],[\"consine\",{\"1\":{\"259\":1}}],[\"consistency\",{\"0\":{\"196\":1},\"1\":{\"176\":2}}],[\"constituency\",{\"0\":{\"229\":1}}],[\"concat\",{\"1\":{\"220\":2}}],[\"connected\",{\"1\":{\"116\":1}}],[\"convs2s\",{\"1\":{\"211\":1}}],[\"conv\",{\"1\":{\"116\":2}}],[\"convolution\",{\"1\":{\"84\":1,\"211\":1}}],[\"contingency\",{\"1\":{\"101\":1}}],[\"context\",{\"0\":{\"126\":1,\"133\":1},\"1\":{\"49\":1,\"133\":7,\"134\":6,\"140\":3,\"142\":3,\"144\":2,\"215\":1,\"216\":1}}],[\"contrast\",{\"1\":{\"181\":2,\"204\":2}}],[\"contrastive\",{\"0\":{\"174\":1,\"175\":1},\"1\":{\"171\":1,\"172\":2,\"174\":1}}],[\"contribution\",{\"0\":{\"40\":1,\"64\":1,\"85\":1,\"102\":1,\"122\":1,\"143\":1,\"164\":1,\"182\":1,\"203\":1,\"230\":1,\"261\":1}}],[\"controller\",{\"1\":{\"26\":5,\"27\":1,\"32\":1,\"37\":7,\"38\":2,\"40\":1}}],[\"control\",{\"1\":{\"21\":1,\"95\":2,\"101\":1}}],[\"conf\",{\"1\":{\"244\":1}}],[\"conflation\",{\"1\":{\"53\":1}}],[\"conference\",{\"1\":{\"45\":1}}],[\"confidence\",{\"0\":{\"27\":1}}],[\"conroller\",{\"1\":{\"37\":1}}],[\"code\",{\"1\":{\"2\":1}}],[\"c++\",{\"1\":{\"1\":1,\"3\":1}}],[\"c\",{\"1\":{\"1\":1,\"3\":1,\"73\":9,\"75\":5,\"76\":4,\"174\":1}}],[\"sn​\",{\"1\":{\"253\":1}}],[\"s0​\",{\"1\":{\"253\":1}}],[\"s×a→s\",{\"1\":{\"249\":1}}],[\"smarter\",{\"1\":{\"257\":1}}],[\"small\",{\"1\":{\"36\":1,\"76\":3,\"133\":2}}],[\"smoothing\",{\"1\":{\"227\":1}}],[\"shelf\",{\"1\":{\"257\":16}}],[\"shot\",{\"1\":{\"247\":3,\"258\":2}}],[\"short\",{\"1\":{\"210\":1}}],[\"shuofei\",{\"1\":{\"244\":1,\"250\":1,\"260\":7}}],[\"sharon\",{\"1\":{\"231\":1}}],[\"shazeer\",{\"1\":{\"208\":1,\"217\":1,\"219\":1,\"220\":1,\"221\":2,\"226\":1,\"228\":1,\"229\":1}}],[\"shc​−hd​\",{\"1\":{\"133\":1}}],[\"shift\",{\"1\":{\"46\":3,\"50\":2,\"54\":1}}],[\"s=2\",{\"1\":{\"133\":1}}],[\"s=1∏t​cs​\",{\"1\":{\"21\":1}}],[\"svhn\",{\"1\":{\"116\":2}}],[\"srivastava\",{\"1\":{\"106\":1,\"111\":2,\"112\":1,\"115\":1,\"116\":5,\"117\":1,\"119\":2,\"120\":1,\"121\":1}}],[\"src\",{\"1\":{\"74\":1}}],[\"swc​−hw​\",{\"1\":{\"133\":1}}],[\"sw\",{\"1\":{\"61\":1,\"62\":1}}],[\"swear\",{\"1\":{\"16\":1}}],[\"sky\",{\"1\":{\"61\":1,\"76\":1}}],[\"skipping\",{\"1\":{\"99\":1}}],[\"skiiing\",{\"1\":{\"23\":1}}],[\"skiing\",{\"1\":{\"16\":5,\"37\":1}}],[\"skills\",{\"0\":{\"1\":1}}],[\"sb​\",{\"1\":{\"51\":2}}],[\"symmetric\",{\"1\":{\"195\":1,\"204\":1}}],[\"synthesis\",{\"0\":{\"251\":1},\"1\":{\"251\":1}}],[\"synthetic\",{\"1\":{\"57\":1,\"59\":1,\"60\":1}}],[\"synthia\",{\"0\":{\"60\":1,\"62\":1,\"202\":1},\"1\":{\"57\":2,\"62\":1,\"63\":1,\"79\":1,\"137\":1,\"138\":1,\"178\":2,\"179\":3,\"182\":1,\"200\":1,\"202\":1}}],[\"synethic\",{\"1\":{\"46\":1,\"47\":1,\"49\":1}}],[\"sylwia\",{\"1\":{\"50\":1}}],[\"ssl\",{\"1\":{\"50\":1,\"64\":1}}],[\"speech\",{\"0\":{\"117\":1}}],[\"sparse\",{\"1\":{\"91\":1}}],[\"space\",{\"0\":{\"151\":1},\"1\":{\"65\":1,\"99\":2,\"150\":1,\"151\":8,\"157\":2,\"165\":1,\"249\":4,\"252\":1}}],[\"spatial\",{\"1\":{\"49\":2}}],[\"sprechmann\",{\"1\":{\"19\":1,\"20\":1,\"22\":1,\"23\":2}}],[\"salakhutdinov\",{\"1\":{\"106\":1}}],[\"sarsa\",{\"1\":{\"101\":2}}],[\"sa​\",{\"1\":{\"51\":3}}],[\"sampling\",{\"0\":{\"44\":1,\"54\":1,\"75\":1,\"82\":1},\"1\":{\"65\":1,\"74\":1,\"131\":1}}],[\"sample\",{\"1\":{\"23\":1,\"25\":2,\"26\":1,\"29\":1,\"32\":1,\"75\":2,\"82\":2,\"157\":1,\"254\":2}}],[\"sampled\",{\"1\":{\"21\":1}}],[\"safe\",{\"1\":{\"41\":1,\"257\":8}}],[\"sub\",{\"1\":{\"227\":1}}],[\"sutskever\",{\"1\":{\"106\":1}}],[\"supervise\",{\"1\":{\"50\":2}}],[\"supervised\",{\"0\":{\"168\":1,\"197\":1},\"1\":{\"50\":1,\"65\":5,\"70\":2,\"74\":1,\"139\":2,\"170\":1,\"187\":2}}],[\"summarize\",{\"1\":{\"252\":1}}],[\"summarization\",{\"0\":{\"252\":1},\"1\":{\"254\":1}}],[\"summary\",{\"0\":{\"38\":1,\"80\":1},\"1\":{\"83\":1}}],[\"summer\",{\"1\":{\"3\":1}}],[\"summercamp\",{\"1\":{\"3\":1}}],[\"surround\",{\"1\":{\"37\":1}}],[\"sidetable\",{\"1\":{\"257\":1}}],[\"sidewalk\",{\"1\":{\"53\":1}}],[\"singapore\",{\"1\":{\"244\":1}}],[\"single\",{\"1\":{\"159\":1,\"220\":5}}],[\"simclrv2\",{\"1\":{\"197\":1,\"200\":1}}],[\"similarity\",{\"1\":{\"174\":2,\"175\":1}}],[\"simple\",{\"0\":{\"105\":1},\"1\":{\"64\":1,\"165\":1}}],[\"simplified\",{\"0\":{\"29\":1}}],[\"si​\",{\"1\":{\"155\":2}}],[\"silver\",{\"1\":{\"90\":1,\"97\":1,\"100\":1,\"101\":1,\"153\":1}}],[\"size\",{\"0\":{\"30\":1,\"36\":1,\"121\":1,\"139\":1,\"140\":1},\"1\":{\"76\":1,\"121\":2,\"139\":4,\"140\":5,\"141\":1,\"143\":1,\"181\":4,\"259\":1}}],[\"sitcon\",{\"1\":{\"3\":2}}],[\"sliding\",{\"0\":{\"28\":1,\"29\":1,\"135\":1},\"1\":{\"28\":1,\"29\":1,\"135\":1}}],[\"s\",{\"1\":{\"25\":2,\"93\":4,\"97\":1,\"131\":1,\"132\":1,\"133\":1,\"134\":2,\"135\":1,\"152\":4,\"154\":16,\"159\":8,\"173\":1,\"174\":2,\"249\":2,\"252\":6,\"253\":1}}],[\"sofa\",{\"1\":{\"257\":1}}],[\"soft\",{\"1\":{\"191\":1,\"192\":2,\"196\":1}}],[\"softmax\",{\"0\":{\"223\":1},\"1\":{\"75\":1,\"191\":1,\"193\":2,\"216\":1,\"218\":2,\"219\":1,\"221\":1,\"227\":1,\"254\":1}}],[\"solution\",{\"1\":{\"163\":2}}],[\"solaris\",{\"1\":{\"16\":3,\"23\":1,\"36\":1,\"37\":1}}],[\"some\",{\"0\":{\"63\":1},\"1\":{\"257\":1}}],[\"source\",{\"1\":{\"46\":3,\"47\":1,\"49\":2,\"50\":2,\"54\":3,\"61\":3,\"62\":1,\"70\":1,\"73\":3,\"74\":2,\"75\":1,\"83\":1,\"131\":5,\"134\":1,\"170\":3,\"173\":4,\"174\":1,\"187\":1,\"190\":3,\"192\":2,\"197\":2,\"198\":3}}],[\"sota\",{\"1\":{\"16\":1,\"64\":1,\"84\":3,\"115\":1,\"138\":3,\"143\":2,\"170\":1,\"201\":1,\"202\":1,\"203\":1,\"212\":1,\"245\":2}}],[\"sort\",{\"1\":{\"4\":1}}],[\"seen\",{\"1\":{\"260\":1}}],[\"see\",{\"1\":{\"257\":3}}],[\"seed\",{\"1\":{\"75\":1,\"82\":3}}],[\"sentence\",{\"1\":{\"227\":3}}],[\"seng\",{\"1\":{\"169\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3}}],[\"sequential\",{\"1\":{\"226\":2}}],[\"sequence\",{\"1\":{\"25\":1,\"211\":1,\"212\":1,\"214\":1,\"215\":5,\"216\":3,\"218\":3,\"220\":1,\"221\":1,\"224\":1,\"253\":3,\"259\":1}}],[\"sequences\",{\"1\":{\"21\":1}}],[\"seq2seq\",{\"0\":{\"215\":1},\"1\":{\"213\":1,\"215\":2,\"216\":1}}],[\"selection\",{\"0\":{\"140\":1}}],[\"self\",{\"0\":{\"50\":1,\"73\":1,\"168\":1,\"197\":1,\"212\":1,\"226\":1},\"1\":{\"65\":1,\"71\":1,\"73\":2,\"74\":1,\"129\":1,\"131\":1,\"170\":1,\"187\":1,\"188\":1,\"201\":1,\"212\":2,\"221\":1,\"226\":3,\"251\":1}}],[\"sex\",{\"1\":{\"107\":1}}],[\"seaquest\",{\"1\":{\"99\":1}}],[\"separable\",{\"1\":{\"84\":1}}],[\"separation\",{\"1\":{\"74\":1}}],[\"separate\",{\"1\":{\"35\":6,\"37\":1,\"38\":1,\"40\":1}}],[\"segformer\",{\"1\":{\"74\":3,\"81\":1,\"137\":1}}],[\"segmentationb\",{\"1\":{\"170\":1}}],[\"segmentation\",{\"0\":{\"68\":1,\"126\":1,\"168\":1,\"185\":1},\"1\":{\"47\":1,\"49\":2,\"50\":1,\"51\":1,\"54\":1,\"56\":1,\"65\":5,\"69\":1,\"70\":3,\"71\":1,\"74\":2,\"76\":1,\"86\":2,\"128\":1,\"129\":1,\"132\":1,\"133\":5,\"144\":2,\"170\":1,\"187\":1,\"190\":1,\"191\":2,\"195\":1,\"200\":2,\"203\":1,\"204\":1}}],[\"sematic\",{\"1\":{\"71\":1,\"129\":1}}],[\"semantic\",{\"0\":{\"68\":1,\"126\":1,\"168\":1,\"185\":1},\"1\":{\"47\":1,\"49\":3,\"50\":1,\"51\":4,\"65\":4,\"69\":1,\"70\":3,\"74\":2,\"76\":1,\"86\":2,\"128\":1,\"133\":6,\"144\":2,\"170\":2,\"187\":1,\"191\":2,\"195\":1,\"203\":1,\"204\":1}}],[\"semi\",{\"1\":{\"50\":2,\"65\":5,\"70\":1,\"187\":1}}],[\"setups\",{\"0\":{\"259\":1}}],[\"settings\",{\"0\":{\"34\":1}}],[\"set\",{\"1\":{\"8\":1,\"9\":1,\"63\":5,\"100\":2}}],[\"security\",{\"1\":{\"3\":1}}],[\"scheduler\",{\"1\":{\"259\":1}}],[\"schedule\",{\"1\":{\"236\":1}}],[\"sce\",{\"1\":{\"195\":1}}],[\"scienceworld\",{\"1\":{\"245\":1,\"257\":4,\"260\":1}}],[\"science\",{\"1\":{\"186\":1}}],[\"scist\",{\"1\":{\"0\":1,\"3\":1}}],[\"scalable\",{\"1\":{\"165\":1,\"257\":1}}],[\"scaled\",{\"0\":{\"218\":1},\"1\":{\"218\":1,\"220\":1}}],[\"scale\",{\"1\":{\"74\":1,\"134\":3,\"137\":1,\"142\":2,\"143\":1}}],[\"score\",{\"1\":{\"227\":1}}],[\"scorebaseline​\",{\"1\":{\"162\":1}}],[\"scorehuman​\",{\"1\":{\"162\":1}}],[\"scores\",{\"1\":{\"34\":2}}],[\"script\",{\"1\":{\"5\":1}}],[\"style\",{\"1\":{\"258\":1}}],[\"st+i​\",{\"1\":{\"155\":3,\"160\":4}}],[\"st+1​\",{\"1\":{\"97\":1}}],[\"studies\",{\"0\":{\"181\":1}}],[\"study\",{\"0\":{\"142\":1},\"1\":{\"260\":1}}],[\"student\",{\"1\":{\"73\":2,\"128\":1,\"131\":2,\"173\":1,\"197\":2,\"198\":4}}],[\"stuff\",{\"1\":{\"76\":1}}],[\"stop\",{\"1\":{\"63\":1}}],[\"storage\",{\"1\":{\"5\":1}}],[\"structure\",{\"0\":{\"185\":1,\"196\":1},\"1\":{\"187\":1,\"198\":1,\"204\":1}}],[\"structured\",{\"1\":{\"65\":1}}],[\"stride\",{\"1\":{\"133\":1}}],[\"string\",{\"1\":{\"7\":2}}],[\"street\",{\"1\":{\"116\":1}}],[\"strong\",{\"1\":{\"65\":1}}],[\"strategies\",{\"0\":{\"68\":1},\"1\":{\"41\":1,\"74\":2,\"86\":1,\"165\":1}}],[\"step\",{\"1\":{\"227\":2,\"254\":5,\"260\":2}}],[\"stephan\",{\"1\":{\"59\":1}}],[\"steps−1\",{\"1\":{\"227\":1}}],[\"steps=4000\",{\"1\":{\"227\":1}}],[\"steps\",{\"1\":{\"35\":1,\"260\":1}}],[\"steven\",{\"1\":{\"15\":1,\"25\":1,\"31\":1,\"34\":1,\"35\":2,\"36\":2,\"37\":2,\"38\":2}}],[\"st​ht​​∼πϕ​\",{\"1\":{\"254\":1}}],[\"st​∈s\",{\"1\":{\"252\":1}}],[\"st​∼πθ​\",{\"1\":{\"252\":1}}],[\"st​\",{\"1\":{\"20\":1,\"97\":1,\"131\":1,\"254\":7}}],[\"stacks\",{\"0\":{\"221\":1}}],[\"stage\",{\"1\":{\"192\":2}}],[\"state\",{\"0\":{\"25\":1,\"35\":1,\"252\":1},\"1\":{\"19\":1,\"25\":4,\"32\":1,\"91\":1,\"93\":1,\"97\":3,\"100\":1,\"152\":1,\"154\":1,\"155\":1,\"210\":2,\"245\":4,\"249\":1,\"252\":10,\"253\":4,\"254\":7,\"260\":10,\"261\":1}}],[\"start\",{\"1\":{\"2\":2}}],[\"也都有這樣的設計\",{\"1\":{\"258\":1}}],[\"也產出\",{\"1\":{\"254\":1}}],[\"也避免不必要的\",{\"1\":{\"251\":1}}],[\"也避免了上述提及的幾個問題\",{\"1\":{\"91\":1}}],[\"也持續購買各種生活物品\",{\"1\":{\"238\":1}}],[\"也終於領到了學生證\",{\"1\":{\"238\":1}}],[\"也稍微聊了一下\",{\"1\":{\"237\":1}}],[\"也買了垃圾袋\",{\"1\":{\"237\":1}}],[\"也很開心能聽得懂對方的日文\",{\"1\":{\"236\":1}}],[\"也很期待未來也還有機會可以繼續了解和開發\",{\"1\":{\"11\":1}}],[\"也回覆了簡單的\",{\"1\":{\"236\":1}}],[\"也讓工作人員看了一下護照\",{\"1\":{\"236\":1}}],[\"也讓我認識到\",{\"1\":{\"11\":1}}],[\"也取得了筑波大學交換一學年的機會\",{\"1\":{\"235\":1}}],[\"也不是稀有到一個禮拜都遇不到\",{\"1\":{\"240\":1}}],[\"也不知道為什麼在学園東大通り這種車流量大的路段還會有一整路沒有路燈\",{\"1\":{\"236\":1}}],[\"也不知道後面會輸出\",{\"1\":{\"219\":1}}],[\"也不斷在猶豫到底要不要嘗試用日文溝通\",{\"1\":{\"236\":1}}],[\"也不需要擔心中間的流失\",{\"1\":{\"226\":1}}],[\"也標記後如下圖所示\",{\"1\":{\"218\":1}}],[\"也能夠將過去的資訊好好地保留\",{\"1\":{\"212\":1}}],[\"也能感受到他們對我們的提問的重視\",{\"1\":{\"11\":1}}],[\"也額外定義\",{\"1\":{\"191\":1}}],[\"也如同\",{\"1\":{\"173\":1}}],[\"也與過去的做法相同\",{\"1\":{\"173\":1}}],[\"也並不是每次加上\",{\"1\":{\"162\":1}}],[\"也希望能夠有一個部分能夠讓模型能同時考慮\",{\"1\":{\"134\":1}}],[\"也因此確認了上面的結論\",{\"1\":{\"260\":1}}],[\"也因此在圖片的輸入上通常會刻意先將輸入圖片的解析度降低\",{\"1\":{\"128\":1}}],[\"也因為如此\",{\"1\":{\"26\":1}}],[\"也許距離稍稍受限\",{\"1\":{\"238\":1}}],[\"也許你只是缺一罐一日分の野菜\",{\"1\":{\"237\":1}}],[\"也許過程比想像中簡單許多\",{\"1\":{\"236\":1}}],[\"也許某一個\",{\"1\":{\"119\":1}}],[\"也許才有機會遇到\",{\"1\":{\"16\":1}}],[\"也確實發現會平滑許多\",{\"1\":{\"100\":1}}],[\"也變得能夠預測了\",{\"1\":{\"80\":1}}],[\"也可以拆成\",{\"1\":{\"191\":1}}],[\"也可以達到類似的效果\",{\"1\":{\"158\":1}}],[\"也可以看出\",{\"1\":{\"138\":1}}],[\"也可以再進一步加上其他的技巧去降低\",{\"1\":{\"116\":1}}],[\"也可以應用在\",{\"1\":{\"113\":1}}],[\"也可以搭配其他的\",{\"1\":{\"113\":1}}],[\"也可以是\",{\"1\":{\"73\":1}}],[\"也可以盡可能至少在\",{\"1\":{\"8\":1}}],[\"也會被納入考量產生\",{\"1\":{\"216\":1,\"221\":1}}],[\"也會將這個\",{\"1\":{\"214\":1}}],[\"也會因為擷取到過於細節的特徵導致大的物件無法好好辨認\",{\"1\":{\"128\":1}}],[\"也會最大\",{\"1\":{\"82\":1}}],[\"也會期待其信心水平也要是高的\",{\"1\":{\"73\":1}}],[\"也會依據得到的\",{\"1\":{\"26\":1}}],[\"也跟最後評估的\",{\"1\":{\"63\":1}}],[\"也獲得不錯的成果\",{\"1\":{\"47\":1}}],[\"也說明了實際上這樣的更新方式是能夠依照不同的遊戲去適應的\",{\"1\":{\"163\":1}}],[\"也說明了\",{\"1\":{\"38\":1}}],[\"也限制了數值範圍\",{\"1\":{\"34\":1}}],[\"也就使梯度收斂\",{\"1\":{\"218\":1}}],[\"也就可以更好地減輕\",{\"1\":{\"196\":1}}],[\"也就可以用\",{\"1\":{\"131\":1}}],[\"也就意味著擷取自己跟大家的相似度有多少\",{\"1\":{\"221\":1}}],[\"也就意味著並不是所有的\",{\"1\":{\"173\":1}}],[\"也就意味著需要\",{\"1\":{\"158\":1}}],[\"也就不需要再使用\",{\"1\":{\"159\":1,\"160\":1}}],[\"也就不會有需要儲存過多資訊導致的記憶體不足問題\",{\"1\":{\"135\":1}}],[\"也就如下\",{\"1\":{\"155\":1}}],[\"也就形成\",{\"1\":{\"76\":1}}],[\"也就有更高的機會可以學更多次\",{\"1\":{\"75\":1}}],[\"也就是圖中的\",{\"1\":{\"260\":1}}],[\"也就是多考慮了\",{\"1\":{\"253\":1}}],[\"也就是知道\",{\"1\":{\"251\":1}}],[\"也就是有些\",{\"1\":{\"196\":1}}],[\"也就是對應到\",{\"1\":{\"116\":1}}],[\"也就是要讓底下的\",{\"1\":{\"93\":1}}],[\"也就是要找到\",{\"1\":{\"93\":1}}],[\"也就是在\",{\"1\":{\"93\":1}}],[\"也就是\",{\"1\":{\"30\":1,\"134\":1,\"153\":1,\"155\":1,\"174\":1,\"191\":1,\"195\":1,\"221\":1}}],[\"也就是讓底下的期望值最大化\",{\"1\":{\"27\":1}}],[\"也就是說在時間\",{\"1\":{\"219\":1}}],[\"也就是說會在訓練的過程當中透過當下的預測給這些\",{\"1\":{\"187\":1}}],[\"也就是說最後的\",{\"1\":{\"163\":1}}],[\"也就是說對於一個參數\",{\"1\":{\"157\":1}}],[\"也就是說理想上每經過一輪更新\",{\"1\":{\"100\":1}}],[\"也就是說可以直接從\",{\"1\":{\"96\":1}}],[\"也就是說我們會期待產生出來的\",{\"1\":{\"73\":1}}],[\"也就是說我們對於\",{\"1\":{\"50\":1}}],[\"也就是說我現在面前有\",{\"1\":{\"27\":1}}],[\"也就是說能夠順利到達\",{\"1\":{\"35\":1}}],[\"也就是說\",{\"1\":{\"27\":1,\"73\":1,\"74\":1,\"75\":1,\"93\":1,\"116\":1,\"163\":1,\"170\":1,\"216\":1,\"220\":1,\"222\":1,\"249\":1,\"251\":1,\"253\":3}}],[\"也就是說這種做法的正確性是被確保的\",{\"1\":{\"25\":1}}],[\"也就是前面定義的\",{\"1\":{\"19\":1}}],[\"也就能夠得到\",{\"1\":{\"21\":1}}],[\"也就很難往下一步去發展\",{\"1\":{\"11\":1}}],[\"也提出了一個可以在所有\",{\"1\":{\"16\":1}}],[\"也需要嘗試越過那些障礙\",{\"1\":{\"16\":1}}],[\"也寫了一個評分程式去評估好壞\",{\"1\":{\"7\":1}}],[\"也想說難得有不錯的運算資源\",{\"1\":{\"6\":1}}],[\"也有一些相異於\",{\"1\":{\"211\":1}}],[\"也有介紹過同樣的\",{\"1\":{\"200\":1}}],[\"也有部分是源自於這樣的相似性帶來的好處\",{\"1\":{\"49\":1}}],[\"也有人套了\",{\"1\":{\"10\":1}}],[\"也有聽說有部分的組別\",{\"1\":{\"6\":1}}],[\"也有先提供了一些資源\",{\"1\":{\"5\":1}}],[\"也開放資源\",{\"1\":{\"6\":1}}],[\"也歡迎一起來討論\",{\"1\":{\"0\":1}}],[\"也是單一的\",{\"1\":{\"260\":1}}],[\"也是評量一個語言模型優劣的方法\",{\"1\":{\"228\":1}}],[\"也是想要解決\",{\"1\":{\"187\":1}}],[\"也是有些包含浮水印\",{\"1\":{\"7\":1}}],[\"也是頗有趣\",{\"1\":{\"6\":1}}],[\"也是\",{\"1\":{\"0\":1}}],[\"紀錄學習的點滴\",{\"1\":{\"0\":1}}],[\"m2​\",{\"1\":{\"175\":2}}],[\"m1​\",{\"1\":{\"175\":2}}],[\"m=1nt​​\",{\"1\":{\"131\":1}}],[\"m=1ns​​\",{\"1\":{\"131\":2}}],[\"m​∈rht​×wt​×3\",{\"1\":{\"131\":1}}],[\"m​∈rhs​×ws​×3\",{\"1\":{\"131\":1}}],[\"m​∈\",{\"1\":{\"131\":1}}],[\"m​\",{\"1\":{\"131\":3}}],[\"mnist\",{\"1\":{\"116\":2,\"119\":1}}],[\"mnih\",{\"1\":{\"90\":1,\"97\":1,\"100\":1,\"101\":1}}],[\"mdp\",{\"1\":{\"93\":1}}],[\"mmsegmentation\",{\"1\":{\"79\":1,\"178\":1}}],[\"mthings\",{\"1\":{\"76\":2}}],[\"mpi\",{\"1\":{\"69\":1,\"127\":1}}],[\"mscoco\",{\"1\":{\"56\":1}}],[\"m\",{\"1\":{\"51\":3,\"218\":1}}],[\"mu\",{\"1\":{\"169\":1,\"174\":1,\"175\":1,\"176\":1,\"179\":2,\"180\":1,\"181\":3,\"231\":1}}],[\"mutation\",{\"1\":{\"107\":1}}],[\"multihead\",{\"1\":{\"220\":1}}],[\"multimedia\",{\"1\":{\"169\":1},\"2\":{\"184\":1}}],[\"multi\",{\"0\":{\"134\":1,\"220\":1},\"1\":{\"27\":1,\"94\":1,\"143\":1,\"160\":1,\"212\":1,\"220\":4,\"221\":4,\"261\":1}}],[\"muzero\",{\"1\":{\"16\":3,\"38\":3}}],[\"moco\",{\"1\":{\"204\":1}}],[\"momentum\",{\"1\":{\"193\":1,\"204\":2}}],[\"modal\",{\"1\":{\"261\":1}}],[\"modified\",{\"1\":{\"187\":1}}],[\"models\",{\"1\":{\"76\":1,\"201\":1,\"247\":3}}],[\"model\",{\"0\":{\"112\":1,\"197\":1,\"228\":1,\"243\":1,\"253\":1,\"254\":1},\"1\":{\"0\":1,\"5\":1,\"6\":1,\"50\":1,\"61\":1,\"73\":4,\"74\":1,\"93\":1,\"94\":1,\"95\":1,\"113\":1,\"131\":1,\"137\":1,\"143\":1,\"153\":1,\"197\":3,\"198\":6,\"203\":1,\"210\":1,\"227\":2,\"230\":1,\"245\":5,\"249\":1,\"251\":1,\"253\":7,\"254\":3,\"260\":7,\"261\":1}}],[\"monte\",{\"1\":{\"157\":1}}],[\"montezuma\",{\"1\":{\"16\":2,\"37\":1}}],[\"mohammad\",{\"1\":{\"148\":1,\"162\":3,\"163\":1}}],[\"moving\",{\"1\":{\"73\":1,\"131\":1,\"194\":2}}],[\"middle\",{\"1\":{\"257\":1}}],[\"mistral\",{\"1\":{\"245\":1,\"258\":1,\"260\":2}}],[\"miscellaneous\",{\"1\":{\"1\":1}}],[\"million\",{\"1\":{\"227\":2}}],[\"microsoft\",{\"1\":{\"186\":1}}],[\"mic\",{\"1\":{\"179\":1}}],[\"mit\",{\"1\":{\"74\":2,\"79\":1,\"83\":1,\"137\":1}}],[\"miou\",{\"1\":{\"62\":1,\"70\":1,\"74\":1,\"80\":5,\"84\":1,\"138\":2,\"139\":1,\"141\":2,\"142\":3,\"179\":3,\"181\":3,\"182\":1,\"201\":1,\"202\":1}}],[\"mix\",{\"1\":{\"54\":1}}],[\"mixup\",{\"1\":{\"51\":1}}],[\"mixing\",{\"0\":{\"51\":1,\"53\":1},\"1\":{\"51\":3,\"53\":2,\"54\":2,\"56\":2}}],[\"mixed\",{\"0\":{\"44\":1,\"54\":1},\"1\":{\"53\":1,\"65\":1,\"175\":1}}],[\"mini\",{\"1\":{\"194\":2}}],[\"minigpt4\",{\"1\":{\"6\":2}}],[\"min\",{\"1\":{\"19\":1,\"28\":1,\"34\":1}}],[\"my\",{\"1\":{\"3\":1}}],[\"myfirstctf\",{\"1\":{\"2\":1}}],[\"mall\",{\"1\":{\"237\":1,\"239\":1}}],[\"matrix\",{\"1\":{\"216\":1}}],[\"matthieu\",{\"1\":{\"149\":1}}],[\"markov\",{\"1\":{\"249\":1}}],[\"markdown\",{\"1\":{\"1\":1}}],[\"marius\",{\"1\":{\"58\":1}}],[\"masked\",{\"1\":{\"221\":1}}],[\"masking\",{\"0\":{\"219\":1},\"1\":{\"219\":1}}],[\"mask\",{\"1\":{\"51\":3,\"56\":1,\"76\":1,\"221\":1}}],[\"majchrowska\",{\"1\":{\"50\":1}}],[\"map做出預測\",{\"1\":{\"74\":1}}],[\"map\",{\"1\":{\"49\":1,\"51\":3,\"74\":1,\"134\":1,\"174\":2}}],[\"maps\",{\"1\":{\"49\":2,\"74\":1}}],[\"mab\",{\"1\":{\"27\":2,\"41\":1}}],[\"maximum\",{\"1\":{\"226\":1}}],[\"maximizing\",{\"1\":{\"165\":1}}],[\"maxb∈a​q\",{\"1\":{\"153\":1}}],[\"max​q\",{\"1\":{\"93\":1}}],[\"maxc\",{\"1\":{\"73\":1}}],[\"max\",{\"1\":{\"19\":2}}],[\"mandarin\",{\"1\":{\"1\":1}}],[\"machine\",{\"1\":{\"1\":1,\"2\":1,\"3\":1}}],[\"mlp\",{\"1\":{\"137\":1}}],[\"ml\",{\"1\":{\"0\":1}}],[\"median\",{\"1\":{\"162\":1}}],[\"medium\",{\"1\":{\"46\":2,\"134\":1,\"155\":1}}],[\"meire\",{\"1\":{\"148\":1,\"162\":3,\"163\":1}}],[\"mean\",{\"1\":{\"38\":3,\"157\":2,\"162\":1}}],[\"methods\",{\"1\":{\"165\":1}}],[\"method\",{\"1\":{\"64\":1,\"258\":2}}],[\"methodology\",{\"0\":{\"24\":1,\"52\":1,\"72\":1,\"97\":1,\"110\":1,\"130\":1,\"156\":1,\"172\":1,\"189\":1,\"217\":1,\"248\":1}}],[\"meta\",{\"1\":{\"26\":5,\"27\":1,\"32\":1,\"37\":8,\"38\":2,\"40\":1}}],[\"memory\",{\"0\":{\"141\":1},\"1\":{\"6\":1,\"97\":2,\"155\":1,\"210\":1}}],[\"message\",{\"1\":{\"5\":3}}],[\"me\",{\"0\":{\"0\":1}}],[\"lknow​=−eκ\",{\"1\":{\"253\":1}}],[\"lknow​\",{\"1\":{\"253\":1}}],[\"lkd​=lces​\",{\"1\":{\"197\":1}}],[\"lkd​\",{\"1\":{\"197\":1,\"198\":1}}],[\"lklt​=kl\",{\"1\":{\"196\":1}}],[\"lklt​\",{\"1\":{\"196\":1}}],[\"lpatch​=−o1​\",{\"1\":{\"175\":1}}],[\"lpatch​\",{\"1\":{\"175\":1}}],[\"lpixel​=−c\",{\"1\":{\"174\":1}}],[\"lpixel​\",{\"1\":{\"174\":1}}],[\"lces​\",{\"1\":{\"198\":1}}],[\"lces​lcet​​=e\",{\"1\":{\"173\":1}}],[\"lcet​=−i=1∑h×w​k=1∑k​y^​t\",{\"1\":{\"191\":1}}],[\"lcet​=e\",{\"1\":{\"173\":1}}],[\"lcet​\",{\"1\":{\"173\":2}}],[\"lce​\",{\"1\":{\"134\":2}}],[\"lˉ\",{\"1\":{\"157\":1,\"159\":2}}],[\"lv\",{\"1\":{\"155\":1,\"160\":2}}],[\"lhrdat​=\",{\"1\":{\"134\":1}}],[\"lhrdas​=\",{\"1\":{\"134\":1}}],[\"lrate=dm−0\",{\"1\":{\"227\":1}}],[\"lregt​=−i=1∑h×w​j=1∑k​logpt\",{\"1\":{\"196\":1}}],[\"lregt​\",{\"1\":{\"196\":1}}],[\"lr\",{\"1\":{\"128\":2,\"132\":3,\"133\":5,\"134\":9,\"140\":1,\"142\":1,\"143\":1}}],[\"l1\",{\"1\":{\"113\":1}}],[\"l+1\",{\"1\":{\"112\":10}}],[\"l∈\",{\"1\":{\"112\":1}}],[\"l=ls​+lt​+λfd​lfd​\",{\"1\":{\"76\":1}}],[\"l=5\",{\"1\":{\"19\":1}}],[\"lfd\",{\"1\":{\"76\":1}}],[\"ltotoal​\",{\"1\":{\"198\":1}}],[\"ltotal​=lces​+lscet​+γ1​lklt​+γ2​lregt​\",{\"1\":{\"196\":1}}],[\"ltotal​=lces​+lcet​+αlpixel​+βlpatch​\",{\"1\":{\"176\":1}}],[\"ltotal​\",{\"1\":{\"196\":1}}],[\"lt=lce​\",{\"1\":{\"131\":1}}],[\"lt\",{\"1\":{\"73\":1,\"131\":1}}],[\"lstm\",{\"1\":{\"210\":1}}],[\"lscet​=αlce​\",{\"1\":{\"195\":1}}],[\"lslce​\",{\"1\":{\"131\":1}}],[\"ls​\",{\"1\":{\"76\":1}}],[\"ls\",{\"1\":{\"73\":1}}],[\"luc\",{\"1\":{\"69\":2,\"74\":5,\"80\":2,\"81\":1,\"82\":1,\"83\":2,\"84\":1,\"127\":1,\"128\":1,\"132\":1,\"133\":1,\"135\":1,\"138\":3,\"139\":2,\"140\":2,\"141\":1,\"142\":2}}],[\"lukas\",{\"1\":{\"69\":2,\"74\":5,\"80\":2,\"81\":1,\"82\":1,\"83\":2,\"84\":1,\"127\":1,\"128\":1,\"131\":1,\"132\":2,\"133\":1,\"135\":2,\"138\":3,\"139\":2,\"140\":2,\"141\":1,\"142\":2}}],[\"lee\",{\"1\":{\"215\":1,\"231\":1}}],[\"leemeng\",{\"1\":{\"214\":2,\"231\":1}}],[\"lebel\",{\"1\":{\"54\":1,\"195\":1}}],[\"level\",{\"0\":{\"31\":1},\"1\":{\"49\":3,\"74\":1,\"170\":3}}],[\"length\",{\"1\":{\"28\":1,\"30\":2,\"36\":1,\"226\":1,\"259\":1}}],[\"learnable\",{\"1\":{\"220\":1}}],[\"learned\",{\"1\":{\"142\":1}}],[\"learner\",{\"1\":{\"22\":1,\"32\":1}}],[\"learn\",{\"1\":{\"75\":1}}],[\"learning\",{\"0\":{\"77\":1,\"81\":1,\"89\":1,\"113\":1,\"168\":1,\"174\":1,\"175\":1,\"185\":1,\"196\":1},\"1\":{\"0\":2,\"1\":2,\"2\":1,\"3\":1,\"21\":2,\"41\":5,\"49\":1,\"50\":3,\"65\":5,\"70\":2,\"74\":2,\"77\":1,\"81\":1,\"91\":4,\"92\":3,\"95\":1,\"100\":1,\"102\":3,\"131\":1,\"139\":2,\"152\":1,\"165\":3,\"171\":1,\"172\":2,\"174\":1,\"187\":3,\"188\":2,\"198\":1,\"204\":3,\"216\":4,\"231\":1,\"257\":1,\"259\":2},\"2\":{\"43\":1,\"104\":1,\"167\":1,\"263\":1}}],[\"l\",{\"1\":{\"19\":1,\"21\":1,\"25\":1,\"26\":1,\"54\":1,\"112\":17,\"144\":1,\"152\":1,\"154\":1,\"155\":1,\"157\":2,\"159\":2}}],[\"limitation\",{\"1\":{\"261\":1}}],[\"li\",{\"1\":{\"231\":1}}],[\"li​\",{\"1\":{\"93\":1}}],[\"liang\",{\"1\":{\"47\":1,\"144\":1}}],[\"life\",{\"1\":{\"19\":2}}],[\"linear\",{\"1\":{\"77\":1,\"95\":1,\"101\":1,\"157\":1,\"220\":2,\"221\":1}}],[\"line\",{\"1\":{\"1\":1}}],[\"loc\",{\"1\":{\"257\":2}}],[\"local\",{\"1\":{\"49\":1,\"74\":1,\"245\":3,\"249\":1,\"252\":1,\"253\":3,\"254\":4,\"260\":4,\"261\":1}}],[\"looking\",{\"1\":{\"257\":1}}],[\"lora\",{\"1\":{\"253\":1,\"258\":1,\"259\":1}}],[\"low\",{\"1\":{\"128\":2,\"132\":2,\"133\":4,\"134\":1,\"140\":1}}],[\"logk​\",{\"1\":{\"226\":1}}],[\"log\",{\"1\":{\"27\":1,\"28\":1,\"29\":1}}],[\"losses2\",{\"1\":{\"198\":1}}],[\"losses\",{\"1\":{\"198\":1}}],[\"loss\",{\"0\":{\"21\":1,\"173\":1,\"195\":1},\"1\":{\"18\":1,\"21\":5,\"25\":4,\"54\":1,\"73\":3,\"76\":3,\"93\":1,\"128\":1,\"131\":3,\"134\":1,\"142\":2,\"151\":1,\"152\":1,\"154\":1,\"155\":3,\"157\":1,\"159\":1,\"163\":1,\"172\":1,\"173\":2,\"174\":1,\"175\":1,\"176\":1,\"191\":1,\"195\":1,\"196\":4,\"197\":1,\"253\":2}}],[\"long\",{\"1\":{\"16\":1,\"19\":2,\"23\":1,\"30\":4,\"36\":2,\"38\":1,\"210\":1}}],[\"llamafactory\",{\"1\":{\"259\":1}}],[\"llama\",{\"1\":{\"245\":1,\"258\":1}}],[\"llava\",{\"1\":{\"5\":2,\"6\":2}}],[\"llm\",{\"1\":{\"5\":1,\"6\":6,\"7\":1,\"10\":1,\"11\":2,\"230\":1,\"245\":5,\"247\":2,\"249\":3,\"251\":1,\"258\":2,\"261\":2},\"2\":{\"263\":1}}],[\"l4\",{\"1\":{\"5\":1}}],[\"lagent​\",{\"1\":{\"253\":2}}],[\"layers\",{\"1\":{\"116\":1}}],[\"layer\",{\"1\":{\"94\":2,\"112\":5,\"116\":1,\"119\":1,\"120\":1,\"157\":1,\"163\":2,\"220\":2,\"221\":4,\"226\":1,\"227\":1}}],[\"layout\",{\"1\":{\"49\":2}}],[\"labels\",{\"1\":{\"70\":1,\"73\":2,\"131\":1,\"134\":1,\"187\":1,\"188\":1,\"190\":1,\"191\":3,\"192\":2,\"195\":2}}],[\"labelled\",{\"1\":{\"53\":1}}],[\"labelling\",{\"0\":{\"50\":1},\"1\":{\"51\":1,\"53\":1,\"73\":1,\"187\":1}}],[\"labeling\",{\"1\":{\"50\":1}}],[\"labeled\",{\"1\":{\"50\":2}}],[\"label\",{\"0\":{\"135\":1,\"185\":1,\"192\":1},\"1\":{\"46\":2,\"50\":4,\"54\":1,\"73\":7,\"83\":1,\"91\":1,\"116\":1,\"131\":6,\"135\":1,\"170\":1,\"173\":9,\"187\":2,\"190\":1,\"192\":4,\"195\":1,\"196\":2,\"198\":1,\"203\":1,\"204\":1,\"227\":1}}],[\"languagues\",{\"1\":{\"1\":1}}],[\"languages\",{\"1\":{\"1\":1}}],[\"language\",{\"1\":{\"0\":1,\"210\":1,\"247\":3,\"257\":1}}],[\"large\",{\"1\":{\"0\":1,\"133\":2}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n}})=>{e==="suggest"?self.postMessage(st(t,v[s],n)):e==="search"?self.postMessage(et(t,v[s],n)):self.postMessage({suggestions:st(t,v[s],n),results:et(t,v[s],n)})};
//# sourceMappingURL=index.js.map
