const nt="ENTRIES",V="KEYS",T="VALUES",F="";class D{set;_type;_path;constructor(t,s){const n=t._tree,u=Array.from(n.keys());this.set=t,this._type=s,this._path=u.length>0?[{node:n,keys:u}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===F)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==F).join("")}value(){return E(this._path).node.get(F)}result(){switch(this._type){case T:return this.value();case V:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],ut=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const u=t.length+1,o=u+s,i=new Uint8Array(o*u).fill(s+1);for(let r=0;r<u;++r)i[r]=r;for(let r=1;r<o;++r)i[r*u]=r;return R(e,t,s,n,i,1,u,""),n},R=(e,t,s,n,u,o,i,r)=>{const d=o*i;t:for(const l of e.keys())if(l===F){const a=u[d-1];a<=s&&n.set(r,[e.get(l),a])}else{let a=o;for(let h=0;h<l.length;++h,++a){const m=l[h],p=i*a,f=p-i;let c=u[p];const g=Math.max(0,a-s-1),_=Math.min(i-1,a+s);for(let y=g;y<_;++y){const b=m!==t[y],z=u[f+y]+ +b,A=u[f+y+1]+1,w=u[p+y]+1,L=u[p+y+1]=Math.min(z,A,w);L<c&&(c=L)}if(c>s)continue t}R(e.get(l),t,s,n,u,a,i,r+l)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[u,o]=M(n);for(const i of u.keys())if(i!==F&&i.startsWith(o)){const r=new Map;return r.set(i.slice(o.length),u.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ot(this._tree,t)}entries(){return new D(this,nt)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return ut(this._tree,t,s)}get(t){const s=I(this._tree,t);return s!==void 0?s.get(F):void 0}has(t){const s=I(this._tree,t);return s!==void 0&&s.has(F)}keys(){return new D(this,V)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,O(this._tree,t).set(F,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);return n.set(F,s(n.get(F))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);let u=n.get(F);return u===void 0&&n.set(F,u=s()),u}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,u]of t)s.set(n,u);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==F&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},I=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==F&&t.startsWith(s))return I(e.get(s),t.slice(s.length))},O=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const o of e.keys())if(o!==F&&t[n]===o[0]){const i=Math.min(s-n,o.length);let r=1;for(;r<i&&t[n+r]===o[r];)++r;const d=e.get(o);if(r===o.length)e=d;else{const l=new Map;l.set(o.slice(r),d),e.set(t.slice(n,n+r),l),e.delete(o),e=l}n+=r;continue t}const u=new Map;return e.set(t.slice(n),u),u}return e},ot=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(F),s.size===0)W(n);else if(s.size===1){const[u,o]=s.entries().next().value;q(n,u,o)}}},W=e=>{if(e.length===0)return;const[t,s]=M(e);if(t.delete(s),t.size===0)W(e.slice(0,-1));else if(t.size===1){const[n,u]=t.entries().next().value;n!==F&&q(e.slice(0,-1),n,u)}},q=(e,t,s)=>{if(e.length===0)return;const[n,u]=M(e);n.set(u+t,s),n.delete(u)},M=e=>e[e.length-1],it=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},rt=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,S="or",$="and",ct="and_not",lt=(e,t)=>{e.includes(t)||e.push(t)},P=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},N=({score:e},{score:t})=>t-e,ht=()=>new Map,k=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,dt={[S]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:u,terms:o,match:i}=t.get(s);n.score=n.score+u,n.match=Object.assign(n.match,i),P(n.terms,o)}}return e},[$]:(e,t)=>{const s=new Map;for(const n of t.keys()){const u=e.get(n);if(u==null)continue;const{score:o,terms:i,match:r}=t.get(n);P(u.terms,i),s.set(n,{score:u.score+o,terms:u.terms,match:Object.assign(u.match,r)})}return s},[ct]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},at=(e,t,s,n,u,o)=>{const{k:i,b:r,d}=o;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/u)))},ft=e=>(t,s,n)=>{const u=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,o=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:u,prefix:o}},H=(e,t,s,n)=>{for(const u of Object.keys(e._fieldIds))if(e._fieldIds[u]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${u}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},gt=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const u=e._index.fetch(n,ht),o=u.get(t);o==null||o.get(s)==null?H(e,s,t,n):o.get(s)<=1?o.size<=1?u.delete(t):o.delete(s):o.set(s,o.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},mt={k:1.2,b:.7,d:.5},pt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(rt),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:S,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:mt},Ft={combineWith:$,prefix:(e,t,s)=>t===s.length-1},_t={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},yt={..._t,...U},Y=(e,t=S)=>{if(e.length===0)return new Map;const s=t.toLowerCase();return e.reduce(dt[s])||new Map},B=(e,t,s,n,u,o,i,r,d=new Map)=>{if(u==null)return d;for(const l of Object.keys(o)){const a=o[l],h=e._fieldIds[l],m=u.get(h);if(m==null)continue;let p=m.size;const f=e._avgFieldLength[h];for(const c of m.keys()){if(!e._documentIds.has(c)){gt(e,h,c,s),p-=1;continue}const g=i?i(e._documentIds.get(c),s,e._storedFields.get(c)):1;if(!g)continue;const _=m.get(c),y=e._fieldLength.get(c)[h],b=at(_,p,e._documentCount,y,f,r),z=n*a*g*b,A=d.get(c);if(A){A.score+=z,lt(A.terms,t);const w=G(A.match,s);w?w.push(l):A.match[s]=[l]}else d.set(c,{score:z,terms:[t],match:{[s]:[l]}})}}return d},At=(e,t,s)=>{const n={...e._options.searchOptions,...s},u=(n.fields||e._options.fields).reduce((c,g)=>({...c,[g]:G(n.boost,g)||1}),{}),{boostDocument:o,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:l,prefix:a}={...J.weights,...i},h=e._index.get(t.term),m=B(e,t.term,t.term,1,h,u,o,d);let p,f;if(t.prefix&&(p=e._index.atPrefix(t.term)),t.fuzzy){const c=t.fuzzy===!0?.2:t.fuzzy,g=c<1?Math.min(r,Math.round(t.term.length*c)):c;g&&(f=e._index.fuzzyGet(t.term,g))}if(p)for(const[c,g]of p){const _=c.length-t.term.length;if(!_)continue;f?.delete(c);const y=a*c.length/(c.length+.3*_);B(e,t.term,c,y,g,u,o,d,m)}if(f)for(const c of f.keys()){const[g,_]=f.get(c);if(!_)continue;const y=l*c.length/(c.length+_);B(e,t.term,c,y,g,u,o,d,m)}return m},X=(e,t,s={})=>{if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(m=>X(e,m,a));return Y(h,a.combineWith)}const{tokenize:n,processTerm:u,searchOptions:o}=e._options,i={tokenize:n,processTerm:u,...o,...s},{tokenize:r,processTerm:d}=i,l=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(ft(i)).map(a=>At(e,a,i));return Y(l,i.combineWith)},K=(e,t,s={})=>{const n=X(e,t,s),u=[];for(const[o,{score:i,terms:r,match:d}]of n){const l=r.length,a={id:e._documentIds.get(o),score:i*l,terms:Object.keys(d),match:d};Object.assign(a,e._storedFields.get(o)),(s.filter==null||s.filter(a))&&u.push(a)}return u.sort(N),u},Ct=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:o,terms:i}of K(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=o,d.count+=1):n.set(r,{score:o,terms:i,count:1})}const u=[];for(const[o,{score:i,terms:r,count:d}]of n)u.push({suggestion:o,terms:r,score:i/d});return u.sort(N),u};class Et{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?yt:t.autoVacuum;this._options={...pt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...Ft,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const u={};for(const[o,i]of n)u[o]=Object.fromEntries(i);t.push([s,u])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:u,fieldLength:o,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:l},a)=>{if(l!==1&&l!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new Et(a);h._documentCount=t,h._nextId=s,h._documentIds=k(n),h._idToShortId=new Map,h._fieldIds=u,h._fieldLength=k(o),h._avgFieldLength=i,h._storedFields=k(r),h._dirtCount=d||0,h._index=new C;for(const[m,p]of h._documentIds)h._idToShortId.set(p,m);for(const[m,p]of e){const f=new Map;for(const c of Object.keys(p)){let g=p[c];l===1&&(g=g.ds),f.set(parseInt(c,10),k(g))}h._index.set(m,f)}return h},Q=Object.entries,wt=Object.fromEntries,j=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),u=[];let o=0,i=0;const r=(l,a=!1)=>{let h="";i===0?h=l.length>20?`… ${l.slice(-20)}`:l:a?h=l.length+i>100?`${l.slice(0,100-i)}… `:l:h=l.length>20?`${l.slice(0,20)} … ${l.slice(-20)}`:l,h&&u.push(h),i+=h.length,a||(u.push(["mark",t]),i+=t.length,i>=100&&u.push(" …"))};let d=s.indexOf(n,o);if(d===-1)return null;for(;d>=0;){const l=d+n.length;if(r(e.slice(o,d)),o=l,i>100)break;d=s.indexOf(n,o)}return i<100&&r(e.slice(o),!0),u},Z=/[\u4e00-\u9fa5]/g,tt=(e={})=>({fuzzy:.2,prefix:!0,processTerm:t=>{const s=t.match(Z)||[],n=t.replace(Z,"").toLowerCase();return n?[n,...s]:[...s]},...e}),xt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),kt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),et=(e,t,s={})=>{const n={};return K(t,e,tt({boost:{h:2,t:1,c:4},...s})).forEach(u=>{const{id:o,terms:i,score:r}=u,d=o.includes("@"),l=o.includes("#"),[a,h]=o.split(/[#@]/),m=i.sort((f,c)=>f.length-c.length).filter((f,c)=>i.slice(c+1).every(g=>!g.includes(f))),{contents:p}=n[a]??={title:"",contents:[]};if(d)p.push([{type:"customField",key:a,index:h,display:m.map(f=>u.c.map(c=>j(c,f))).flat().filter(f=>f!==null)},r]);else{const f=m.map(c=>j(u.h,c)).filter(c=>c!==null);if(f.length&&p.push([{type:l?"heading":"title",key:a,...l&&{anchor:h},display:f},r]),"t"in u)for(const c of u.t){const g=m.map(_=>j(c,_)).filter(_=>_!==null);g.length&&p.push([{type:"text",key:a,...l&&{anchor:h},display:g},r])}}}),Q(n).sort(([,u],[,o])=>"max"==="total"?xt(u,o):kt(u,o)).map(([u,{title:o,contents:i}])=>{if(!o){const r=it(t,u);r&&(o=r.h)}return{title:o,contents:i.map(([r])=>r)}})},st=(e,t,s={})=>Ct(t,e,tt(s)).map(({suggestion:n})=>n),v=wt(Q(JSON.parse("{\"/\":{\"documentCount\":60,\"nextId\":60,\"documentIds\":{\"0\":\"v-184f4da6\",\"1\":\"v-184f4da6#skills\",\"2\":\"v-184f4da6#競賽成績\",\"3\":\"v-184f4da6#活動參與\",\"4\":\"v-620a6165\",\"5\":\"v-620a6165#比賽題目\",\"6\":\"v-620a6165#比賽過程\",\"7\":\"v-620a6165#datasets-處理\",\"8\":\"v-620a6165#答案產出\",\"9\":\"v-620a6165#結果\",\"10\":\"v-620a6165#報告\",\"11\":\"v-620a6165#總結\",\"12\":\"v-620a6165@0\",\"13\":\"v-620a6165@1\",\"14\":\"v-c0336012\",\"15\":\"v-c0336012#basic-information\",\"16\":\"v-c0336012#what-is-domain-adaption\",\"17\":\"v-c0336012#問題描述\",\"18\":\"v-c0336012#related-works\",\"19\":\"v-c0336012#domain-alignment\",\"20\":\"v-c0336012#pseudo-labelling-or-self-training\",\"21\":\"v-c0336012#mixing\",\"22\":\"v-c0336012#methodology\",\"23\":\"v-c0336012#naive-mixing-to-uda\",\"24\":\"v-c0336012#domain-adaption-via-corss-domain-mixed-sampling-dacs\",\"25\":\"v-c0336012#results\",\"26\":\"v-c0336012#實驗設定\",\"27\":\"v-c0336012#dataset\",\"28\":\"v-c0336012#cityscapes\",\"29\":\"v-c0336012#gta5\",\"30\":\"v-c0336012#synthia\",\"31\":\"v-c0336012#gta5-cityscapes\",\"32\":\"v-c0336012#synthia-cityscapes\",\"33\":\"v-c0336012#some-issues-about-evaluation\",\"34\":\"v-c0336012#contribution\",\"35\":\"v-c0336012#值得一看的文章們\",\"36\":\"v-c0336012@0\",\"37\":\"v-c0336012@1\",\"38\":\"v-5b18c8c4\",\"39\":\"v-5b18c8c4#basic-information\",\"40\":\"v-5b18c8c4#問題描述\",\"41\":\"v-5b18c8c4#related-works\",\"42\":\"v-5b18c8c4#parameter-space-noise-for-exploration\",\"43\":\"v-5b18c8c4#dqn\",\"44\":\"v-5b18c8c4#double-dqn\",\"45\":\"v-5b18c8c4#dueling-dqn\",\"46\":\"v-5b18c8c4#a3c\",\"47\":\"v-5b18c8c4#methodology\",\"48\":\"v-5b18c8c4#基本想法\",\"49\":\"v-5b18c8c4#減少產-random-number-時間\",\"50\":\"v-5b18c8c4#dqn-dueling-dqn\",\"51\":\"v-5b18c8c4#distributed-a3c\",\"52\":\"v-5b18c8c4#results\",\"53\":\"v-5b18c8c4#experiments\",\"54\":\"v-5b18c8c4#analysis\",\"55\":\"v-5b18c8c4#contribution\",\"56\":\"v-5b18c8c4#值得一看的文章們\",\"57\":\"v-5b18c8c4@0\",\"58\":\"v-5b18c8c4@1\",\"59\":\"v-e1e3da16\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[2,29],\"1\":[1,35],\"2\":[1,80],\"3\":[1,56],\"4\":[4,15],\"5\":[1,42],\"6\":[1,86],\"7\":[2,35],\"8\":[1,13],\"9\":[1,13],\"10\":[1,20],\"11\":[1,24],\"12\":[null,null,1],\"13\":[null,null,2],\"14\":[8],\"15\":[2,19],\"16\":[4,49],\"17\":[1,49],\"18\":[2],\"19\":[2,77],\"20\":[6,108],\"21\":[1,66],\"22\":[1],\"23\":[4,54],\"24\":[9,67],\"25\":[1],\"26\":[1,37],\"27\":[1,15],\"28\":[1,13],\"29\":[1,19],\"30\":[1,28],\"31\":[3,48],\"32\":[3,39],\"33\":[4,52],\"34\":[1,19],\"35\":[1,52],\"36\":[null,null,1],\"37\":[null,null,7],\"38\":[4],\"39\":[2,13],\"40\":[1,40],\"41\":[2,19],\"42\":[5,78],\"43\":[1,70],\"44\":[2,43],\"45\":[2,92],\"46\":[1,129],\"47\":[1],\"48\":[1,86],\"49\":[4,48],\"50\":[2,55],\"51\":[2,71],\"52\":[1],\"53\":[1,73],\"54\":[1,61],\"55\":[1,12],\"56\":[1,49],\"57\":[null,null,1],\"58\":[null,null,5],\"59\":[1]},\"averageFieldLength\":[2.0378032272038125,46.53903263258248,0.5012106537530266],\"storedFields\":{\"0\":{\"h\":\"About Me\",\"t\":[\"本名林禾堃，一個喜愛資訊領域的人。目前就讀於清華大學資訊工程學系，過去曾擔任臺南一中資訊社社長，也是 SCIST 的共同創辦人之一。\",\"高中接觸了演算法、資訊安全、網路管理等領域，目前正在朝向 Deep Learning 領域發展，關注的主題包含 Computer Vision、Reinforcement Learning 以及 Large Language Model。\",\"希望透過這個 blog 紀錄學習的點滴，也歡迎一起來討論 ML 領域的各種知識！\"]},\"1\":{\"h\":\"Skills\",\"t\":[\"Programming Languagues\",\"C/C++, Python, JavaScripts\",\"Frameworks\",\"React, Hexo, LINE BOT, PyTorch\",\"Machine Learning\",\"Computer Vision, Reinforcement Learning\",\"Miscellaneous\",\"UNIX Programming, Cryptography, Reverse engineering, Git, Markdown, Vim\",\"Languages\",\"Mandarin (Native), English (TOEFL 81), Japanese (JLPT N1)\"]},\"2\":{\"h\":\"競賽成績\",\"t\":[\"日期\",\"競賽名稱\",\"成績\",\"心得\",\"2024年01月\",\"2024 TSMC CareerHack\",\"無\",\"心得\",\"2022年10月\",\"梅竹黑客松\",\"分組第三名\",\"2022年07月\",\"YTP Final Project\",\"專題第三名\",\"2020年07月\",\"青年黑克松\",\"網頁組第3名\",\"2020年06月\",\"AIS3 pre-exam\",\"第96名\",\"2020年06月\",\"MyFirstCTF\",\"第22名\",\"2020年04月\",\"Google Code Jam Qualification Round\",\"第29755名(30/100分)\",\"2020年03月\",\"Kick Start Round A 2020\",\"第5695名(21/100分)\",\"2020年03月\",\"TOI初選\",\"第62名(212/500分)\",\"2020年02月\",\"TOI校內賽\",\"第7名(314/600分)\",\"2020年01月\",\"TOI海選\",\"300/300分\",\"2019年12月\",\"NPSC決賽\",\"全國第六名\",\"2019年11月\",\"金盾獎\",\"進入決賽\",\"2019年10月\",\"Kick Start Round G 2019\",\"第2427名(10/100分)\",\"2019年07月\",\"FML-based Machine Learning Competition for Human\",\"世界第三名\",\"2019年06月\",\"第七屆高一生程式設計排名賽\",\"128/800\"]},\"3\":{\"h\":\"活動參與\",\"t\":[\"日期\",\"活動\",\"心得/筆記\",\"2023年04月-\",\"機器學習讀書會 成員\",\"2022年10月-\",\"日語學習小組 成員\",\"2023年09月\",\"NTHU CS Camp 講師\",\"2022年07月\",\"IONCamp 營長\",\"2022年\",\"資訊之芽 C 班講師\",\"2022年08月\",\"索拉教育 Python 課程講師\",\"2022年07月\",\"索拉教育 C++ 課程講師\",\"2020年09月\",\"AIS3 CLUB\",\"2020年08月\",\"奧義科技參訪\",\"2020年08月\",\"SITCON 2020\",\"2020年07月\",\"AIS3 2020\",\"2020年07月\",\"2020 ISIP SummerCamp\",\"筆記\",\"2020年-2021年\",\"SCIST\",\"2020年01月\",\"IOICamp\",\"2019年11月\",\"臺灣好厲駭 Machine Learning & Security\",\"筆記\",\"2019年08月\",\"HITCON 2019\",\"2019年08月\",\"HITCON HackDoor\",\"2019年07月\",\"2019 My FirstSecurity Summer Camp\",\"2019年07月\",\"第八屆成功大學大學生活體驗營\",\"2019年03月\",\"SITCON 2019\"]},\"4\":{\"h\":\"2024 TSMC CareerHack 心得\",\"t\":[\"前幾天去參加了 2024 台積電的黑客松，大概是人生第一次走進台積辦公室。\",\"這場比賽是一組四人的比賽，前面有一個預賽，需要解出一些簡單的演算法題目。每個人題目會不太相同，但基本上都不會太難，簡單的 Sort、Greedy、Graph、DP。\"]},\"5\":{\"h\":\"比賽題目\",\"t\":[\"我們這一組拿到的是 AI 看圖說故事 的題目，基本上就是會有一些工地的照片，希望我們可以去找到\",\"照片中有多少人\",\"有多少人有戴安全帽\",\"有多少人沒戴安全帽\",\"安全帽是甚麼顏色\",\"有些圖片上面會有 warning message，所以會有額外的提問\",\"warning message 寫了什麼\",\"有沒有任何人違反了 warning message 的敘述\",\"基本上他們期待我們會運用 LLM 去解決這個問題，TSMC 也有先提供了一些資源\",\"GCP 運算及儲存資源 \",\"L4 GPU (24 GB RAM) x2\",\"100 GB CPU RAM\",\"100 GB Disk\",\"100 GB Bucket storage\",\"LLaVA pretrained model\",\"LLaVA finetune script\",\"一些 Datasets\"]},\"6\":{\"h\":\"比賽過程\",\"t\":[\"比賽會在正式開賽前一周公布題目，也開放資源，因此不少組別在實際進到 HackDay 前就已經做了不少，也有聽說有部分的組別 HackDay 就是拿來做簡報，也是頗有趣。\",\"雖然說是黑客松，不過場地因為是辦公室，所以晚上 6 點就要回家，隔天早上 9 點半再來報到，實際上在辦公室的時間沒有想像中的還要多。\",\"一開始進去到辦公室的感覺就很舒服，可以自己使用的免費咖啡機、電動的升降桌、超級舒服的人體工學椅、超大的雙螢幕。\",\"只能說在設備上直接贏了。\",\"中間還有提供午餐、點心、飲料，都相當地好吃，覺得很開心。\",\"我們這一組在 HackDay 之前的想法是先去做一些 paper research，去調查看看有哪些其他還不錯的 LLM 可以嘗試。\",\"雖然說在比賽之前我們的想法是，如果只會問那些固定的問題的話，那我們不要用 LLM，用其他 CV 的 model 去解決也許會比 LLM 還要強許多，不過寄信去詢問之後得到希望還是使用到 LLM 的回覆，所以我們後續的方向都著重在 LLM 以及 Fine-tune 的研究。\",\"大致上大家看過了幾個 LLM\",\"miniGPT4、miniGPT4_v2\",\"BLIP、InstructBLIP\",\"Flamingo\",\"Fine-tune 的部分主要是參考各個 paper 自己的 fine-tune 說明，其他的大概就是 proxy-tuning。\",\"其實讀 paper 都覺得很好理解，也想說難得有不錯的運算資源，是也可以都 train 看看結果如何。不過理想很美好，現實很骨感。\",\"實際上我們挑了最小的 13B 模型，丟進去訓練還是出現 CUDA out of memory，估計也是沒救，最後只有 LLaVA 活下來，所以我們後續就主要專攻 LLaVA。\"]},\"7\":{\"h\":\"Datasets 處理\",\"t\":[\"我們在 Dataset 處理上花了不少的時間。\",\"在提供的 5 份 datasets 當中，我們發現其中兩份都是教室的監視器錄影畫面，我們想說這裡根本沒人戴安全帽，超級懷疑這個 dataset 的實用程度。\",\"此外，其他的 dataset 也是有些包含浮水印，或是看起來安全帽是後製貼上去的，彩度跟亮度跟環境有些落差。\",\"我們也發現到說在回答顏色的那一題，模型會傾向回答 None，但實際上有顏色才對，所以在前面加上了一些 prefix string，試圖讓 LLM 吐出更多結果。\",\"實驗上為了檢測拿掉兩個 datasets 以及加上 prefix string 是否有比較好，交叉做了一些 fintune，也寫了一個評分程式去評估好壞。\"]},\"8\":{\"h\":\"答案產出\",\"t\":[\"我們最後決定要把多個模型的輸出拿去做類似 Bagging 的操作。\",\"簡單來說，這七個問題，我們相信那些回答分數比較高的模型可以做得比較好，所以就讓他專門來回答這個問題。\",\"如此一來就可以得到完整的輸出結果，也可以盡可能至少在 validation set 上看起來很棒 ouo\"]},\"9\":{\"h\":\"結果\",\"t\":[\"最後在 private set 上的分數大概是 59.7\",\"加權分數: 44.5/70\",\"Bonus: 2.7/5\",\"其實不太好啦XD\"]},\"10\":{\"h\":\"報告\",\"t\":[\"其實 TSMC 的大家都很友善，報告期間也都會跟我們分享他們覺得在每個地方有哪些比較好的做法也許可以嘗試看看。\",\"此外，其他組的報告我們也可以透過實時的直播去看，認識到其他組都用了怎樣的方法去解決。\",\"印象中有人用了 YOLO，也有人套了 OCR，把輸出結果套入 LLM 的輸入。有些組別完成度很高，甚至連 FrontEnd 都完成了，相當佩服。\"]},\"11\":{\"h\":\"總結\",\"t\":[\"我覺得這次到 TSMC 的比賽經驗很不錯，也讓我認識到 TSMC 的 IT team，跟過去自己想像當中在無塵室裏面處理晶圓的那種印象是完全不同，我也能感受到每個員工對我們都很友善。\",\"當時有遇到問題去找他們的時候都可以得到即時的 feedback，也能感受到他們對我們的提問的重視，是一個讓人很喜歡的環境。\",\"這次大概是第一次實際碰 LLM，過去基本上就是看過 paper，讀的時候都覺得嗯嗯嗯很有道理，不過實際上在實作的時候光是硬體的資源可能就是一大障礙，也就很難往下一步去發展。\",\"但總結來說這次有還蠻有趣的體驗，感到很開心，也很期待未來也還有機會可以繼續了解和開發 LLM。\"]},\"12\":{\"c\":[\"Feedbacks\"]},\"13\":{\"c\":[\"TSMC\",\"CareerHack\"]},\"14\":{\"h\":\"DACS: Domain Adaptation via Cross-domain Mixed Sampling\"},\"15\":{\"h\":\"Basic Information\",\"t\":[\"2020 Release\",\"2021 WACV(Winter Conference on Applications of Computer Vision)\",\"Chalmers University of Technology(查爾摩斯理工大學)與 Volvo Cars 共同發表\"]},\"16\":{\"h\":\"What is Domain Adaption\",\"t\":[\"Image from Medium\",\"所謂的 Domain 就是用來描述一群資料他們的分布狀況。\",\"Domain Adaption 的目標是把兩個不同分佈的 Domain (Source Domain 以及 Target Domain) 投射到同一個平面上，使得同類型的資料會相近，反之則相遠。\",\"舉一個在 CV 上的例子。如果我們想要訓練一個模型去做自駕車的街景物件偵測，很多時候我們並不會直接去蒐集真實的資料，像是直接有一台車會去蒐集真實街景資料，這樣所需要的成本會過大。時常我們會訓練在合成資料上(synethic data)，然後再應用在真實的世界當中。\",\"Image from Medium\",\"不過這種情況下一個直覺的問題是，在 虛擬世界(Source Domain) 上也許我們能夠對各種物件去做標記 label，但是對於真實世界(Target Domain)往往會有許多我們沒有的 label、環境與虛擬世界有差距，這種差距被描述為 Domain Shift。當兩個 Domain 相差過大，Domain Shift 過高，就會導致單純在 Source Domain 上訓練的模型難以直接 apply 到 Target Domain 上。\",\"因此，Domain Adaption 想解決的就是盡可能地將 Domain Shift 降低，讓我們得以用較低的成本在虛擬環境中訓練模型，然後應用在真實的環境當中。\"]},\"17\":{\"h\":\"問題描述\",\"t\":[\"近年來透過 CNN 處理 semantic segmentation(影像分割) 的模型雖然有許多，也獲得不錯的成果，不過如果遇到新的 domain，往往就會 work 不太好，尤其是從 synethic data 轉變到 real data 上的時候。\",\"問題在於不同的 domain，各自的 domain distribution 會不同。只訓練在 source domain 的模型對於 target domain 的狀況缺乏認知，導致預測失準。\",\"Info\",\"這就像是同理心，因為缺乏對他人的理解，擅自用自己的思維解讀，就會導致互相的不理解。\",\"Image from Liang-ChiehChen et al. (2015)\",\"可以發現單純用 CNN 就可以得到相當好的影像分割結果。\",\"Image from Yiheng Zhang et al. (2018)\",\"直接把訓練在虛擬環境的模型應用在真實環境，結果相當糟糕。\"]},\"18\":{\"h\":\"Related Works\"},\"19\":{\"h\":\"Domain Alignment\",\"t\":[\"透過 adversarial learning (對抗式學習) 去拉近 source domain 以及 target domain。\",\"我們可以想成現在 Segmentation Network 就是 GAN 的 Generator，然後會有一個 Discriminator 去判別現在給我的究竟是 source domain 還是 target domain 的預測結果。\",\"Image from Yi-Hsuan Tsai et al. (2018)\",\"兩個 Domain 中各取圖片，經過相同的 Segmentation Network，將產出的 semantic maps 做對抗式學習\",\"Info\",\"依照 alignment 的不同，可以分成 pixel level, feature map level, semantic level 等不同的做法。\",\"這樣的做法之所以可行，是源自於即便 domain 不同，在 semantic maps 上的 spatial layout 以及 local context 通常並不會差太多。\",\"DACS 的做法之所以能夠成功，也有部分是源自於這樣的相似性帶來的好處。\",\"Tips\",\"同樣以自駕車的例子來說，即便 synethic data 和 real data 的 domain 有相當大的差異，不過像是馬路、汽車、行人都還是會跟地板黏在一起，其他像是路燈、號誌、天空之類的就通常會像是在半空中。這類的 spatial layout 就相當地雷同。\",\"Image from Yi-Hsuan Tsai et al. (2018)\"]},\"20\":{\"h\":\"pseudo labelling (or self-training)\",\"t\":[\"Adversarial Learning for Semi-Supervised Semantic Segmentation\",\"最初是為了解決 半監督式學習(Semi-Supervise Learning, SSL) 而被提出的。\",\"Info\",\"所謂的半監督式學習也就是說 target domain 的資料上只有一些 labeled data，其他絕大多都是 unlabeled data，這種狀況下訓練模型就被稱為半監督式學習。\",\"而半監督式學習困難的點在於雖然對於 Target Domain 有部分的認知，但是並不全面。\",\"一個簡單的方法是想辦法給這些 unlabeled data 一些 pseudo label。那我們就可以用 supervise learning 的方法解決了。\",\"舉例來說，先在 labeled data 上訓練一個模型，透過這個模型我們就有辦法給 unlabeled data 做 prediction，而 prediction 的結果就當作是他的 pseudo label，就可以再拿去 fine-tune model 了。\",\"Image from Sylwia Majchrowska et al. (2021)\",\"但主要的問題來自於 Domain Shift，畢竟 Source Domain 和 Target Domain 還是存在差異的，並不是所有的 Target Data 都能夠透過 Source Data 去轉移出來。\",\"尤其在 Unsupervised Domain Adaption(UDA) 來說是相當大的問題，在 UDA 當中通常 Domain Shift 都會特別大。\",\"Info\",\"所謂的 UDA 也就是說我們對於 Target Domain 的資料不存在任何 label。換句話說，我們對於 Target Domain 缺乏 label 上的認知。\",\"對於 UDA 來說由於缺乏對於 Target Domain 的認識，一個常見的問題是產出的結果通常會傾向去預測結果為常見的 class。\",\"Info\",\"對陌生人的認識，往往先從貼標籤開始。\",\"例如說在自駕車的道路辨識當中鄰近人行道這種時常出現的 class，如果出現道路或甚至機車，有可能就被誤判成人行道。或是汽車比卡車更常見，導致卡車時常被預測成汽車。\",\"Image from Yang Zou et al (2018)\",\"看 column 4，只有 pseudo labeling 的例子\",\"Info\",\"雖然已經有 paper 提出如 CBST 的方法來降低這種問題，但在邊界上往往還是難以有好的結果。\"]},\"21\":{\"h\":\"Mixing\",\"t\":[\"Mixing 基本上就是從 training image 拿出兩張，透過一些方式混在一起，產生一個新的 training image。最初被用於把 unlabeled image 混合成新的圖片，是一種 data augumentation 的技巧。\",\"像是 Mixup 這種 data augumentation 方法也是屬於 Mixing 的一種。\",\"DACS 當中使用的是 ClassMix 這種 Mixing 方法。\",\"具體來說，ClassMix 的步驟\",\"把兩個圖片 (A,B) 先轉成 semantic map (SA​,SB​)\",\"把 SA​ 其中一半的 classes 對應的 semantic map 做出一個 binary mask (M)\",\"把 mask M apply 在 A 上，跟 B 合成出 XA​。\",\"把 mask M apply 在 SA​ 上，跟 SB​ 合成出 XA​ 對應的 semantic map YA​\",\"Image from Viktor Olsson et al. (2020)\",\"這樣的做法有趣的是能夠將 semantic segmentation 在邊界上往往會出現誤差的問題解決。\",\"邊界上的判斷會因為圖片跟相鄰環境的相似導致模糊不清。但透過剪貼則可以造成不同環境的突兀感，進而解決這個問題。因此這時候 pseudo labelling 就能夠比較好發揮作用。\",\"Image from Viktor Olsson et al. (2020)\"]},\"22\":{\"h\":\"Methodology\"},\"23\":{\"h\":\"Naive Mixing to UDA\",\"t\":[\"最 Naive 的做法就是照著 ClassMix 的方法，將 unlebelled dataset Mixing 成新的 dataset，把 labelled dataset 以及 mixed dataset 拿去訓練。\",\"Info\",\"在 UDA 當中，unlabelled dataset 就是 target domain dataset。\",\"Image from Wilhelm Tranheden at al.\",\"但是這種做法實際上效果很糟糕。像是 sidewalk 被預測成 road，rider 被預測成 person 之類的，許多的 class 都被其他 class 覆蓋。這樣的問題只在 target domain 上會發生，這跟前面提到只使用 pseudo labelling to UDA 會造成的問題是吻合的。\",\"Image from Wilhelm Tranheden at al.\",\"單純的 Naive Mixing 往往在邊界上會有許多誤判的 class\",\"Tips\",\"這種相似的 class 相鄰而導致的誤判被稱為 class conflation\"]},\"24\":{\"h\":\"Domain Adaption via Corss-domain mixed Sampling (DACS)\",\"t\":[\"DACS 的核心做法是不單只是跟 Target Domain 去 mixing，而是將 Source 跟 Target 一起 Mix。如此一來， Target Domain 以及 Source Domain 的關聯性就能被連結起來，降低 Domain Shift。\",\"Image from Wilhelm Tranheden at al.\",\"詳細的步驟具體來說\",\"從 Source Domain (DS​) 取出圖片與 lebel (XS​,YS​)\",\"從 Target Domain (DT​) 取出圖片 XT​\",\"透過 segmentation network fθ​ 取得 XT​ 的 pseudo label YT​^​\",\"將 (XS​,YS​),(XT​,YT​^​) 經過 ClassMix 得到 (XM​,YM​)\",\"把 (XS​,YS​),(XM​,YM​) 拿去訓練。\",\"Image from Wilhelm Tranheden at al.\",\"在 Loss 的設計上也相當直覺，就是希望 XS​ 的預測結果要接近 YS​，XM​ 的結果要接近 YM​。\",\"H: Cross-Entropy\",\"λ: 調整 Mixing 部分的影響程度\",\"L(θ)=E[H(fθ​(XS​),YS​)+λH(fθ​(XM​),YM​)]\"]},\"25\":{\"h\":\"Results\"},\"26\":{\"h\":\"實驗設定\",\"t\":[\"在 segmentation network 的設定上參考了許多過去的研究，選擇採用 DeepLab v2 搭配 ResNet101 作為 backbone。\",\"ResNet101 是 pretrained on ImageNet 跟 MSCOCO。而 Hyperparameter 的設定基本上跟 Yi-Hsuan Tsai et al. (2018) 一樣。\",\"在 Mixing 的方法上雖然任何 based on binary mask 的 Mixing 都可以使用，不過這裡最主要都是使用 ClassMix。\"]},\"27\":{\"h\":\"Dataset\",\"t\":[\"在 synthetic-to-real 有一些常見的 benchmarks。\",\"GTA5 -> Cityscapes\",\"SYNTHIA -> Cityscapes\",\"GTA5 以及 SYNTHIA 都是虛擬世界當中的影像，而 Cityscapes 則是現實世界當中的影像。\"]},\"28\":{\"h\":\"Cityscapes\",\"t\":[\"照片是在城市當中開車拍下的各種照片\",\"Image from Marius Cordts et al. (2016)\",\"2975 training images\",\"19 classes\"]},\"29\":{\"h\":\"GTA5\",\"t\":[\"照片是在 GTA5 下拍攝的\",\"Image from Stephan R. Richter et al.\",\"24966 synthetic training images\",\"19 classes \",\"可對應到 Cityscapes 的 classes\"]},\"30\":{\"h\":\"SYNTHIA\",\"t\":[\"照片是在 Unity 建構的 virtual city 下拍攝\",\"Image from GermanRos et al. (2016)\",\"9400 synthetic training images\",\"16(or 13) classes \",\"都會對到 Cityscapes 的 classes\",\"13 個 classes 的版本是少了 Wall, Fence, Pole\"]},\"31\":{\"h\":\"GTA5 -> Cityscapes\",\"t\":[\"Image from Wilhelm Tranheden at al.\",\"其他的 Model 都是 DeepLab-v2，他們選擇其中 Performance 最好的，但 Backbone 並不一定要是 ResNet 101\",\"Image from Wilhelm Tranheden at al.\",\"Source 是只有使用 source domain 去 train 的模型\",\"Info\",\"可以發現\",\"單純用 source domain 去訓練顯然很糟糕，只對簡單的 class 像是 Road, Build, Veg, Sky, Person, Car 這些普遍做得不錯的 class 有還不錯的 Performance\",\"DACS 在絕大多數並非是最佳的結果上都不會離最佳太遠，除了 SW 有點偏以及 Train 真的很糟\"]},\"32\":{\"h\":\"SYNTHIA -> Cityscapes\",\"t\":[\"考慮到 SYNTHIA 有些 paper 使用 16 個 classes，有些是 13 個 class 的版本，所以在數據上 mIoU 有兩列分別表示 13 個平均跟 16 個的平均。\",\"Image from Wilhelm Tranheden at al.\",\"Info\",\"可以發現\",\"單純用 source domain 去訓練顯然很糟糕，甚至對 Road 的 Performance 都不太好\",\"DACS 在絕大多數並非是最佳的結果上都不會離最佳太遠，除了 SW 頗偏\"]},\"33\":{\"h\":\"Some issues about evaluation\",\"t\":[\"他們認為在其他的 paper 有不少人最後給的結果之所以那麼好看是因為\",\"Cityscapes 並沒有 testset\",\"他們選擇用 validation set 判斷要不要 early stop，這個 validation set 也跟最後評估的 set 是一樣的\",\"針對 validation set 挑選 hyperparameters (?)\",\"所以他們認為這樣不太公平，畢竟在 Validation set 做得很棒不能直接表達在整體會表達很棒。 他們也試著用相同的手段訓練模型，然後拿到了\",\"GTA5 \",\"Baseline: 35.68% (+2.83%)\",\"DACS: 53.84% (+1.7%) (BEST)\",\"SYNTHIA \",\"DACS (13 classes): 55.98% (+1.17%) (1.02% to BEST)\",\"DACS (16 classes): 49.10% (+0.76%) (0.7% to BEST)\"]},\"34\":{\"h\":\"Contribution\",\"t\":[\"Apply SSL method on ClassMix to UDA\",\"Introduce a simple framework with high-performance\",\"Beat SOTA in GTA5 to Cityscape\"]},\"35\":{\"h\":\"值得一看的文章們\",\"t\":[\"【Day 24】半監督式學習（Semi-supervised Learning）（上）\",\"【Day 25】半監督式學習（Semi-supervised Learning）（下）\",\"Notes on “DACS: Domain Adaptation via Cross-domain Mixed Sampling”\",\"物件偵測的領域自適應 (Domain Adaptation)\",\"Adversarial Learning for Semi-Supervised Semantic Segmentation\",\"Domain Adaptation in Computer Vision: Everything You Need to Know\",\"Semi-supervised semantic segmentation needs strong, varied perturbations\",\"ClassMix: Segmentation-Based Data Augmentation for Semi-Supervised Learning\",\"Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training\",\"Learning to Adapt Structured Output Space for Semantic Segmentation\"]},\"36\":{\"c\":[\"Note\"]},\"37\":{\"c\":[\"Paper Read\",\"Domain Adaption\",\"Computer Vision\",\"WACV\"]},\"38\":{\"h\":\"Noisy Networks for Exploration\"},\"39\":{\"h\":\"Basic Information\",\"t\":[\"ICLR 2018\",\"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, et al. @ Google Deepmind\"]},\"40\":{\"h\":\"問題描述\",\"t\":[\"在過去的 RL 當中我們往往仰賴對 agent 的 policy 增加 randomness 去增加 exploration，例如 ϵ-greedy 和 entropy regularization 等。不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索，然而在現實狀況下往往並不會如此簡單，而這種探索的困難度甚至是指數性地成長。\",\"例如在 Alpha Go 當中就至少探索了上千億甚至到幾兆個模擬的遊戲狀態\",\"Info\",\"論文中有提及在 Matthieu Geist, Olivier Pietquin (2014) 有提及一個使用 Neural Network 的方法，不過並沒有保證收斂，因此仍然沒有解決問題。\",\"因此這一篇論文提出一個方法試圖去消除 exploration 效率與品質的問題。\"]},\"41\":{\"h\":\"Related Works\",\"t\":[\"ϵ-greedy、Entropy Regularization\",\"Parameter Space Noise for Exploration\",\"用來加上 Noisy-Net 的各種 RL 架構 \",\"DQN\",\"Double-DQN\",\"Dueling DQN\",\"A3C\"]},\"42\":{\"h\":\"Parameter Space Noise for Exploration\",\"t\":[\"2017 年由 OpenAI 發表在 ICLR 的 paper。其方法與這一篇可說是大同小異。\",\"在過往的研究可以發現到說往往我們在設計讓 agent 有更多的 exploration 的時候都是透過增加 noise 來達成。\",\"Info\",\"舉例而言，ϵ-greedy 就是在 action space 上增加了 noise，讓選擇更多樣，以達成 exploration。\",\"而在 A3C 當中加上 Entropy Regularization，是在 Loss 上鼓勵 policy 的亂度越高越好，達到鼓勵 exploration 的效果。\",\"Image from OpenAI - Better exploration with parameter noise\",\"核心的概念很簡單，過去增加探索的方法大多都是在 action space 上增加 noise，而這裡則選擇在 parameter space 上增加 noise，並且達到了很棒的效果。\",\"Tips\",\"ϵ-greedy 就像是獵人裡面的凱特，行動之前需要先看運氣抽接下來使用的武器，即便自己知道當下用哪一個 action 比較好，卻會受到 ϵ 的限制。\",\"而在 parameter space 加上 noise 就像是可以換個角度去想其他人會怎麼做，試著用那一個人的做法走過一次，得到不同的經驗。\",\"相較之下，action space 加 noise 就比較像是在亂試，反之在 parameter space 上加 noise 就比較有系統性一些。\",\"具體來說就是他們試圖在 parameter 上加上 Gaussian Noise。\",\"θ~=θ+N(0,σ2I)\",\"Action Space Noise\",\"Parameter Space Noise\",\"Videos from OpenAI - Better exploration with parameter noise\",\"Warning\",\"底下的內容只是單純的 Review\"]},\"43\":{\"h\":\"DQN\",\"t\":[\"透過 Neural Network 去學習 optimal action value function Q∗。 Action 的決定上採用了 ϵ-greedy。\",\"Image from Ziyu Wang et al. (2015)\",\"於是他們定義了底下的 Loss function 去試圖得到 Q∗。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γb∈Amax​Q(y,b;θ−)−Q(s,a;θ))2]\",\"D 是上一個 replay buffer 的 transition distribution \",\"state s\",\"action a\",\"reward r=R(s,a)\",\"probability y∼P(⋅∣s,a)\",\"θ− 是被固定的參數\",\"Tips\",\"DQN 帶來了幾個好處\",\"Experience Replay 透過儲存 Experience，更新參數是從 replay buffer 中隨意挑一筆，降低了資料之間的相關性，讓 Neural Network 訓練避免偏差。\",\"Target Network 避免了訓練目標經常地變動造成訓練效果差\",\"使用 Neural Network 替代 action value function 避免了 Q-Learning 的 table 維度過大訓練困難的問題\"]},\"44\":{\"h\":\"Double-DQN\",\"t\":[\"在 DQN 當中我們需要同時訓練兩個 model，也就是 θ 與 θ−。然而 DQN 的設定上對於目標被發現存在高估的問題，因此 Double-DQN 提出了解決這個高估問題的方法。\",\"原始 DQN 目標\",\"Double-DQN 目標\",\"r+γmaxb∈A​Q(y,b;θ−)\",\"r+γQ(y,maxb∈A​Q(y,b;θ);θ−)\",\"高估的狀況如底下的綠線。紫線是目標函數，橘線是綠線與紫線的誤差，不難發現到確實都存在高估的狀況。\",\"然而使用了 Double-DQN 之後的誤差(藍線)就小到幾乎不存在了。\",\"Image from Hado van Hasselt, Arthur Guez, David Silver (2015)\",\"Tips\",\"Double-DQN 帶來的幾個好處\",\"讓 DQN 高估的問題消失，有更好的效果\"]},\"45\":{\"h\":\"Dueling DQN\",\"t\":[\"Dueling DQN 的概念仍然是透過 Neural Network 去學習 optimal action value function Q∗。 Action 的決定上採用了 ϵ-greedy。\",\"Image from Ziyu Wang et al. (2015)\",\"與 DQN 不同的地方在於他並不是直接去學習 Q∗，而是另外定義了一個 Advantage functionA。\",\"A(s,a)=Q(s,a)−V(s)\",\"V(s) 就像是 baseline，表示著在當前這個 state s 底下你預期可以拿到多好的 return，所以 A(s,a) 意義上就是在看每個 action 有多好多壞。\",\"透過 V 和 A 的總和就能夠得到 Q。上圖就是在最後分開成兩個輸出結果 V 和 A，最後合併成 Q。\",\"Q(s,a;θ)=V(s;θV​)+A(s,a;θA​)\",\"Info\",\"在實務上為了避免像是 V(s) 都是 0，實際上跟 DQN 沒有差異的問題，因此細節上是還會對 A(s,a) 加上總和為 0 的限制。\",\"Q(s,a,θ,α,β)=V(a,θ,α)+​A(s,a,θ,β)−∣A∣1​a′∈∣A∣∑​A(s,a′,θ,β)​\",\"α,β 只是調整 V 和 A 兩部分影響程度的參數。\",\"把 Dueling DQN 搭配 Double DQN 之後可以得到底下的 Loss。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γQ(y,argb∈Amax​Q(y,b;θ);θ−)−Q(s,a;θ))2]\",\"Tips\",\"Dueling DQN 帶來了幾個好處\",\"使用 Advantage function 增加模型的更新與 exploration。\"]},\"46\":{\"h\":\"A3C\",\"t\":[\"在 DQN 當中使用了 Experience Reply 去避免訓練資料上的強關聯性，然而存在幾個缺點\",\"需要額外的 memory 去儲存 replay buffer\",\"需要 off-policy alogorithm，對於 online RL 來說可能導致收斂不穩定以及緩慢等問題\",\"A3C 的概念就如同火影忍者的影分身之術，讓每個分身在各自的環境當中訓練，訓練成效也就翻倍。\",\"Image from Arthur Juliani@Medium\",\"A3C 是使用 advantage actor-critic 的方式，會直接去學 policy 以及 value function。因此在參數上也就包含了兩項 θπ​ 以及 θv​ 分別表示 policy 以及 value function 的參數。考慮在時間 t 往後看 k 步的更新，參照 A3C 的論文，兩個參數的 Loss 計算分別如下。\",\"∇θπ​​Lπ(θπ​)LV(θv​)​=−Eπ[i=0∑k​∇θπ​​log(π(at+i​∣st+i​;θπ​))A(st+i​,at+i​;θv​)+βi=0∑k​∇θπ​​H(π(⋅∣st+i​;θπ​))]=i=0∑k​Eπ[(Qi​−V(st+i​;θv​))2∣st+i​]​\",\"A：Advantage function 也就是 A3C 當中的 R−V\",\"H：Entropy function 根據 A3C 的論文，加上這一項能夠促使模型更好 exploration\",\"β：調整 A 和 H 的影響程度\",\"Qi​：在時間 i 時於 state st+i​ 執行 policy π 的 return\",\"Note\",\"紅色的部分也就是前面提及的 entropy regularization\",\"最後整體的 Loss 也就如下\",\"L(θ)=Lπ(θπ​)+λLV(θv​)\",\"λ 用來調整兩個 Loss 的影響力\",\"Info\",\"Note：原始 A3C 論文中更新一步，並且沒有使用 entropy 的算式\",\"θ′θv′​​:dθ←dθ+∇θ′​logπ(ai​∣si​;θ′)(R−V(si​;θv′​)):dθv​←dθv​+∂(R−V(si​;θv′​))2/∂θv′​​\",\"Tips\",\"A3C 帶來了幾個好處\",\"降低訓練資料之間的關聯性 畢竟每個 agent 訓練的環境都不同，得到的資料也就不同\",\"能夠使用 on-policy 或是 off-policy，增加通用性\",\"可以平行化加速訓練\",\"穩定地訓練\"]},\"47\":{\"h\":\"Methodology\"},\"48\":{\"h\":\"基本想法\",\"t\":[\"Noisy-Net 的想法跟 Parameter Space Noise for Exploration 的想法基本上是相同方向，都是要對 parameter space 去加上 noise。\",\"作法上，對於每個可訓練的參數拆解成 ζ=(μ,σ)，然後再透過 zero-mean 的 ϵ 增加 noise。也就是說對於一個參數 θ 我們會寫成：\",\"θ=defμ+σ⊙ϵ\",\"所以對於一個 Linear Layer 來說\",\"y=wx+b⇒y=(μw+σw⊙ϵw)x+(μb+σb⊙ϵb)\",\"就只是這樣而已，不要想太多！\",\"Tips\",\"刻意挑 zero-mean 的 noise 是為了採用底下的特性方便後續 Loss 的計算。\",\"Lˉ(ζ)=E[L(θ)]\",\"因此\",\"∇Lˉ(ζ)=∇E[L(θ)]=E[∇μ,Σ​L(μ+Σ⊙ϵ)]\",\"Σ 包含了所有 σ\",\"加上了 Monte-Carlo approximation 之後，可以用單一的 sample ξ 去近似\",\"∇Lˉ(ζ)≈∇μ,Σ​L(μ+Σ⊙ξ)\",\"跟 OpenAI 提出的方法略為不同的地方在於他並不是直接對 network 的參數加上 Gaussian Noise，而是給了參數 ϵw,ϵb 去決定要加怎樣的 noise。\",\"在每一個 episode 開始之前先把參數加上 noise，接下來這一整個 episode 就都是用這個 network 去訓練，意即在過程中不會對 noise 做調整。\"]},\"49\":{\"h\":\"減少產 random number 時間\",\"t\":[\"這樣的做法下每一個 episode 都需要 random noise 在 weight 和 bias 上。假如 w∈Rq×p,b∈Rq，那麼 ϵw∈Rq×p,ϵb∈Rq，也就意味著需要 random 出 pq+q 個數值。\",\"上面基本的做法作者稱他為 Independent Gaussian noise，而接下來作者給出一個 Factorised Gaussian noise 的做法。\",\"基本上就是將 random number 拆分\",\"ϵi,jw​ϵjb​​=f(ϵi​)f(ϵj​)=f(ϵj​)​\",\"其中 f(x)=sgn(x)∣x∣​,ϵi​∈Rq,ϵj​∈Rp。\",\"如此一來，只需要產出 p+q 個 random number 也可以達到類似的效果。\"]},\"50\":{\"h\":\"DQN & Dueling DQN\",\"t\":[\"由於 DQN 和 Dueling DQN 是在 single-thread 上訓練，因此上述的 Random Overhead 會比較大，在這裡採用 Factorised Gaussian noise。\",\"現在 Network 的部分改用 Noisy Network，也就不需要再使用 ϵ-greedy 了。\",\"原本的 DQN 對 Loss 的定義如下。\",\"L(θ)=E(s,a,r,y)∼D​[(r+γb∈Amax​Q(y,b;θ−)−Q(s,a;θ))2]\",\"現在替換成 Noisy Net 的形式\",\"Lˉ(ζ)=E[E(s,a,r,y)∼D​[r+γb∈Amax​Q(y,b,ϵ′;ζ−)−Q(s,a,ϵ;ζ)]2]\",\"最外層的期望值是對 ϵ 和 ϵ′\",\"同樣地也可以對 Dueling DQN 做修改。原本的定義為\",\"L(θ)=E(s,a,r,y)∼D​[(r+γQ(y,argb∈Amax​Q(y,b;θ);θ−)−Q(s,a;θ))2]\",\"現在替換成 Noisy Net 的形式\",\"Lˉ(ζ)=E[E(s,a,r,y)∼D​[r+γQ(y,argb∈Amax​Q(y,b,ϵ′′;ζ),ϵ′;ζ−)−Q(s,a,ϵ;ζ)]2]\"]},\"51\":{\"h\":\"Distributed A3C\",\"t\":[\"由於 A3C 是在 multi-thread 上訓練，因此不太需要考慮上述的 Random Overhead，在這裡採用 Independent Gaussian noise 即可。\",\"現在 Network 的部分改用 Noisy Network，也就不需要再使用 Entropy function 了。\",\"原本的 A3C\",\"∇θπ​​Lπ(θπ​)LV(θv​)​=−Eπ[i=0∑k​∇θπ​​log(π(at+i​∣st+i​;θπ​))A(st+i​,at+i​;θv​)+βi=0∑k​∇θπ​​H(π(⋅∣st+i​;θπ​))]=i=0∑k​Eπ[(Qi​−V(st+i​;θv​))2∣st+i​]​\",\"NoisyNet-A3C\",\"∇ζπ​​Lπ(ζπ​)LV(ζv​)​=−E[Eπ[i=0∑k​∇ζπ​​log(π(at+i​∣st+i​;ζπ​,ϵ))A(st+i​,at+i​;ζv​,ϵ)]]=E[i=0∑k​Eπ[(Qi​−V(st+i​;ζv​,ϵ))∣st+i​]]2​\",\"Noise initialize details\",\"Independent Gaussian noise\",\"μi,j​∼U[−p3​​,+p3​​]\",\"p 是 input 的數量\",\"σi,j​=0.017\",\"Factorised Gaussian noise\",\"μi,j​∼U[−p1​​,+p1​​]\",\"p 是 input 的數量\",\"σi,j​=p​0.5​\"]},\"52\":{\"h\":\"Results\"},\"53\":{\"h\":\"Experiments\",\"t\":[\"實驗是做在 57 Atari games 上。每 1M 個 frames 評估一次，episode 每 108K frames 會 truncate 一次。將沒有做任何修正的 DQN、Dueling DQN、A3C 作為 Baseline。\",\"首先把 Baseline 以及加上 NoisyNet 的模型都跟 Human 比較，底下是用來評估優劣的評分方式。\",\"100×ScoreHuman​−ScoreRandom​Scoreagent​−ScoreRandom​​\",\"Note\",\"最後得出的結果為 0：跟 Random 一樣糟 最後得出的結果為 100：跟 Human 一樣好\",\"可以從分數上明顯看出來加上了 NoisyNet 後對於 Mean 以及 Median 都有正面的影響。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\",\"接下來評估加上 NoisyNet 帶來的影響力，評分方式會也跟 Baseline 比較。\",\"100×max(ScoreHuman​,ScoreBaseline​)−ScoreRandom​Scoreagent​−ScoreBaseline​​\",\"可以看到在大多數的遊戲加上了 NoisyNet 之後的結果都有些進步。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\",\"不過進步主要在 DQN 以及 Dueling 上較為顯著。A3C 的部分在退步也是有幾項退步蠻多，也並不是每次加上 NoisyNet 都會帶來 improvement。\",\"從訓練中的曲線也可以明顯看到 NoisyNet 可以帶來很不錯的 improvement。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\"]},\"54\":{\"h\":\"Analysis\",\"t\":[\"為了進一步去釐清這樣的做法為什麼是可行、合理的，作者進一步去研究。\",\"回顧一下我們加上 Noise 的方法，是把一個可訓練參數拆成 ζ=(μ,σ)，再額外多一個 Noise ϵ。\",\"θ=defμ+σ⊙ϵ\",\"理想上，我們最後的 Loss 應該要能夠好好收斂，也就是說最後的 solution 應該要是 deterministic。那麼這裡加上的 ϵ 就應該隨著訓練慢慢被忽視，作用只在於訓練的前中期提供 exploration。因此，我們也就會期待 σ 這個參數會漸漸趨近於 0 了！\",\"定義底下的平均\",\"Σˉ=Nweights​1​i∑​∣σiw​∣\",\"作者發現在每一個遊戲當中最後一個 Layer 的 Σˉ 都是會逐漸趨近於 0 的，然而若觀察倒數第二個 Layer 卻並不一定了，有些甚至是遞增的。也就是說，其實 NoisyNet 並不會都得出 deterministic solution。\",\"此外，透過 Σˉ 的曲線也可以發現到在不同的遊戲當中他們的更新曲線相當地不同，也說明了實際上這樣的更新方式是能夠依照不同的遊戲去適應的。\",\"Image from Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot et al. (2018)\"]},\"55\":{\"h\":\"Contribution\",\"t\":[\"提供一個簡單又有效的 Exploration 方式\",\"能夠在 on-policy 以及 off-policy 上適用\",\"能夠輕易地套用在所有的 RL 算法當中\"]},\"56\":{\"h\":\"值得一看的文章們\",\"t\":[\"强化学习中on-policy 与off-policy有什么区别？\",\"Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)\",\"Asynchronous Methods for Deep Reinforcement Learning\",\"Deep Exploration via Randomized Value Functions\",\"Kalman Temporal Differences\",\"VIME: Variational Information Maximizing Exploration\",\"Parameter Space Noise for Exploration\",\"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\",\"Better exploration with parameter noise\",\"强化学习中的探索与利用（count-based)\"]},\"57\":{\"c\":[\"Note\"]},\"58\":{\"c\":[\"Paper Read\",\"Reinforcement Learning\",\"ICLR\"]},\"59\":{\"h\":\"Posts\"}},\"dirtCount\":0,\"index\":[[\"强化学习中的探索与利用\",{\"1\":{\"56\":1}}],[\"强化学习中on\",{\"1\":{\"56\":1}}],[\"与off\",{\"1\":{\"56\":1}}],[\"算法當中\",{\"1\":{\"55\":1}}],[\"能夠輕易地套用在所有的\",{\"1\":{\"55\":1}}],[\"能夠在\",{\"1\":{\"55\":1}}],[\"能夠使用\",{\"1\":{\"46\":1}}],[\"方式\",{\"1\":{\"55\":1}}],[\"方法\",{\"1\":{\"21\":1}}],[\"方法也是屬於\",{\"1\":{\"21\":1}}],[\"提供一個簡單又有效的\",{\"1\":{\"55\":1}}],[\"提出的方法略為不同的地方在於他並不是直接對\",{\"1\":{\"48\":1}}],[\"提出了解決這個高估問題的方法\",{\"1\":{\"44\":1}}],[\"提出如\",{\"1\":{\"20\":1}}],[\"卻並不一定了\",{\"1\":{\"54\":1}}],[\"卻會受到\",{\"1\":{\"42\":1}}],[\"定義底下的平均\",{\"1\":{\"54\":1}}],[\"應該要是\",{\"1\":{\"54\":1}}],[\"應該要能夠好好收斂\",{\"1\":{\"54\":1}}],[\"理想上\",{\"1\":{\"54\":1}}],[\"再額外多一個\",{\"1\":{\"54\":1}}],[\"回顧一下我們加上\",{\"1\":{\"54\":1}}],[\"合理的\",{\"1\":{\"54\":1}}],[\"合成出\",{\"1\":{\"21\":2}}],[\"為了進一步去釐清這樣的做法為什麼是可行\",{\"1\":{\"54\":1}}],[\"評分方式會也跟\",{\"1\":{\"53\":1}}],[\"評估一次\",{\"1\":{\"53\":1}}],[\"接下來評估加上\",{\"1\":{\"53\":1}}],[\"接下來這一整個\",{\"1\":{\"48\":1}}],[\"後對於\",{\"1\":{\"53\":1}}],[\"首先把\",{\"1\":{\"53\":1}}],[\"每\",{\"1\":{\"53\":2}}],[\"每個人題目會不太相同\",{\"1\":{\"4\":1}}],[\"∣st+i​\",{\"1\":{\"51\":1}}],[\"∣x∣​\",{\"1\":{\"49\":1}}],[\"即可\",{\"1\":{\"51\":1}}],[\"即便自己知道當下用哪一個\",{\"1\":{\"42\":1}}],[\"即便\",{\"1\":{\"19\":1}}],[\"同樣地也可以對\",{\"1\":{\"50\":1}}],[\"同樣以自駕車的例子來說\",{\"1\":{\"19\":1}}],[\"原本的定義為\",{\"1\":{\"50\":1}}],[\"原本的\",{\"1\":{\"50\":1,\"51\":1}}],[\"原始\",{\"1\":{\"44\":1,\"46\":1}}],[\"現在替換成\",{\"1\":{\"50\":2}}],[\"現在\",{\"1\":{\"50\":1,\"51\":1}}],[\"現實很骨感\",{\"1\":{\"6\":1}}],[\"由於\",{\"1\":{\"50\":1,\"51\":1}}],[\"拆分\",{\"1\":{\"49\":1}}],[\"出\",{\"1\":{\"49\":1}}],[\"那麼這裡加上的\",{\"1\":{\"54\":1}}],[\"那麼\",{\"1\":{\"49\":1}}],[\"那我們就可以用\",{\"1\":{\"20\":1}}],[\"那我們不要用\",{\"1\":{\"6\":1}}],[\"假如\",{\"1\":{\"49\":1}}],[\"減少產\",{\"0\":{\"49\":1}}],[\"意即在過程中不會對\",{\"1\":{\"48\":1}}],[\"意義上就是在看每個\",{\"1\":{\"45\":1}}],[\"開始之前先把參數加上\",{\"1\":{\"48\":1}}],[\"≈∇μ\",{\"1\":{\"48\":1}}],[\"ξ\",{\"1\":{\"48\":1}}],[\"包含了所有\",{\"1\":{\"48\":1}}],[\"∇ζπ​​lπ\",{\"1\":{\"51\":1}}],[\"∇μ\",{\"1\":{\"48\":1}}],[\"∇lˉ\",{\"1\":{\"48\":2}}],[\"∇θπ​​lπ\",{\"1\":{\"46\":1,\"51\":1}}],[\"ζv​\",{\"1\":{\"51\":3}}],[\"ζπ​\",{\"1\":{\"51\":2}}],[\"ζ−\",{\"1\":{\"50\":2}}],[\"ζ\",{\"1\":{\"48\":3,\"50\":5}}],[\"ζ=\",{\"1\":{\"48\":1,\"54\":1}}],[\"刻意挑\",{\"1\":{\"48\":1}}],[\"σˉ\",{\"1\":{\"54\":2}}],[\"σˉ=nweights​1​i∑​∣σiw​∣\",{\"1\":{\"54\":1}}],[\"σi\",{\"1\":{\"51\":2}}],[\"σ​l\",{\"1\":{\"48\":2}}],[\"σ\",{\"1\":{\"48\":3,\"54\":2}}],[\"σ2i\",{\"1\":{\"42\":1}}],[\"μi\",{\"1\":{\"51\":2}}],[\"μ+σ⊙ξ\",{\"1\":{\"48\":1}}],[\"μ+σ⊙ϵ\",{\"1\":{\"48\":1}}],[\"μb+σb⊙ϵb\",{\"1\":{\"48\":1}}],[\"μw+σw⊙ϵw\",{\"1\":{\"48\":1}}],[\"μ\",{\"1\":{\"48\":1,\"54\":1}}],[\"作者發現在每一個遊戲當中最後一個\",{\"1\":{\"54\":1}}],[\"作者進一步去研究\",{\"1\":{\"54\":1}}],[\"作用只在於訓練的前中期提供\",{\"1\":{\"54\":1}}],[\"作法上\",{\"1\":{\"48\":1}}],[\"作為\",{\"1\":{\"26\":1,\"53\":1}}],[\"基本想法\",{\"0\":{\"48\":1}}],[\"基本上就是將\",{\"1\":{\"49\":1}}],[\"基本上就是從\",{\"1\":{\"21\":1}}],[\"基本上就是會有一些工地的照片\",{\"1\":{\"5\":1}}],[\"基本上他們期待我們會運用\",{\"1\":{\"5\":1}}],[\"穩定地訓練\",{\"1\":{\"46\":1}}],[\"∂θv\",{\"1\":{\"46\":1}}],[\"論文中更新一步\",{\"1\":{\"46\":1}}],[\"論文中有提及在\",{\"1\":{\"40\":1}}],[\"紅色的部分也就是前面提及的\",{\"1\":{\"46\":1}}],[\"執行\",{\"1\":{\"46\":1}}],[\"時間\",{\"0\":{\"49\":1}}],[\"時於\",{\"1\":{\"46\":1}}],[\"時常我們會訓練在合成資料上\",{\"1\":{\"16\":1}}],[\"根據\",{\"1\":{\"46\":1}}],[\"π\",{\"1\":{\"46\":3,\"51\":3}}],[\"計算分別如下\",{\"1\":{\"46\":1}}],[\"參照\",{\"1\":{\"46\":1}}],[\"步的更新\",{\"1\":{\"46\":1}}],[\"往後看\",{\"1\":{\"46\":1}}],[\"往往在邊界上會有許多誤判的\",{\"1\":{\"23\":1}}],[\"往往先從貼標籤開始\",{\"1\":{\"20\":1}}],[\"往往就會\",{\"1\":{\"17\":1}}],[\"往往會有許多我們沒有的\",{\"1\":{\"16\":1}}],[\"考慮在時間\",{\"1\":{\"46\":1}}],[\"考慮到\",{\"1\":{\"32\":1}}],[\"分別表示\",{\"1\":{\"46\":1}}],[\"分組第三名\",{\"1\":{\"2\":1}}],[\"訓練的環境都不同\",{\"1\":{\"46\":1}}],[\"訓練成效也就翻倍\",{\"1\":{\"46\":1}}],[\"訓練避免偏差\",{\"1\":{\"43\":1}}],[\"需要\",{\"1\":{\"46\":1}}],[\"需要額外的\",{\"1\":{\"46\":1}}],[\"需要解出一些簡單的演算法題目\",{\"1\":{\"4\":1}}],[\"兩部分影響程度的參數\",{\"1\":{\"45\":1}}],[\"兩個參數的\",{\"1\":{\"46\":1}}],[\"兩個\",{\"1\":{\"19\":1}}],[\"​=−e\",{\"1\":{\"51\":1}}],[\"​=−eπ\",{\"1\":{\"46\":1,\"51\":1}}],[\"​logπ\",{\"1\":{\"46\":1}}],[\"​​\",{\"1\":{\"46\":2}}],[\"​\",{\"1\":{\"45\":1,\"46\":3,\"49\":1,\"51\":1}}],[\"∈∣a∣∑​a\",{\"1\":{\"45\":1}}],[\"β\",{\"1\":{\"45\":4,\"46\":1}}],[\"α\",{\"1\":{\"45\":3}}],[\"沒有差異的問題\",{\"1\":{\"45\":1}}],[\"底下是用來評估優劣的評分方式\",{\"1\":{\"53\":1}}],[\"底下你預期可以拿到多好的\",{\"1\":{\"45\":1}}],[\"底下的內容只是單純的\",{\"1\":{\"42\":1}}],[\"表示著在當前這個\",{\"1\":{\"45\":1}}],[\"−scorerandom​scoreagent​−scorebaseline​​\",{\"1\":{\"53\":1}}],[\"−p1​​\",{\"1\":{\"51\":1}}],[\"−p3​​\",{\"1\":{\"51\":1}}],[\"−∣a∣1​a\",{\"1\":{\"45\":1}}],[\"−v\",{\"1\":{\"45\":1}}],[\"−q\",{\"1\":{\"43\":1,\"45\":1,\"50\":4}}],[\"=sgn\",{\"1\":{\"49\":1}}],[\"=f\",{\"1\":{\"49\":1}}],[\"=∇e\",{\"1\":{\"48\":1}}],[\"=lπ\",{\"1\":{\"46\":1}}],[\"=i=0∑k​eπ\",{\"1\":{\"46\":1,\"51\":1}}],[\"=v\",{\"1\":{\"45\":2}}],[\"=q\",{\"1\":{\"45\":1}}],[\"=e\",{\"1\":{\"24\":1,\"43\":1,\"45\":1,\"48\":2,\"50\":4,\"51\":1}}],[\"帶來的影響力\",{\"1\":{\"53\":1}}],[\"帶來的幾個好處\",{\"1\":{\"44\":1}}],[\"帶來了幾個好處\",{\"1\":{\"43\":1,\"45\":1,\"46\":1}}],[\"藍線\",{\"1\":{\"44\":1}}],[\"橘線是綠線與紫線的誤差\",{\"1\":{\"44\":1}}],[\"紫線是目標函數\",{\"1\":{\"44\":1}}],[\"高估的問題消失\",{\"1\":{\"44\":1}}],[\"高估的狀況如底下的綠線\",{\"1\":{\"44\":1}}],[\"高中接觸了演算法\",{\"1\":{\"0\":1}}],[\"目標\",{\"1\":{\"44\":2}}],[\"目前正在朝向\",{\"1\":{\"0\":1}}],[\"目前就讀於清華大學資訊工程學系\",{\"1\":{\"0\":1}}],[\"維度過大訓練困難的問題\",{\"1\":{\"43\":1}}],[\"避免了\",{\"1\":{\"43\":1}}],[\"避免了訓練目標經常地變動造成訓練效果差\",{\"1\":{\"43\":1}}],[\"替代\",{\"1\":{\"43\":1}}],[\"更新參數是從\",{\"1\":{\"43\":1}}],[\"⋅∣st+i​\",{\"1\":{\"46\":1,\"51\":1}}],[\"⋅∣s\",{\"1\":{\"43\":1}}],[\"∼d​\",{\"1\":{\"43\":1,\"45\":1,\"50\":4}}],[\"於是他們定義了底下的\",{\"1\":{\"43\":1}}],[\"qi​\",{\"1\":{\"46\":1}}],[\"qi​−v\",{\"1\":{\"46\":1,\"51\":2}}],[\"q\",{\"1\":{\"43\":1,\"45\":4}}],[\"q∗\",{\"1\":{\"43\":2,\"45\":2}}],[\"qualification\",{\"1\":{\"2\":1}}],[\"反之在\",{\"1\":{\"42\":1}}],[\"反之則相遠\",{\"1\":{\"16\":1}}],[\"試著用那一個人的做法走過一次\",{\"1\":{\"42\":1}}],[\"試圖讓\",{\"1\":{\"7\":1}}],[\"加\",{\"1\":{\"42\":1}}],[\"加上了\",{\"1\":{\"48\":1}}],[\"加上這一項能夠促使模型更好\",{\"1\":{\"46\":1}}],[\"加上總和為\",{\"1\":{\"45\":1}}],[\"加上\",{\"1\":{\"42\":1}}],[\"加權分數\",{\"1\":{\"9\":1}}],[\"比較\",{\"1\":{\"53\":2}}],[\"比較好\",{\"1\":{\"42\":1}}],[\"比賽會在正式開賽前一周公布題目\",{\"1\":{\"6\":1}}],[\"比賽過程\",{\"0\":{\"6\":1}}],[\"比賽題目\",{\"0\":{\"5\":1}}],[\"行動之前需要先看運氣抽接下來使用的武器\",{\"1\":{\"42\":1}}],[\"行人都還是會跟地板黏在一起\",{\"1\":{\"19\":1}}],[\"核心的概念很簡單\",{\"1\":{\"42\":1}}],[\"達到鼓勵\",{\"1\":{\"42\":1}}],[\"以達成\",{\"1\":{\"42\":1}}],[\"以及加上\",{\"1\":{\"7\":1,\"53\":1}}],[\"以及\",{\"1\":{\"0\":1,\"6\":1,\"16\":1,\"19\":2,\"23\":1,\"24\":1,\"27\":1,\"46\":3,\"53\":2,\"55\":1}}],[\"讓每個分身在各自的環境當中訓練\",{\"1\":{\"46\":1}}],[\"讓\",{\"1\":{\"43\":1,\"44\":1}}],[\"讓選擇更多樣\",{\"1\":{\"42\":1}}],[\"讓我們得以用較低的成本在虛擬環境中訓練模型\",{\"1\":{\"16\":1}}],[\"來達成\",{\"1\":{\"42\":1}}],[\"來說\",{\"1\":{\"48\":1}}],[\"來說可能導致收斂不穩定以及緩慢等問題\",{\"1\":{\"46\":1}}],[\"來說由於缺乏對於\",{\"1\":{\"20\":1}}],[\"來說是相當大的問題\",{\"1\":{\"20\":1}}],[\"發表在\",{\"1\":{\"42\":1}}],[\"年由\",{\"1\":{\"42\":1}}],[\"架構\",{\"1\":{\"41\":1}}],[\"用來調整兩個\",{\"1\":{\"46\":1}}],[\"用來加上\",{\"1\":{\"41\":1}}],[\"用其他\",{\"1\":{\"6\":1}}],[\"效率與品質的問題\",{\"1\":{\"40\":1}}],[\"然而若觀察倒數第二個\",{\"1\":{\"54\":1}}],[\"然而存在幾個缺點\",{\"1\":{\"46\":1}}],[\"然而使用了\",{\"1\":{\"44\":1}}],[\"然而\",{\"1\":{\"44\":1}}],[\"然而在現實狀況下往往並不會如此簡單\",{\"1\":{\"40\":1}}],[\"然後再透過\",{\"1\":{\"48\":1}}],[\"然後再應用在真實的世界當中\",{\"1\":{\"16\":1}}],[\"然後拿到了\",{\"1\":{\"33\":1}}],[\"然後會有一個\",{\"1\":{\"19\":1}}],[\"然後應用在真實的環境當中\",{\"1\":{\"16\":1}}],[\"等\",{\"1\":{\"40\":1}}],[\"等不同的做法\",{\"1\":{\"19\":1}}],[\"ϵj​∈rp\",{\"1\":{\"49\":1}}],[\"ϵj​\",{\"1\":{\"49\":2}}],[\"ϵi​∈rq\",{\"1\":{\"49\":1}}],[\"ϵi​\",{\"1\":{\"49\":1}}],[\"ϵi\",{\"1\":{\"49\":1}}],[\"ϵb∈rq\",{\"1\":{\"49\":1}}],[\"ϵb\",{\"1\":{\"48\":1}}],[\"ϵw∈rq×p\",{\"1\":{\"49\":1}}],[\"ϵw\",{\"1\":{\"48\":1}}],[\"ϵ\",{\"1\":{\"40\":1,\"41\":1,\"42\":3,\"43\":1,\"45\":1,\"48\":1,\"50\":8,\"51\":3,\"54\":2}}],[\"例如在\",{\"1\":{\"40\":1}}],[\"例如\",{\"1\":{\"40\":1}}],[\"例如說在自駕車的道路辨識當中鄰近人行道這種時常出現的\",{\"1\":{\"20\":1}}],[\"增加通用性\",{\"1\":{\"46\":1}}],[\"增加模型的更新與\",{\"1\":{\"45\":1}}],[\"增加\",{\"1\":{\"40\":1,\"48\":1}}],[\"kalman\",{\"1\":{\"56\":1}}],[\"k\",{\"1\":{\"46\":1}}],[\"know\",{\"1\":{\"35\":1}}],[\"kick\",{\"1\":{\"2\":2}}],[\"物件偵測的領域自適應\",{\"1\":{\"35\":1}}],[\"下\",{\"1\":{\"35\":1}}],[\"下拍攝\",{\"1\":{\"30\":1}}],[\"下拍攝的\",{\"1\":{\"29\":1}}],[\"值得一看的文章們\",{\"0\":{\"35\":1,\"56\":1}}],[\"017\",{\"1\":{\"51\":1}}],[\"0\",{\"1\":{\"33\":1,\"42\":1,\"45\":2,\"53\":1,\"54\":2}}],[\"02\",{\"1\":{\"33\":1}}],[\"+p1​​\",{\"1\":{\"51\":1}}],[\"+p3​​\",{\"1\":{\"51\":1}}],[\"+λlv\",{\"1\":{\"46\":1}}],[\"+λh\",{\"1\":{\"24\":1}}],[\"+βi=0∑k​∇θπ​​h\",{\"1\":{\"46\":1,\"51\":1}}],[\"+​a\",{\"1\":{\"45\":1}}],[\"+a\",{\"1\":{\"45\":1}}],[\"+0\",{\"1\":{\"33\":1}}],[\"+1\",{\"1\":{\"33\":2}}],[\"+2\",{\"1\":{\"33\":1}}],[\"挑選\",{\"1\":{\"33\":1}}],[\"針對\",{\"1\":{\"33\":1}}],[\"判斷要不要\",{\"1\":{\"33\":1}}],[\"並且沒有使用\",{\"1\":{\"46\":1}}],[\"並且達到了很棒的效果\",{\"1\":{\"42\":1}}],[\"並沒有\",{\"1\":{\"33\":1}}],[\"並不會都得出\",{\"1\":{\"54\":1}}],[\"並不一定要是\",{\"1\":{\"31\":1}}],[\"並不是所有的\",{\"1\":{\"20\":1}}],[\"他們也試著用相同的手段訓練模型\",{\"1\":{\"33\":1}}],[\"他們選擇用\",{\"1\":{\"33\":1}}],[\"他們選擇其中\",{\"1\":{\"31\":1}}],[\"他們認為在其他的\",{\"1\":{\"33\":1}}],[\"頗偏\",{\"1\":{\"32\":1}}],[\"甚至對\",{\"1\":{\"32\":1}}],[\"甚至連\",{\"1\":{\"10\":1}}],[\"使用\",{\"1\":{\"32\":1,\"43\":1,\"45\":1}}],[\"使得同類型的資料會相近\",{\"1\":{\"16\":1}}],[\"真的很糟\",{\"1\":{\"31\":1}}],[\"除了\",{\"1\":{\"31\":1,\"32\":1}}],[\"單純用\",{\"1\":{\"31\":1,\"32\":1}}],[\"單純的\",{\"1\":{\"23\":1}}],[\"個數值\",{\"1\":{\"49\":1}}],[\"個的平均\",{\"1\":{\"32\":1}}],[\"個平均跟\",{\"1\":{\"32\":1}}],[\"個\",{\"1\":{\"30\":1,\"32\":2,\"49\":1,\"53\":1}}],[\"建構的\",{\"1\":{\"30\":1}}],[\"可對應到\",{\"1\":{\"29\":1}}],[\"可以帶來很不錯的\",{\"1\":{\"53\":1}}],[\"可以看到在大多數的遊戲加上了\",{\"1\":{\"53\":1}}],[\"可以從分數上明顯看出來加上了\",{\"1\":{\"53\":1}}],[\"可以用單一的\",{\"1\":{\"48\":1}}],[\"可以平行化加速訓練\",{\"1\":{\"46\":1}}],[\"可以發現\",{\"1\":{\"31\":1,\"32\":1}}],[\"可以發現單純用\",{\"1\":{\"17\":1}}],[\"可以分成\",{\"1\":{\"19\":1}}],[\"可以嘗試\",{\"1\":{\"6\":1}}],[\"可以自己使用的免費咖啡機\",{\"1\":{\"6\":1}}],[\"照片是在\",{\"1\":{\"29\":1,\"30\":1}}],[\"照片是在城市當中開車拍下的各種照片\",{\"1\":{\"28\":1}}],[\"照片中有多少人\",{\"1\":{\"5\":1}}],[\"則是現實世界當中的影像\",{\"1\":{\"27\":1}}],[\">\",{\"0\":{\"31\":1,\"32\":1},\"1\":{\"27\":2}}],[\"搭配\",{\"1\":{\"26\":1,\"45\":1}}],[\"選擇採用\",{\"1\":{\"26\":1}}],[\"θ=defμ+σ⊙ϵ\",{\"1\":{\"48\":1,\"54\":1}}],[\"θv\",{\"1\":{\"46\":3}}],[\"θv​\",{\"1\":{\"45\":1,\"46\":5,\"51\":3}}],[\"θπ​\",{\"1\":{\"46\":5,\"51\":3}}],[\"θa​\",{\"1\":{\"45\":1}}],[\"θ−\",{\"1\":{\"43\":2,\"44\":3,\"45\":1,\"50\":2}}],[\"θ~=θ+n\",{\"1\":{\"42\":1}}],[\"θ\",{\"1\":{\"24\":1,\"43\":2,\"44\":2,\"45\":8,\"46\":3,\"48\":3,\"50\":5}}],[\"部分的影響程度\",{\"1\":{\"24\":1}}],[\"調整\",{\"1\":{\"24\":1,\"46\":1}}],[\"λ\",{\"1\":{\"24\":1,\"46\":1}}],[\"得到的資料也就不同\",{\"1\":{\"46\":1}}],[\"得到不同的經驗\",{\"1\":{\"42\":1}}],[\"得到\",{\"1\":{\"24\":1}}],[\"經過\",{\"1\":{\"24\":1}}],[\"經過相同的\",{\"1\":{\"19\":1}}],[\"取得\",{\"1\":{\"24\":1}}],[\"取出圖片\",{\"1\":{\"24\":1}}],[\"取出圖片與\",{\"1\":{\"24\":1}}],[\"從訓練中的曲線也可以明顯看到\",{\"1\":{\"53\":1}}],[\"從\",{\"1\":{\"24\":2}}],[\"詳細的步驟具體來說\",{\"1\":{\"24\":1}}],[\"會\",{\"1\":{\"53\":1}}],[\"會比較大\",{\"1\":{\"50\":1}}],[\"會直接去學\",{\"1\":{\"46\":1}}],[\"會造成的問題是吻合的\",{\"1\":{\"23\":1}}],[\"會不同\",{\"1\":{\"17\":1}}],[\"覆蓋\",{\"1\":{\"23\":1}}],[\"許多的\",{\"1\":{\"23\":1}}],[\"之後的結果都有些進步\",{\"1\":{\"53\":1}}],[\"之後的誤差\",{\"1\":{\"44\":1}}],[\"之後\",{\"1\":{\"48\":1}}],[\"之後可以得到底下的\",{\"1\":{\"45\":1}}],[\"之類的\",{\"1\":{\"23\":1}}],[\"之前的想法是先去做一些\",{\"1\":{\"6\":1}}],[\"被預測成\",{\"1\":{\"23\":2}}],[\"拿去訓練\",{\"1\":{\"23\":1,\"24\":1}}],[\"拿出兩張\",{\"1\":{\"21\":1}}],[\"將沒有做任何修正的\",{\"1\":{\"53\":1}}],[\"將\",{\"1\":{\"23\":1,\"24\":1}}],[\"將產出的\",{\"1\":{\"19\":1}}],[\"進而解決這個問題\",{\"1\":{\"21\":1}}],[\"進入決賽\",{\"1\":{\"2\":1}}],[\"邊界上的判斷會因為圖片跟相鄰環境的相似導致模糊不清\",{\"1\":{\"21\":1}}],[\"x\",{\"1\":{\"49\":2}}],[\"x+\",{\"1\":{\"48\":1}}],[\"xm​\",{\"1\":{\"24\":4}}],[\"xt​\",{\"1\":{\"24\":3}}],[\"xs​\",{\"1\":{\"24\":5}}],[\"xa​\",{\"1\":{\"21\":2}}],[\"x2\",{\"1\":{\"5\":1}}],[\"跟\",{\"1\":{\"21\":2,\"24\":1,\"26\":1,\"48\":1,\"53\":2}}],[\"跟過去自己想像當中在無塵室裏面處理晶圓的那種印象是完全不同\",{\"1\":{\"11\":1}}],[\"先轉成\",{\"1\":{\"21\":1}}],[\"先在\",{\"1\":{\"20\":1}}],[\"把\",{\"1\":{\"21\":3,\"23\":1,\"24\":1,\"45\":1}}],[\"把兩個圖片\",{\"1\":{\"21\":1}}],[\"把輸出結果套入\",{\"1\":{\"10\":1}}],[\"具體來說就是他們試圖在\",{\"1\":{\"42\":1}}],[\"具體來說\",{\"1\":{\"21\":1}}],[\"像是\",{\"1\":{\"21\":1,\"23\":1,\"31\":1}}],[\"像是直接有一台車會去蒐集真實街景資料\",{\"1\":{\"16\":1}}],[\"混合成新的圖片\",{\"1\":{\"21\":1}}],[\"產生一個新的\",{\"1\":{\"21\":1}}],[\"雖然已經有\",{\"1\":{\"20\":1}}],[\"雖然說在比賽之前我們的想法是\",{\"1\":{\"6\":1}}],[\"雖然說是黑客松\",{\"1\":{\"6\":1}}],[\"49\",{\"1\":{\"33\":1}}],[\"4\",{\"1\":{\"20\":1}}],[\"44\",{\"1\":{\"9\":1}}],[\"zero\",{\"1\":{\"48\":2}}],[\"ziyu\",{\"1\":{\"43\":1,\"45\":1}}],[\"zou\",{\"1\":{\"20\":1}}],[\"zhang\",{\"1\":{\"17\":1}}],[\"導致卡車時常被預測成汽車\",{\"1\":{\"20\":1}}],[\"導致預測失準\",{\"1\":{\"17\":1}}],[\"或是\",{\"1\":{\"46\":1}}],[\"或是汽車比卡車更常見\",{\"1\":{\"20\":1}}],[\"或是看起來安全帽是後製貼上去的\",{\"1\":{\"7\":1}}],[\"對\",{\"1\":{\"50\":1}}],[\"對應的\",{\"1\":{\"21\":2}}],[\"對陌生人的認識\",{\"1\":{\"20\":1}}],[\"對於每個可訓練的參數拆解成\",{\"1\":{\"48\":1}}],[\"對於\",{\"1\":{\"20\":1,\"46\":1}}],[\"對抗式學習\",{\"1\":{\"19\":1}}],[\"缺乏\",{\"1\":{\"20\":1}}],[\"換句話說\",{\"1\":{\"20\":1}}],[\"uda\",{\"0\":{\"23\":1},\"1\":{\"20\":4,\"23\":2,\"34\":1}}],[\"unlabelled\",{\"1\":{\"23\":1}}],[\"unlabeled\",{\"1\":{\"20\":3,\"21\":1}}],[\"unlebelled\",{\"1\":{\"23\":1}}],[\"unsupervised\",{\"1\":{\"20\":1}}],[\"unity\",{\"1\":{\"30\":1}}],[\"university\",{\"1\":{\"15\":1}}],[\"unix\",{\"1\":{\"1\":1}}],[\"尤其在\",{\"1\":{\"20\":1}}],[\"尤其是從\",{\"1\":{\"17\":1}}],[\"畢竟每個\",{\"1\":{\"46\":1}}],[\"畢竟在\",{\"1\":{\"33\":1}}],[\"畢竟\",{\"1\":{\"20\":1}}],[\"了\",{\"1\":{\"20\":1,\"50\":1,\"51\":1,\"54\":1}}],[\"做修改\",{\"1\":{\"50\":1}}],[\"做調整\",{\"1\":{\"48\":1}}],[\"做得很棒不能直接表達在整體會表達很棒\",{\"1\":{\"33\":1}}],[\"做出一個\",{\"1\":{\"21\":1}}],[\"做\",{\"1\":{\"20\":1}}],[\"做對抗式學習\",{\"1\":{\"19\":1}}],[\"舉例而言\",{\"1\":{\"42\":1}}],[\"舉例來說\",{\"1\":{\"20\":1}}],[\"舉一個在\",{\"1\":{\"16\":1}}],[\"而接下來作者給出一個\",{\"1\":{\"49\":1}}],[\"而是給了參數\",{\"1\":{\"48\":1}}],[\"而是另外定義了一個\",{\"1\":{\"45\":1}}],[\"而是將\",{\"1\":{\"24\":1}}],[\"而這裡則選擇在\",{\"1\":{\"42\":1}}],[\"而這種探索的困難度甚至是指數性地成長\",{\"1\":{\"40\":1}}],[\"而在\",{\"1\":{\"42\":2}}],[\"而\",{\"1\":{\"20\":1,\"26\":1,\"27\":1}}],[\"而半監督式學習困難的點在於雖然對於\",{\"1\":{\"20\":1}}],[\"而被提出的\",{\"1\":{\"20\":1}}],[\"半監督式學習\",{\"1\":{\"20\":1,\"35\":2}}],[\"最外層的期望值是對\",{\"1\":{\"50\":1}}],[\"最好的\",{\"1\":{\"31\":1}}],[\"最\",{\"1\":{\"23\":1}}],[\"最初被用於把\",{\"1\":{\"21\":1}}],[\"最初是為了解決\",{\"1\":{\"20\":1}}],[\"最後得出的結果為\",{\"1\":{\"53\":2}}],[\"最後整體的\",{\"1\":{\"46\":1}}],[\"最後合併成\",{\"1\":{\"45\":1}}],[\"最後在\",{\"1\":{\"9\":1}}],[\"最後只有\",{\"1\":{\"6\":1}}],[\"天空之類的就通常會像是在半空中\",{\"1\":{\"19\":1}}],[\"號誌\",{\"1\":{\"19\":1}}],[\"汽車\",{\"1\":{\"19\":1}}],[\"和\",{\"1\":{\"19\":1,\"20\":1,\"40\":1,\"45\":3,\"46\":1,\"49\":1,\"50\":2}}],[\"通常並不會差太多\",{\"1\":{\"19\":1}}],[\"依照\",{\"1\":{\"19\":1}}],[\"中隨意挑一筆\",{\"1\":{\"43\":1}}],[\"中各取圖片\",{\"1\":{\"19\":1}}],[\"中間還有提供午餐\",{\"1\":{\"6\":1}}],[\"還是存在差異的\",{\"1\":{\"20\":1}}],[\"還是\",{\"1\":{\"19\":1}}],[\"還要強許多\",{\"1\":{\"6\":1}}],[\"透過儲存\",{\"1\":{\"43\":1}}],[\"透過一些方式混在一起\",{\"1\":{\"21\":1}}],[\"透過這個模型我們就有辦法給\",{\"1\":{\"20\":1}}],[\"透過\",{\"1\":{\"19\":1,\"24\":1,\"43\":1,\"45\":1,\"54\":1}}],[\"直接把訓練在虛擬環境的模型應用在真實環境\",{\"1\":{\"17\":1}}],[\"擅自用自己的思維解讀\",{\"1\":{\"17\":1}}],[\"因為缺乏對他人的理解\",{\"1\":{\"17\":1}}],[\"因此不太需要考慮上述的\",{\"1\":{\"51\":1}}],[\"因此不少組別在實際進到\",{\"1\":{\"6\":1}}],[\"因此上述的\",{\"1\":{\"50\":1}}],[\"因此在參數上也就包含了兩項\",{\"1\":{\"46\":1}}],[\"因此細節上是還會對\",{\"1\":{\"45\":1}}],[\"因此這一篇論文提出一個方法試圖去消除\",{\"1\":{\"40\":1}}],[\"因此這時候\",{\"1\":{\"21\":1}}],[\"因此仍然沒有解決問題\",{\"1\":{\"40\":1}}],[\"因此\",{\"1\":{\"16\":1,\"44\":1,\"48\":1,\"54\":1}}],[\"只需要產出\",{\"1\":{\"49\":1}}],[\"只是調整\",{\"1\":{\"45\":1}}],[\"只對簡單的\",{\"1\":{\"31\":1}}],[\"只有\",{\"1\":{\"20\":1}}],[\"只訓練在\",{\"1\":{\"17\":1}}],[\"只能說在設備上直接贏了\",{\"1\":{\"6\":1}}],[\"各自的\",{\"1\":{\"17\":1}}],[\"問題在於不同的\",{\"1\":{\"17\":1}}],[\"問題描述\",{\"0\":{\"17\":1,\"40\":1}}],[\"轉變到\",{\"1\":{\"17\":1}}],[\"不要想太多\",{\"1\":{\"48\":1}}],[\"不難發現到確實都存在高估的狀況\",{\"1\":{\"44\":1}}],[\"不同的地方在於他並不是直接去學習\",{\"1\":{\"45\":1}}],[\"不同\",{\"1\":{\"19\":1}}],[\"不太好\",{\"1\":{\"17\":1}}],[\"不過進步主要在\",{\"1\":{\"53\":1}}],[\"不過並沒有保證收斂\",{\"1\":{\"40\":1}}],[\"不過這樣的做法往往只能在較於簡單的環境當中有比較有效率的探索\",{\"1\":{\"40\":1}}],[\"不過這裡最主要都是使用\",{\"1\":{\"26\":1}}],[\"不過這種情況下一個直覺的問題是\",{\"1\":{\"16\":1}}],[\"不過像是馬路\",{\"1\":{\"19\":1}}],[\"不過如果遇到新的\",{\"1\":{\"17\":1}}],[\"不過實際上在實作的時候光是硬體的資源可能就是一大障礙\",{\"1\":{\"11\":1}}],[\"不過理想很美好\",{\"1\":{\"6\":1}}],[\"不過寄信去詢問之後得到希望還是使用到\",{\"1\":{\"6\":1}}],[\"不過場地因為是辦公室\",{\"1\":{\"6\":1}}],[\"影像分割\",{\"1\":{\"17\":1}}],[\"近年來透過\",{\"1\":{\"17\":1}}],[\"降低訓練資料之間的關聯性\",{\"1\":{\"46\":1}}],[\"降低了資料之間的相關性\",{\"1\":{\"43\":1}}],[\"降低\",{\"1\":{\"16\":1,\"24\":1}}],[\"想解決的就是盡可能地將\",{\"1\":{\"16\":1}}],[\"到\",{\"1\":{\"16\":1}}],[\"就應該隨著訓練慢慢被忽視\",{\"1\":{\"54\":1}}],[\"就都是用這個\",{\"1\":{\"48\":1}}],[\"就只是這樣而已\",{\"1\":{\"48\":1}}],[\"就小到幾乎不存在了\",{\"1\":{\"44\":1}}],[\"就比較有系統性一些\",{\"1\":{\"42\":1}}],[\"就比較像是在亂試\",{\"1\":{\"42\":1}}],[\"就像是\",{\"1\":{\"45\":1}}],[\"就像是可以換個角度去想其他人會怎麼做\",{\"1\":{\"42\":1}}],[\"就像是獵人裡面的凱特\",{\"1\":{\"42\":1}}],[\"就能夠比較好發揮作用\",{\"1\":{\"21\":1}}],[\"就可以再拿去\",{\"1\":{\"20\":1}}],[\"就可以得到相當好的影像分割結果\",{\"1\":{\"17\":1}}],[\"就相當地雷同\",{\"1\":{\"19\":1}}],[\"就會導致互相的不理解\",{\"1\":{\"17\":1}}],[\"就會導致單純在\",{\"1\":{\"16\":1}}],[\"就是在\",{\"1\":{\"42\":1}}],[\"就是希望\",{\"1\":{\"24\":1}}],[\"就是\",{\"1\":{\"19\":1,\"23\":1}}],[\"就是用來描述一群資料他們的分布狀況\",{\"1\":{\"16\":1}}],[\"就是拿來做簡報\",{\"1\":{\"6\":1}}],[\"過高\",{\"1\":{\"16\":1}}],[\"過去增加探索的方法大多都是在\",{\"1\":{\"42\":1}}],[\"過去基本上就是看過\",{\"1\":{\"11\":1}}],[\"過去曾擔任臺南一中資訊社社長\",{\"1\":{\"0\":1}}],[\"相較之下\",{\"1\":{\"42\":1}}],[\"相鄰而導致的誤判被稱為\",{\"1\":{\"23\":1}}],[\"相差過大\",{\"1\":{\"16\":1}}],[\"相當佩服\",{\"1\":{\"10\":1}}],[\"環境與虛擬世界有差距\",{\"1\":{\"16\":1}}],[\"虛擬世界\",{\"1\":{\"16\":1}}],[\"在這裡採用\",{\"1\":{\"50\":1,\"51\":1}}],[\"在每一個\",{\"1\":{\"48\":1}}],[\"在時間\",{\"1\":{\"46\":1}}],[\"在實務上為了避免像是\",{\"1\":{\"45\":1}}],[\"在過往的研究可以發現到說往往我們在設計讓\",{\"1\":{\"42\":1}}],[\"在過去的\",{\"1\":{\"40\":1}}],[\"在絕大多數並非是最佳的結果上都不會離最佳太遠\",{\"1\":{\"31\":1,\"32\":1}}],[\"在邊界上往往會出現誤差的問題解決\",{\"1\":{\"21\":1}}],[\"在\",{\"1\":{\"16\":1,\"19\":1,\"20\":1,\"21\":2,\"23\":1,\"24\":1,\"26\":2,\"27\":1,\"44\":1,\"46\":1,\"49\":1}}],[\"在提供的\",{\"1\":{\"7\":1}}],[\"很多時候我們並不會直接去蒐集真實的資料\",{\"1\":{\"16\":1}}],[\"投射到同一個平面上\",{\"1\":{\"16\":1}}],[\"所謂的半監督式學習也就是說\",{\"1\":{\"20\":1}}],[\"所謂的\",{\"1\":{\"16\":1,\"20\":1}}],[\"所以對於一個\",{\"1\":{\"48\":1}}],[\"所以\",{\"1\":{\"45\":1}}],[\"所以他們認為這樣不太公平\",{\"1\":{\"33\":1}}],[\"所以在數據上\",{\"1\":{\"32\":1}}],[\"所以在前面加上了一些\",{\"1\":{\"7\":1}}],[\"所以就讓他專門來回答這個問題\",{\"1\":{\"8\":1}}],[\"所以我們後續就主要專攻\",{\"1\":{\"6\":1}}],[\"所以我們後續的方向都著重在\",{\"1\":{\"6\":1}}],[\"所以晚上\",{\"1\":{\"6\":1}}],[\"所以會有額外的提問\",{\"1\":{\"5\":1}}],[\"共同發表\",{\"1\":{\"15\":1}}],[\"與\",{\"1\":{\"15\":1,\"44\":1,\"45\":1}}],[\"查爾摩斯理工大學\",{\"1\":{\"15\":1}}],[\"w∈rq×p\",{\"1\":{\"49\":1}}],[\"weight\",{\"1\":{\"49\":1}}],[\"with\",{\"1\":{\"34\":1,\"42\":2,\"56\":2}}],[\"wilhelm\",{\"1\":{\"23\":2,\"24\":2,\"31\":2,\"32\":1}}],[\"winter\",{\"1\":{\"15\":1}}],[\"works\",{\"0\":{\"18\":1,\"41\":1}}],[\"work\",{\"1\":{\"17\":1}}],[\"what\",{\"0\":{\"16\":1}}],[\"wang\",{\"1\":{\"43\":1,\"45\":1}}],[\"wall\",{\"1\":{\"30\":1}}],[\"wacv\",{\"1\":{\"15\":1},\"2\":{\"37\":1}}],[\"warning\",{\"1\":{\"5\":3,\"42\":1}}],[\"感到很開心\",{\"1\":{\"11\":1}}],[\"讀的時候都覺得嗯嗯嗯很有道理\",{\"1\":{\"11\":1}}],[\"當兩個\",{\"1\":{\"16\":1}}],[\"當時有遇到問題去找他們的時候都可以得到即時的\",{\"1\":{\"11\":1}}],[\"當中的\",{\"1\":{\"46\":1}}],[\"當中使用了\",{\"1\":{\"46\":1}}],[\"當中使用的是\",{\"1\":{\"21\":1}}],[\"當中我們需要同時訓練兩個\",{\"1\":{\"44\":1}}],[\"當中我們往往仰賴對\",{\"1\":{\"40\":1}}],[\"當中加上\",{\"1\":{\"42\":1}}],[\"當中就至少探索了上千億甚至到幾兆個模擬的遊戲狀態\",{\"1\":{\"40\":1}}],[\"當中通常\",{\"1\":{\"20\":1}}],[\"當中\",{\"1\":{\"7\":1,\"23\":1}}],[\"我也能感受到每個員工對我們都很友善\",{\"1\":{\"11\":1}}],[\"我覺得這次到\",{\"1\":{\"11\":1}}],[\"我們也就會期待\",{\"1\":{\"54\":1}}],[\"我們也發現到說在回答顏色的那一題\",{\"1\":{\"7\":1}}],[\"我們最後的\",{\"1\":{\"54\":1}}],[\"我們最後決定要把多個模型的輸出拿去做類似\",{\"1\":{\"8\":1}}],[\"我們會寫成\",{\"1\":{\"48\":1}}],[\"我們對於\",{\"1\":{\"20\":1}}],[\"我們可以想成現在\",{\"1\":{\"19\":1}}],[\"我們相信那些回答分數比較高的模型可以做得比較好\",{\"1\":{\"8\":1}}],[\"我們想說這裡根本沒人戴安全帽\",{\"1\":{\"7\":1}}],[\"我們發現其中兩份都是教室的監視器錄影畫面\",{\"1\":{\"7\":1}}],[\"我們在\",{\"1\":{\"7\":1}}],[\"我們這一組在\",{\"1\":{\"6\":1}}],[\"我們這一組拿到的是\",{\"1\":{\"5\":1}}],[\"總結\",{\"0\":{\"11\":1}}],[\"y=wx+b⇒y=\",{\"1\":{\"48\":1}}],[\"y∼p\",{\"1\":{\"43\":1}}],[\"y\",{\"1\":{\"43\":2,\"44\":3,\"45\":3,\"50\":10}}],[\"you\",{\"1\":{\"35\":1}}],[\"yolo\",{\"1\":{\"10\":1}}],[\"ym​\",{\"1\":{\"24\":4}}],[\"yt​^​\",{\"1\":{\"24\":2}}],[\"ytp\",{\"1\":{\"2\":1}}],[\"ys​\",{\"1\":{\"24\":5}}],[\"ya​\",{\"1\":{\"21\":1}}],[\"yang\",{\"1\":{\"20\":1}}],[\"yi\",{\"1\":{\"19\":2,\"26\":1}}],[\"yiheng\",{\"1\":{\"17\":1}}],[\"印象中有人用了\",{\"1\":{\"10\":1}}],[\"認識到其他組都用了怎樣的方法去解決\",{\"1\":{\"10\":1}}],[\"報告期間也都會跟我們分享他們覺得在每個地方有哪些比較好的做法也許可以嘗試看看\",{\"1\":{\"10\":1}}],[\"報告\",{\"0\":{\"10\":1}}],[\"76\",{\"1\":{\"33\":1}}],[\"70\",{\"1\":{\"9\":1}}],[\"7\",{\"1\":{\"9\":2,\"33\":2}}],[\"上適用\",{\"1\":{\"55\":1}}],[\"上較為顯著\",{\"1\":{\"53\":1}}],[\"上面基本的做法作者稱他為\",{\"1\":{\"49\":1}}],[\"上圖就是在最後分開成兩個輸出結果\",{\"1\":{\"45\":1}}],[\"上加上\",{\"1\":{\"42\":1}}],[\"上加\",{\"1\":{\"42\":1}}],[\"上增加\",{\"1\":{\"42\":2}}],[\"上增加了\",{\"1\":{\"42\":1}}],[\"上鼓勵\",{\"1\":{\"42\":1}}],[\"上會發生\",{\"1\":{\"23\":1}}],[\"上訓練\",{\"1\":{\"50\":1,\"51\":1}}],[\"上訓練一個模型\",{\"1\":{\"20\":1}}],[\"上訓練的模型難以直接\",{\"1\":{\"16\":1}}],[\"上\",{\"1\":{\"16\":1,\"21\":2,\"35\":1,\"49\":1,\"53\":1}}],[\"上也許我們能夠對各種物件去做標記\",{\"1\":{\"16\":1}}],[\"上的認知\",{\"1\":{\"20\":1}}],[\"上的\",{\"1\":{\"19\":1}}],[\"上的時候\",{\"1\":{\"17\":1}}],[\"上的例子\",{\"1\":{\"16\":1}}],[\"上的分數大概是\",{\"1\":{\"9\":1}}],[\"上看起來很棒\",{\"1\":{\"8\":1}}],[\"結果相當糟糕\",{\"1\":{\"17\":1}}],[\"結果\",{\"0\":{\"9\":1}}],[\"如此一來\",{\"1\":{\"24\":1,\"49\":1}}],[\"如此一來就可以得到完整的輸出結果\",{\"1\":{\"8\":1}}],[\"如果出現道路或甚至機車\",{\"1\":{\"20\":1}}],[\"如果我們想要訓練一個模型去做自駕車的街景物件偵測\",{\"1\":{\"16\":1}}],[\"如果只會問那些固定的問題的話\",{\"1\":{\"6\":1}}],[\"這個參數會漸漸趨近於\",{\"1\":{\"54\":1}}],[\"這個\",{\"1\":{\"33\":1}}],[\"這些普遍做得不錯的\",{\"1\":{\"31\":1}}],[\"這跟前面提到只使用\",{\"1\":{\"23\":1}}],[\"這種相似的\",{\"1\":{\"23\":1}}],[\"這種\",{\"1\":{\"21\":2}}],[\"這種狀況下訓練模型就被稱為半監督式學習\",{\"1\":{\"20\":1}}],[\"這種差距被描述為\",{\"1\":{\"16\":1}}],[\"這類的\",{\"1\":{\"19\":1}}],[\"這樣的問題只在\",{\"1\":{\"23\":1}}],[\"這樣的做法下每一個\",{\"1\":{\"49\":1}}],[\"這樣的做法有趣的是能夠將\",{\"1\":{\"21\":1}}],[\"這樣的做法之所以可行\",{\"1\":{\"19\":1}}],[\"這樣所需要的成本會過大\",{\"1\":{\"16\":1}}],[\"這就像是同理心\",{\"1\":{\"17\":1}}],[\"這次大概是第一次實際碰\",{\"1\":{\"11\":1}}],[\"這七個問題\",{\"1\":{\"8\":1}}],[\"這場比賽是一組四人的比賽\",{\"1\":{\"4\":1}}],[\"簡單來說\",{\"1\":{\"8\":1}}],[\"簡單的\",{\"1\":{\"4\":1}}],[\"答案產出\",{\"0\":{\"8\":1}}],[\"交叉做了一些\",{\"1\":{\"7\":1}}],[\"是把一個可訓練參數拆成\",{\"1\":{\"54\":1}}],[\"是為了採用底下的特性方便後續\",{\"1\":{\"48\":1}}],[\"是使用\",{\"1\":{\"46\":1}}],[\"是被固定的參數\",{\"1\":{\"43\":1}}],[\"是上一個\",{\"1\":{\"43\":1}}],[\"是在\",{\"1\":{\"42\":1,\"50\":1,\"51\":1}}],[\"是只有使用\",{\"1\":{\"31\":1}}],[\"是\",{\"1\":{\"26\":1,\"51\":2}}],[\"是一樣的\",{\"1\":{\"33\":1}}],[\"是一種\",{\"1\":{\"21\":1}}],[\"是一個讓人很喜歡的環境\",{\"1\":{\"11\":1}}],[\"是源自於即便\",{\"1\":{\"19\":1}}],[\"是否有比較好\",{\"1\":{\"7\":1}}],[\"是也可以都\",{\"1\":{\"6\":1}}],[\"實驗是做在\",{\"1\":{\"53\":1}}],[\"實驗設定\",{\"0\":{\"26\":1}}],[\"實驗上為了檢測拿掉兩個\",{\"1\":{\"7\":1}}],[\"實際上跟\",{\"1\":{\"45\":1}}],[\"實際上我們挑了最小的\",{\"1\":{\"6\":1}}],[\"實際上在辦公室的時間沒有想像中的還要多\",{\"1\":{\"6\":1}}],[\"吐出更多結果\",{\"1\":{\"7\":1}}],[\"但\",{\"1\":{\"31\":1}}],[\"但透過剪貼則可以造成不同環境的突兀感\",{\"1\":{\"21\":1}}],[\"但在邊界上往往還是難以有好的結果\",{\"1\":{\"20\":1}}],[\"但主要的問題來自於\",{\"1\":{\"20\":1}}],[\"但是這種做法實際上效果很糟糕\",{\"1\":{\"23\":1}}],[\"但是並不全面\",{\"1\":{\"20\":1}}],[\"但是對於真實世界\",{\"1\":{\"16\":1}}],[\"但總結來說這次有還蠻有趣的體驗\",{\"1\":{\"11\":1}}],[\"但實際上有顏色才對\",{\"1\":{\"7\":1}}],[\"但基本上都不會太難\",{\"1\":{\"4\":1}}],[\"彩度跟亮度跟環境有些落差\",{\"1\":{\"7\":1}}],[\"此外\",{\"1\":{\"7\":1,\"10\":1,\"54\":1}}],[\"份\",{\"1\":{\"7\":1}}],[\"57\",{\"1\":{\"53\":1}}],[\"5​\",{\"1\":{\"51\":1}}],[\"55\",{\"1\":{\"33\":1}}],[\"53\",{\"1\":{\"33\":1}}],[\"59\",{\"1\":{\"9\":1}}],[\"5\",{\"1\":{\"7\":1,\"9\":2}}],[\"500分\",{\"1\":{\"2\":1}}],[\"處理上花了不少的時間\",{\"1\":{\"7\":1}}],[\"處理\",{\"0\":{\"7\":1},\"1\":{\"17\":1}}],[\"活下來\",{\"1\":{\"6\":1}}],[\"活動\",{\"1\":{\"3\":1}}],[\"活動參與\",{\"0\":{\"3\":1}}],[\"估計也是沒救\",{\"1\":{\"6\":1}}],[\"overhead\",{\"1\":{\"50\":1,\"51\":1}}],[\"optimal\",{\"1\":{\"43\":1,\"45\":1}}],[\"openai\",{\"1\":{\"42\":3,\"48\":1}}],[\"olivier\",{\"1\":{\"40\":1}}],[\"olsson\",{\"1\":{\"21\":2}}],[\"or\",{\"0\":{\"20\":1},\"1\":{\"30\":1}}],[\"online\",{\"1\":{\"46\":1}}],[\"on\",{\"1\":{\"15\":1,\"26\":2,\"34\":1,\"35\":1,\"46\":1,\"55\":1}}],[\"ocr\",{\"1\":{\"10\":1}}],[\"ouo\",{\"1\":{\"8\":1}}],[\"output\",{\"1\":{\"35\":1}}],[\"out\",{\"1\":{\"6\":1}}],[\"off\",{\"1\":{\"46\":2,\"55\":1}}],[\"of\",{\"1\":{\"6\":1,\"15\":2}}],[\"丟進去訓練還是出現\",{\"1\":{\"6\":1}}],[\"模型會傾向回答\",{\"1\":{\"7\":1}}],[\"模型\",{\"1\":{\"6\":1}}],[\"看\",{\"1\":{\"20\":1}}],[\"看看結果如何\",{\"1\":{\"6\":1}}],[\"看圖說故事\",{\"1\":{\"5\":1}}],[\"都有正面的影響\",{\"1\":{\"53\":1}}],[\"都需要\",{\"1\":{\"49\":1}}],[\"都不太好\",{\"1\":{\"32\":1}}],[\"都是會逐漸趨近於\",{\"1\":{\"54\":1}}],[\"都是要對\",{\"1\":{\"48\":1}}],[\"都是\",{\"1\":{\"31\":1,\"45\":1}}],[\"都是虛擬世界當中的影像\",{\"1\":{\"27\":1}}],[\"都會帶來\",{\"1\":{\"53\":1}}],[\"都會對到\",{\"1\":{\"30\":1}}],[\"都會特別大\",{\"1\":{\"20\":1}}],[\"都可以使用\",{\"1\":{\"26\":1}}],[\"都被其他\",{\"1\":{\"23\":1}}],[\"都能夠透過\",{\"1\":{\"20\":1}}],[\"都完成了\",{\"1\":{\"10\":1}}],[\"都覺得很好理解\",{\"1\":{\"6\":1}}],[\"都相當地好吃\",{\"1\":{\"6\":1}}],[\"其中\",{\"1\":{\"49\":1}}],[\"其中一半的\",{\"1\":{\"21\":1}}],[\"其方法與這一篇可說是大同小異\",{\"1\":{\"42\":1}}],[\"其他絕大多都是\",{\"1\":{\"20\":1}}],[\"其他像是路燈\",{\"1\":{\"19\":1}}],[\"其他組的報告我們也可以透過實時的直播去看\",{\"1\":{\"10\":1}}],[\"其他的\",{\"1\":{\"7\":1,\"31\":1}}],[\"其他的大概就是\",{\"1\":{\"6\":1}}],[\"其實\",{\"1\":{\"10\":1,\"54\":1}}],[\"其實不太好啦xd\",{\"1\":{\"9\":1}}],[\"其實讀\",{\"1\":{\"6\":1}}],[\"說明\",{\"1\":{\"6\":1}}],[\"自己的\",{\"1\":{\"6\":1}}],[\"v\",{\"1\":{\"45\":5}}],[\"variational\",{\"1\":{\"56\":1}}],[\"varied\",{\"1\":{\"35\":1}}],[\"van\",{\"1\":{\"44\":1}}],[\"value\",{\"1\":{\"43\":2,\"45\":1,\"46\":2,\"56\":1}}],[\"validation\",{\"1\":{\"8\":1,\"33\":4}}],[\"veg\",{\"1\":{\"31\":1}}],[\"volvo\",{\"1\":{\"15\":1}}],[\"v2\",{\"1\":{\"6\":1,\"26\":1,\"31\":1}}],[\"videos\",{\"1\":{\"42\":1}}],[\"virtual\",{\"1\":{\"30\":1}}],[\"viktor\",{\"1\":{\"21\":2}}],[\"via\",{\"0\":{\"14\":1,\"24\":1},\"1\":{\"35\":2,\"56\":1}}],[\"vime\",{\"1\":{\"56\":1}}],[\"vim\",{\"1\":{\"1\":1}}],[\"vision\",{\"1\":{\"0\":1,\"1\":1,\"15\":1,\"35\":1},\"2\":{\"37\":1}}],[\"大致上大家看過了幾個\",{\"1\":{\"6\":1}}],[\"大概是人生第一次走進台積辦公室\",{\"1\":{\"4\":1}}],[\"去訓練\",{\"1\":{\"48\":1}}],[\"去訓練顯然很糟糕\",{\"1\":{\"31\":1,\"32\":1}}],[\"去決定要加怎樣的\",{\"1\":{\"48\":1}}],[\"去近似\",{\"1\":{\"48\":1}}],[\"去加上\",{\"1\":{\"48\":1}}],[\"去儲存\",{\"1\":{\"46\":1}}],[\"去避免訓練資料上的強關聯性\",{\"1\":{\"46\":1}}],[\"去試圖得到\",{\"1\":{\"43\":1}}],[\"去學習\",{\"1\":{\"43\":1,\"45\":1}}],[\"去增加\",{\"1\":{\"40\":1}}],[\"去\",{\"1\":{\"24\":1,\"31\":1}}],[\"去轉移出來\",{\"1\":{\"20\":1}}],[\"去判別現在給我的究竟是\",{\"1\":{\"19\":1}}],[\"去拉近\",{\"1\":{\"19\":1}}],[\"去解決也許會比\",{\"1\":{\"6\":1}}],[\"去解決這個問題\",{\"1\":{\"5\":1}}],[\"去調查看看有哪些其他還不錯的\",{\"1\":{\"6\":1}}],[\"覺得很開心\",{\"1\":{\"6\":1}}],[\"飲料\",{\"1\":{\"6\":1}}],[\"超級懷疑這個\",{\"1\":{\"7\":1}}],[\"超級舒服的人體工學椅\",{\"1\":{\"6\":1}}],[\"超大的雙螢幕\",{\"1\":{\"6\":1}}],[\"電動的升降桌\",{\"1\":{\"6\":1}}],[\"點心\",{\"1\":{\"6\":1}}],[\"點半再來報到\",{\"1\":{\"6\":1}}],[\"點就要回家\",{\"1\":{\"6\":1}}],[\"98\",{\"1\":{\"33\":1}}],[\"9400\",{\"1\":{\"30\":1}}],[\"9\",{\"1\":{\"6\":1}}],[\"隔天早上\",{\"1\":{\"6\":1}}],[\"68\",{\"1\":{\"33\":1}}],[\"6\",{\"1\":{\"6\":1}}],[\"600分\",{\"1\":{\"2\":1}}],[\"一次\",{\"1\":{\"53\":1}}],[\"一樣好\",{\"1\":{\"53\":1}}],[\"一樣糟\",{\"1\":{\"53\":1}}],[\"一樣\",{\"1\":{\"26\":1}}],[\"一起\",{\"1\":{\"24\":1}}],[\"一個常見的問題是產出的結果通常會傾向去預測結果為常見的\",{\"1\":{\"20\":1}}],[\"一個簡單的方法是想辦法給這些\",{\"1\":{\"20\":1}}],[\"一個喜愛資訊領域的人\",{\"1\":{\"0\":1}}],[\"一開始進去到辦公室的感覺就很舒服\",{\"1\":{\"6\":1}}],[\"一些\",{\"1\":{\"5\":1,\"20\":1}}],[\"運算及儲存資源\",{\"1\":{\"5\":1}}],[\"寫了什麼\",{\"1\":{\"5\":1}}],[\"有多好多壞\",{\"1\":{\"45\":1}}],[\"有多少人沒戴安全帽\",{\"1\":{\"5\":1}}],[\"有多少人有戴安全帽\",{\"1\":{\"5\":1}}],[\"有更好的效果\",{\"1\":{\"44\":1}}],[\"有更多的\",{\"1\":{\"42\":1}}],[\"有提及一個使用\",{\"1\":{\"40\":1}}],[\"有不少人最後給的結果之所以那麼好看是因為\",{\"1\":{\"33\":1}}],[\"有兩列分別表示\",{\"1\":{\"32\":1}}],[\"有點偏以及\",{\"1\":{\"31\":1}}],[\"有還不錯的\",{\"1\":{\"31\":1}}],[\"有一些常見的\",{\"1\":{\"27\":1}}],[\"有可能就被誤判成人行道\",{\"1\":{\"20\":1}}],[\"有部分的認知\",{\"1\":{\"20\":1}}],[\"有相當大的差異\",{\"1\":{\"19\":1}}],[\"有些甚至是遞增的\",{\"1\":{\"54\":1}}],[\"有些是\",{\"1\":{\"32\":1}}],[\"有些\",{\"1\":{\"32\":1}}],[\"有些組別完成度很高\",{\"1\":{\"10\":1}}],[\"有些圖片上面會有\",{\"1\":{\"5\":1}}],[\"有沒有任何人違反了\",{\"1\":{\"5\":1}}],[\"安全帽是甚麼顏色\",{\"1\":{\"5\":1}}],[\"希望我們可以去找到\",{\"1\":{\"5\":1}}],[\"希望透過這個\",{\"1\":{\"0\":1}}],[\"的曲線也可以發現到在不同的遊戲當中他們的更新曲線相當地不同\",{\"1\":{\"54\":1}}],[\"的數量\",{\"1\":{\"51\":2}}],[\"的形式\",{\"1\":{\"50\":2}}],[\"的定義如下\",{\"1\":{\"50\":1}}],[\"的部分在退步也是有幾項退步蠻多\",{\"1\":{\"53\":1}}],[\"的部分改用\",{\"1\":{\"50\":1,\"51\":1}}],[\"的部分主要是參考各個\",{\"1\":{\"6\":1}}],[\"的計算\",{\"1\":{\"48\":1}}],[\"的想法基本上是相同方向\",{\"1\":{\"48\":1}}],[\"的想法跟\",{\"1\":{\"48\":1}}],[\"的算式\",{\"1\":{\"46\":1}}],[\"的影響力\",{\"1\":{\"46\":1}}],[\"的影響程度\",{\"1\":{\"46\":1}}],[\"的論文\",{\"1\":{\"46\":2}}],[\"的參數加上\",{\"1\":{\"48\":1}}],[\"的參數\",{\"1\":{\"46\":1}}],[\"的方式\",{\"1\":{\"46\":1}}],[\"的方法上雖然任何\",{\"1\":{\"26\":1}}],[\"的方法\",{\"1\":{\"23\":1,\"40\":1,\"54\":1}}],[\"的方法來降低這種問題\",{\"1\":{\"20\":1}}],[\"的方法解決了\",{\"1\":{\"20\":1}}],[\"的概念就如同火影忍者的影分身之術\",{\"1\":{\"46\":1}}],[\"的概念仍然是透過\",{\"1\":{\"45\":1}}],[\"的總和就能夠得到\",{\"1\":{\"45\":1}}],[\"的決定上採用了\",{\"1\":{\"43\":1,\"45\":1}}],[\"的限制\",{\"1\":{\"42\":1,\"45\":1}}],[\"的效果\",{\"1\":{\"42\":1}}],[\"的亂度越高越好\",{\"1\":{\"42\":1}}],[\"的時候都是透過增加\",{\"1\":{\"42\":1}}],[\"的各種\",{\"1\":{\"41\":1}}],[\"的版本\",{\"1\":{\"32\":1}}],[\"的版本是少了\",{\"1\":{\"30\":1}}],[\"的設定上對於目標被發現存在高估的問題\",{\"1\":{\"44\":1}}],[\"的設定上參考了許多過去的研究\",{\"1\":{\"26\":1}}],[\"的設定基本上跟\",{\"1\":{\"26\":1}}],[\"的設計上也相當直覺\",{\"1\":{\"24\":1}}],[\"的結果要接近\",{\"1\":{\"24\":1}}],[\"的結果就當作是他的\",{\"1\":{\"20\":1}}],[\"的關聯性就能被連結起來\",{\"1\":{\"24\":1}}],[\"的核心做法是不單只是跟\",{\"1\":{\"24\":1}}],[\"的做法\",{\"1\":{\"49\":1}}],[\"的做法就是照著\",{\"1\":{\"23\":1}}],[\"的做法之所以能夠成功\",{\"1\":{\"19\":1}}],[\"的步驟\",{\"1\":{\"21\":1}}],[\"的一種\",{\"1\":{\"21\":1}}],[\"的技巧\",{\"1\":{\"21\":1}}],[\"的例子\",{\"1\":{\"20\":1}}],[\"的認識\",{\"1\":{\"20\":1}}],[\"的資料不存在任何\",{\"1\":{\"20\":1}}],[\"的資料上只有一些\",{\"1\":{\"20\":1}}],[\"的不同\",{\"1\":{\"19\":1}}],[\"的預測結果要接近\",{\"1\":{\"24\":1}}],[\"的預測結果\",{\"1\":{\"19\":1}}],[\"的狀況缺乏認知\",{\"1\":{\"17\":1}}],[\"的模型都跟\",{\"1\":{\"53\":1}}],[\"的模型\",{\"1\":{\"31\":1}}],[\"的模型對於\",{\"1\":{\"17\":1}}],[\"的模型雖然有許多\",{\"1\":{\"17\":1}}],[\"的目標是把兩個不同分佈的\",{\"1\":{\"16\":1}}],[\"的比賽經驗很不錯\",{\"1\":{\"11\":1}}],[\"的輸入\",{\"1\":{\"10\":1}}],[\"的大家都很友善\",{\"1\":{\"10\":1}}],[\"的操作\",{\"1\":{\"8\":1}}],[\"的實用程度\",{\"1\":{\"7\":1}}],[\"的研究\",{\"1\":{\"6\":1}}],[\"的回覆\",{\"1\":{\"6\":1}}],[\"的\",{\"1\":{\"6\":1,\"11\":1,\"19\":2,\"24\":1,\"26\":1,\"29\":1,\"30\":1,\"32\":1,\"40\":1,\"42\":1,\"43\":2,\"46\":1,\"48\":2,\"54\":2}}],[\"的敘述\",{\"1\":{\"5\":1}}],[\"的題目\",{\"1\":{\"5\":1}}],[\"的共同創辦人之一\",{\"1\":{\"0\":1}}],[\"differences\",{\"1\":{\"56\":1}}],[\"distributed\",{\"0\":{\"51\":1}}],[\"distribution\",{\"1\":{\"17\":1,\"43\":1}}],[\"discriminator\",{\"1\":{\"19\":1}}],[\"disk\",{\"1\":{\"5\":1}}],[\"deterministic\",{\"1\":{\"54\":2}}],[\"details\",{\"1\":{\"51\":1}}],[\"deepmind\",{\"1\":{\"39\":1}}],[\"deeplab\",{\"1\":{\"26\":1,\"31\":1}}],[\"deep\",{\"1\":{\"0\":1,\"56\":2}}],[\"dθv​←dθv​+∂\",{\"1\":{\"46\":1}}],[\"dθ←dθ+∇θ\",{\"1\":{\"46\":1}}],[\"d\",{\"1\":{\"43\":1}}],[\"dueling\",{\"0\":{\"45\":1,\"50\":1},\"1\":{\"41\":1,\"45\":3,\"50\":2,\"53\":2}}],[\"double\",{\"0\":{\"44\":1},\"1\":{\"41\":1,\"44\":4,\"45\":1}}],[\"domain\",{\"0\":{\"14\":2,\"16\":1,\"19\":1,\"24\":2},\"1\":{\"16\":14,\"17\":5,\"19\":7,\"20\":10,\"23\":2,\"24\":6,\"31\":2,\"32\":1,\"35\":5},\"2\":{\"37\":1}}],[\"dqn\",{\"0\":{\"43\":1,\"44\":1,\"45\":1,\"50\":2},\"1\":{\"41\":3,\"43\":1,\"44\":8,\"45\":6,\"46\":1,\"50\":4,\"53\":3}}],[\"dt​\",{\"1\":{\"24\":1}}],[\"ds​\",{\"1\":{\"24\":1}}],[\"david\",{\"1\":{\"44\":1}}],[\"day\",{\"1\":{\"35\":2}}],[\"data\",{\"1\":{\"16\":1,\"17\":2,\"19\":2,\"20\":7,\"21\":2,\"35\":1}}],[\"dataset\",{\"0\":{\"27\":1},\"1\":{\"7\":3,\"23\":6}}],[\"datasets\",{\"0\":{\"7\":1},\"1\":{\"5\":1,\"7\":2}}],[\"dacs\",{\"0\":{\"14\":1,\"24\":1},\"1\":{\"19\":1,\"21\":1,\"24\":1,\"31\":1,\"32\":1,\"33\":3,\"35\":1}}],[\"dp\",{\"1\":{\"4\":1}}],[\"前就已經做了不少\",{\"1\":{\"6\":1}}],[\"前面有一個預賽\",{\"1\":{\"4\":1}}],[\"前幾天去參加了\",{\"1\":{\"4\":1}}],[\"台積電的黑客松\",{\"1\":{\"4\":1}}],[\"臺灣好厲駭\",{\"1\":{\"3\":1}}],[\"improvement\",{\"1\":{\"53\":2}}],[\"images\",{\"1\":{\"28\":1,\"29\":1,\"30\":1}}],[\"imagenet\",{\"1\":{\"26\":1}}],[\"image\",{\"1\":{\"16\":2,\"17\":2,\"19\":2,\"20\":2,\"21\":5,\"23\":2,\"24\":2,\"28\":1,\"29\":1,\"30\":1,\"31\":2,\"32\":1,\"42\":1,\"43\":1,\"44\":1,\"45\":1,\"46\":1,\"53\":3,\"54\":1}}],[\"i=0∑k​eπ\",{\"1\":{\"51\":1}}],[\"i=0∑k​∇ζπ​​log\",{\"1\":{\"51\":1}}],[\"i=0∑k​∇θπ​​log\",{\"1\":{\"46\":1,\"51\":1}}],[\"i\",{\"1\":{\"46\":1}}],[\"iclr\",{\"1\":{\"39\":1,\"42\":1},\"2\":{\"58\":1}}],[\"issues\",{\"0\":{\"33\":1}}],[\"is\",{\"0\":{\"16\":1}}],[\"isip\",{\"1\":{\"3\":1}}],[\"input\",{\"1\":{\"51\":2}}],[\"initialize\",{\"1\":{\"51\":1}}],[\"independent\",{\"1\":{\"49\":1,\"51\":2}}],[\"in\",{\"1\":{\"34\":1,\"35\":1}}],[\"introduce\",{\"1\":{\"34\":1}}],[\"info\",{\"1\":{\"17\":1,\"19\":1,\"20\":4,\"23\":1,\"31\":1,\"32\":1,\"40\":1,\"42\":1,\"45\":1,\"46\":1}}],[\"information\",{\"0\":{\"15\":1,\"39\":1},\"1\":{\"56\":1}}],[\"instructblip\",{\"1\":{\"6\":1}}],[\"it\",{\"1\":{\"11\":1}}],[\"ioicamp\",{\"1\":{\"3\":1}}],[\"ioncamp\",{\"1\":{\"3\":1}}],[\"奧義科技參訪\",{\"1\":{\"3\":1}}],[\"課程講師\",{\"1\":{\"3\":2}}],[\"索拉教育\",{\"1\":{\"3\":2}}],[\"班講師\",{\"1\":{\"3\":1}}],[\"資訊之芽\",{\"1\":{\"3\":1}}],[\"資訊安全\",{\"1\":{\"0\":1}}],[\"營長\",{\"1\":{\"3\":1}}],[\"講師\",{\"1\":{\"3\":1}}],[\"日語學習小組\",{\"1\":{\"3\":1}}],[\"日期\",{\"1\":{\"2\":1,\"3\":1}}],[\"成新的\",{\"1\":{\"23\":1}}],[\"成員\",{\"1\":{\"3\":2}}],[\"成績\",{\"1\":{\"2\":1}}],[\"機器學習讀書會\",{\"1\":{\"3\":1}}],[\"筆記\",{\"1\":{\"3\":3}}],[\"8\",{\"1\":{\"56\":1}}],[\"84\",{\"1\":{\"33\":1}}],[\"83\",{\"1\":{\"33\":1}}],[\"800\",{\"1\":{\"2\":1}}],[\"81\",{\"1\":{\"1\":1}}],[\"1m\",{\"1\":{\"53\":1}}],[\"1\",{\"1\":{\"33\":1}}],[\"17\",{\"1\":{\"33\":1}}],[\"13\",{\"1\":{\"30\":2,\"32\":2,\"33\":1}}],[\"13b\",{\"1\":{\"6\":1}}],[\"16\",{\"1\":{\"30\":1,\"32\":2,\"33\":1}}],[\"19\",{\"1\":{\"28\":1,\"29\":1}}],[\"128\",{\"1\":{\"2\":1}}],[\"108k\",{\"1\":{\"53\":1}}],[\"101\",{\"1\":{\"31\":1}}],[\"100×max\",{\"1\":{\"53\":1}}],[\"100×scorehuman​−scorerandom​scoreagent​−scorerandom​​\",{\"1\":{\"53\":1}}],[\"100\",{\"1\":{\"5\":3,\"53\":1}}],[\"100分\",{\"1\":{\"2\":3}}],[\"10\",{\"1\":{\"2\":1,\"33\":1}}],[\"世界第三名\",{\"1\":{\"2\":1}}],[\"hasselt\",{\"1\":{\"44\":1}}],[\"hado\",{\"1\":{\"44\":1}}],[\"hackday\",{\"1\":{\"6\":3}}],[\"hackdoor\",{\"1\":{\"3\":1}}],[\"high\",{\"1\":{\"34\":1}}],[\"hitcon\",{\"1\":{\"3\":2}}],[\"hyperparameters\",{\"1\":{\"33\":1}}],[\"hyperparameter\",{\"1\":{\"26\":1}}],[\"h\",{\"1\":{\"24\":2,\"46\":2}}],[\"hsuan\",{\"1\":{\"19\":2,\"26\":1}}],[\"human\",{\"1\":{\"2\":1,\"53\":2}}],[\"hexo\",{\"1\":{\"1\":1}}],[\"金盾獎\",{\"1\":{\"2\":1}}],[\"全國第六名\",{\"1\":{\"2\":1}}],[\"35\",{\"1\":{\"33\":1}}],[\"314\",{\"1\":{\"2\":1}}],[\"300分\",{\"1\":{\"2\":1}}],[\"300\",{\"1\":{\"2\":1}}],[\"30\",{\"1\":{\"2\":1}}],[\"2​\",{\"1\":{\"51\":1}}],[\"2∣st+i​\",{\"1\":{\"46\":1,\"51\":1}}],[\"25\",{\"1\":{\"35\":1}}],[\"2975\",{\"1\":{\"28\":1}}],[\"2\",{\"1\":{\"9\":1,\"43\":1,\"45\":1,\"46\":1,\"50\":4}}],[\"24966\",{\"1\":{\"29\":1}}],[\"24\",{\"1\":{\"5\":1,\"35\":1}}],[\"2017\",{\"1\":{\"42\":1}}],[\"2014\",{\"1\":{\"40\":1}}],[\"2016\",{\"1\":{\"28\":1,\"30\":1}}],[\"2018\",{\"1\":{\"17\":1,\"19\":2,\"20\":1,\"26\":1,\"39\":1,\"53\":3,\"54\":1}}],[\"2015\",{\"1\":{\"17\":1,\"43\":1,\"44\":1,\"45\":1}}],[\"2019年03月\",{\"1\":{\"3\":1}}],[\"2019年08月\",{\"1\":{\"3\":2}}],[\"2019年06月\",{\"1\":{\"2\":1}}],[\"2019年07月\",{\"1\":{\"2\":1,\"3\":2}}],[\"2019年10月\",{\"1\":{\"2\":1}}],[\"2019年11月\",{\"1\":{\"2\":1,\"3\":1}}],[\"2019年12月\",{\"1\":{\"2\":1}}],[\"2019\",{\"1\":{\"2\":1,\"3\":3}}],[\"2021\",{\"1\":{\"15\":1,\"20\":1}}],[\"2021年\",{\"1\":{\"3\":1}}],[\"2023年09月\",{\"1\":{\"3\":1}}],[\"2023年04月\",{\"1\":{\"3\":1}}],[\"2020年\",{\"1\":{\"3\":1}}],[\"2020年08月\",{\"1\":{\"3\":2}}],[\"2020年09月\",{\"1\":{\"3\":1}}],[\"2020年01月\",{\"1\":{\"2\":1,\"3\":1}}],[\"2020年02月\",{\"1\":{\"2\":1}}],[\"2020年03月\",{\"1\":{\"2\":2}}],[\"2020年04月\",{\"1\":{\"2\":1}}],[\"2020年06月\",{\"1\":{\"2\":2}}],[\"2020年07月\",{\"1\":{\"2\":1,\"3\":2}}],[\"2020\",{\"1\":{\"2\":1,\"3\":3,\"15\":1,\"21\":2}}],[\"2022年08月\",{\"1\":{\"3\":1}}],[\"2022年07月\",{\"1\":{\"2\":1,\"3\":2}}],[\"2022年\",{\"1\":{\"3\":1}}],[\"2022年10月\",{\"1\":{\"2\":1,\"3\":1}}],[\"2024\",{\"0\":{\"4\":1},\"1\":{\"2\":1,\"4\":1}}],[\"2024年01月\",{\"1\":{\"2\":1}}],[\"212\",{\"1\":{\"2\":1}}],[\"21\",{\"1\":{\"2\":1}}],[\"r−v\",{\"1\":{\"46\":3}}],[\"r+γq\",{\"1\":{\"44\":1,\"45\":1,\"50\":2}}],[\"r+γmaxb∈a​q\",{\"1\":{\"44\":1}}],[\"r+γb∈amax​q\",{\"1\":{\"43\":1,\"50\":2}}],[\"r=r\",{\"1\":{\"43\":1}}],[\"randomized\",{\"1\":{\"56\":1}}],[\"random\",{\"0\":{\"49\":1},\"1\":{\"49\":4,\"50\":1,\"51\":1,\"53\":1}}],[\"randomness\",{\"1\":{\"40\":1}}],[\"ram\",{\"1\":{\"5\":2}}],[\"rl\",{\"1\":{\"40\":1,\"41\":1,\"46\":1,\"55\":1}}],[\"richter\",{\"1\":{\"29\":1}}],[\"rider\",{\"1\":{\"23\":1}}],[\"r\",{\"1\":{\"29\":1,\"43\":1,\"45\":1,\"50\":4}}],[\"road\",{\"1\":{\"23\":1,\"31\":1,\"32\":1}}],[\"round\",{\"1\":{\"2\":3}}],[\"reply\",{\"1\":{\"46\":1}}],[\"replay\",{\"1\":{\"43\":3,\"46\":1}}],[\"return\",{\"1\":{\"45\":1,\"46\":1}}],[\"reward\",{\"1\":{\"43\":1}}],[\"review\",{\"1\":{\"42\":1}}],[\"reverse\",{\"1\":{\"1\":1}}],[\"regularization\",{\"1\":{\"40\":1,\"41\":1,\"42\":1,\"46\":1}}],[\"resnet\",{\"1\":{\"31\":1}}],[\"resnet101\",{\"1\":{\"26\":2}}],[\"results\",{\"0\":{\"25\":1,\"52\":1}}],[\"research\",{\"1\":{\"6\":1}}],[\"related\",{\"0\":{\"18\":1,\"41\":1}}],[\"release\",{\"1\":{\"15\":1}}],[\"read\",{\"2\":{\"37\":1,\"58\":1}}],[\"real\",{\"1\":{\"17\":1,\"19\":1,\"27\":1}}],[\"react\",{\"1\":{\"1\":1}}],[\"reinforcement\",{\"1\":{\"0\":1,\"1\":1,\"56\":3},\"2\":{\"58\":1}}],[\"guez\",{\"1\":{\"44\":1}}],[\"games\",{\"1\":{\"53\":1}}],[\"gaussian\",{\"1\":{\"42\":1,\"48\":1,\"49\":2,\"50\":1,\"51\":3}}],[\"gan\",{\"1\":{\"19\":1}}],[\"go\",{\"1\":{\"40\":1}}],[\"google\",{\"1\":{\"2\":1,\"39\":1}}],[\"gheshlaghi\",{\"1\":{\"39\":1,\"53\":3,\"54\":1}}],[\"geist\",{\"1\":{\"40\":1}}],[\"germanros\",{\"1\":{\"30\":1}}],[\"generator\",{\"1\":{\"19\":1}}],[\"gta5\",{\"0\":{\"29\":1,\"31\":1},\"1\":{\"27\":2,\"29\":1,\"33\":1,\"34\":1}}],[\"gb\",{\"1\":{\"5\":4}}],[\"gpu\",{\"1\":{\"5\":1}}],[\"gcp\",{\"1\":{\"5\":1}}],[\"graph\",{\"1\":{\"4\":1}}],[\"greedy\",{\"1\":{\"4\":1,\"40\":1,\"41\":1,\"42\":2,\"43\":1,\"45\":1,\"50\":1}}],[\"g\",{\"1\":{\"2\":1}}],[\"git\",{\"1\":{\"1\":1}}],[\"第八屆成功大學大學生活體驗營\",{\"1\":{\"3\":1}}],[\"第七屆高一生程式設計排名賽\",{\"1\":{\"2\":1}}],[\"第7名\",{\"1\":{\"2\":1}}],[\"第62名\",{\"1\":{\"2\":1}}],[\"第5695名\",{\"1\":{\"2\":1}}],[\"第2427名\",{\"1\":{\"2\":1}}],[\"第29755名\",{\"1\":{\"2\":1}}],[\"第22名\",{\"1\":{\"2\":1}}],[\"第96名\",{\"1\":{\"2\":1}}],[\"eπ\",{\"1\":{\"51\":1}}],[\"e\",{\"1\":{\"50\":2}}],[\"episode\",{\"1\":{\"48\":2,\"49\":1,\"53\":1}}],[\"experiments\",{\"0\":{\"53\":1}}],[\"experience\",{\"1\":{\"43\":2,\"46\":1}}],[\"exploration\",{\"0\":{\"38\":1,\"42\":1},\"1\":{\"40\":2,\"41\":1,\"42\":5,\"45\":1,\"46\":1,\"48\":1,\"54\":1,\"55\":1,\"56\":4}}],[\"exam\",{\"1\":{\"2\":1}}],[\"evolution\",{\"1\":{\"56\":1}}],[\"everything\",{\"1\":{\"35\":1}}],[\"evaluation\",{\"0\":{\"33\":1}}],[\"early\",{\"1\":{\"33\":1}}],[\"entropy\",{\"1\":{\"24\":1,\"40\":1,\"41\":1,\"42\":1,\"46\":3,\"51\":1}}],[\"english\",{\"1\":{\"1\":1}}],[\"engineering\",{\"1\":{\"1\":1}}],[\"et\",{\"1\":{\"17\":2,\"19\":2,\"20\":2,\"21\":2,\"26\":1,\"28\":1,\"29\":1,\"30\":1,\"39\":1,\"43\":1,\"45\":1,\"53\":3,\"54\":1}}],[\"as\",{\"1\":{\"56\":1}}],[\"asynchronous\",{\"1\":{\"56\":2}}],[\"analysis\",{\"0\":{\"54\":1}}],[\"approximation\",{\"1\":{\"48\":1}}],[\"apply\",{\"1\":{\"16\":1,\"21\":2,\"34\":1}}],[\"applications\",{\"1\":{\"15\":1}}],[\"actor\",{\"1\":{\"46\":1,\"56\":1}}],[\"action\",{\"1\":{\"42\":5,\"43\":4,\"45\":3}}],[\"argb∈amax​q\",{\"1\":{\"45\":1,\"50\":2}}],[\"arthur\",{\"1\":{\"44\":1,\"46\":1}}],[\"a3c\",{\"0\":{\"46\":1,\"51\":1},\"1\":{\"41\":1,\"42\":1,\"46\":7,\"51\":3,\"53\":2,\"56\":1}}],[\"agents\",{\"1\":{\"56\":1}}],[\"agent\",{\"1\":{\"40\":1,\"42\":1,\"46\":1}}],[\"azar\",{\"1\":{\"39\":1,\"53\":3,\"54\":1}}],[\"augmentation\",{\"1\":{\"35\":1}}],[\"augumentation\",{\"1\":{\"21\":2}}],[\"atari\",{\"1\":{\"53\":1}}],[\"at+i​\",{\"1\":{\"46\":1,\"51\":2}}],[\"at+i​∣st+i​\",{\"1\":{\"46\":1,\"51\":2}}],[\"at\",{\"1\":{\"23\":2,\"24\":2,\"31\":2,\"32\":1}}],[\"advantage\",{\"1\":{\"45\":2,\"46\":2}}],[\"adversarial\",{\"1\":{\"19\":1,\"20\":1,\"35\":1}}],[\"adapt\",{\"1\":{\"35\":1}}],[\"adaption\",{\"0\":{\"16\":1,\"24\":1},\"1\":{\"16\":2,\"20\":1},\"2\":{\"37\":1}}],[\"adaptation\",{\"0\":{\"14\":1},\"1\":{\"35\":4}}],[\"alternative\",{\"1\":{\"56\":1}}],[\"alogorithm\",{\"1\":{\"46\":1}}],[\"alpha\",{\"1\":{\"40\":1}}],[\"alignment\",{\"0\":{\"19\":1},\"1\":{\"19\":1}}],[\"al\",{\"1\":{\"17\":2,\"19\":2,\"20\":2,\"21\":2,\"23\":2,\"24\":2,\"26\":1,\"28\":1,\"29\":1,\"30\":1,\"31\":2,\"32\":1,\"39\":1,\"43\":1,\"45\":1,\"53\":3,\"54\":1}}],[\"ai​∣si​\",{\"1\":{\"46\":1}}],[\"ai\",{\"1\":{\"5\":1}}],[\"ais3\",{\"1\":{\"2\":1,\"3\":2}}],[\"a\",{\"1\":{\"2\":1,\"21\":2,\"34\":1,\"43\":5,\"45\":18,\"46\":3,\"50\":8,\"51\":2,\"56\":1}}],[\"about\",{\"0\":{\"0\":1,\"33\":1}}],[\"網頁組第3名\",{\"1\":{\"2\":1}}],[\"網路管理等領域\",{\"1\":{\"0\":1}}],[\"青年黑克松\",{\"1\":{\"2\":1}}],[\"專題第三名\",{\"1\":{\"2\":1}}],[\"f\",{\"1\":{\"49\":2}}],[\"factorised\",{\"1\":{\"49\":1,\"50\":1,\"51\":1}}],[\"functions\",{\"1\":{\"56\":1}}],[\"functiona\",{\"1\":{\"45\":1}}],[\"function\",{\"1\":{\"43\":3,\"45\":2,\"46\":4,\"51\":1}}],[\"fθ​\",{\"1\":{\"24\":3}}],[\"fence\",{\"1\":{\"30\":1}}],[\"feature\",{\"1\":{\"19\":1}}],[\"feedbacks\",{\"2\":{\"12\":1}}],[\"feedback\",{\"1\":{\"11\":1}}],[\"frames\",{\"1\":{\"53\":2}}],[\"framework\",{\"1\":{\"34\":1}}],[\"frameworks\",{\"1\":{\"1\":1}}],[\"from\",{\"1\":{\"16\":2,\"17\":2,\"19\":2,\"20\":2,\"21\":2,\"23\":2,\"24\":2,\"28\":1,\"29\":1,\"30\":1,\"31\":2,\"32\":1,\"42\":2,\"43\":1,\"44\":1,\"45\":1,\"46\":1,\"53\":3,\"54\":1}}],[\"frontend\",{\"1\":{\"10\":1}}],[\"flamingo\",{\"1\":{\"6\":1}}],[\"fintune\",{\"1\":{\"7\":1}}],[\"fine\",{\"1\":{\"6\":3,\"20\":1}}],[\"finetune\",{\"1\":{\"5\":1}}],[\"final\",{\"1\":{\"2\":1}}],[\"firstsecurity\",{\"1\":{\"3\":1}}],[\"fortunato\",{\"1\":{\"39\":1,\"53\":3,\"54\":1}}],[\"for\",{\"0\":{\"38\":1,\"42\":1},\"1\":{\"2\":1,\"20\":1,\"35\":4,\"41\":1,\"48\":1,\"56\":2}}],[\"fml\",{\"1\":{\"2\":1}}],[\"梅竹黑客松\",{\"1\":{\"2\":1}}],[\"無\",{\"1\":{\"2\":1}}],[\"truncate\",{\"1\":{\"53\":1}}],[\"transition\",{\"1\":{\"43\":1}}],[\"tranheden\",{\"1\":{\"23\":2,\"24\":2,\"31\":2,\"32\":1}}],[\"training\",{\"0\":{\"20\":1},\"1\":{\"21\":2,\"28\":1,\"29\":1,\"30\":1,\"35\":1}}],[\"train\",{\"1\":{\"6\":1,\"31\":2}}],[\"thread\",{\"1\":{\"50\":1,\"51\":1}}],[\"t\",{\"1\":{\"46\":1}}],[\"table\",{\"1\":{\"43\":1}}],[\"target\",{\"1\":{\"16\":3,\"17\":1,\"19\":2,\"20\":7,\"23\":2,\"24\":4,\"43\":1}}],[\"tips\",{\"1\":{\"19\":1,\"23\":1,\"42\":1,\"43\":1,\"44\":1,\"45\":1,\"46\":1,\"48\":1}}],[\"tsai\",{\"1\":{\"19\":2,\"26\":1}}],[\"tsmc\",{\"0\":{\"4\":1},\"1\":{\"2\":1,\"5\":1,\"10\":1,\"11\":2},\"2\":{\"13\":1}}],[\"temporal\",{\"1\":{\"56\":1}}],[\"tensorflow\",{\"1\":{\"56\":1}}],[\"testset\",{\"1\":{\"33\":1}}],[\"technology\",{\"1\":{\"15\":1}}],[\"team\",{\"1\":{\"11\":1}}],[\"tuning\",{\"1\":{\"6\":1}}],[\"tune\",{\"1\":{\"6\":3,\"20\":1}}],[\"to\",{\"0\":{\"23\":1},\"1\":{\"23\":1,\"27\":1,\"33\":2,\"34\":2,\"35\":2,\"56\":1}}],[\"toi海選\",{\"1\":{\"2\":1}}],[\"toi校內賽\",{\"1\":{\"2\":1}}],[\"toi初選\",{\"1\":{\"2\":1}}],[\"toefl\",{\"1\":{\"1\":1}}],[\"心得\",{\"0\":{\"4\":1},\"1\":{\"2\":2,\"3\":1}}],[\"競賽名稱\",{\"1\":{\"2\":1}}],[\"競賽成績\",{\"0\":{\"2\":1}}],[\"number\",{\"0\":{\"49\":1},\"1\":{\"49\":2}}],[\"net\",{\"1\":{\"41\":1,\"48\":1,\"50\":2}}],[\"networks\",{\"0\":{\"38\":1}}],[\"network\",{\"1\":{\"19\":2,\"24\":1,\"26\":1,\"40\":1,\"43\":4,\"45\":1,\"48\":2,\"50\":2,\"51\":2}}],[\"neural\",{\"1\":{\"40\":1,\"43\":3,\"45\":1}}],[\"needs\",{\"1\":{\"35\":1}}],[\"need\",{\"1\":{\"35\":1}}],[\"noise\",{\"0\":{\"42\":1},\"1\":{\"41\":1,\"42\":12,\"48\":8,\"49\":3,\"50\":1,\"51\":4,\"54\":2,\"56\":2}}],[\"noisynet\",{\"1\":{\"51\":1,\"53\":6,\"54\":1}}],[\"noisy\",{\"0\":{\"38\":1},\"1\":{\"41\":1,\"48\":1,\"50\":3,\"51\":1}}],[\"note\",{\"1\":{\"46\":2,\"53\":1},\"2\":{\"36\":1,\"57\":1}}],[\"notes\",{\"1\":{\"35\":1}}],[\"none\",{\"1\":{\"7\":1}}],[\"naive\",{\"0\":{\"23\":1},\"1\":{\"23\":2}}],[\"native\",{\"1\":{\"1\":1}}],[\"nthu\",{\"1\":{\"3\":1}}],[\"npsc決賽\",{\"1\":{\"2\":1}}],[\"n1\",{\"1\":{\"1\":1}}],[\"j​=p​0\",{\"1\":{\"51\":1}}],[\"j​=0\",{\"1\":{\"51\":1}}],[\"j​∼u\",{\"1\":{\"51\":2}}],[\"jw​ϵjb​​=f\",{\"1\":{\"49\":1}}],[\"juliani\",{\"1\":{\"46\":1}}],[\"jlpt\",{\"1\":{\"1\":1}}],[\"jam\",{\"1\":{\"2\":1}}],[\"japanese\",{\"1\":{\"1\":1}}],[\"javascripts\",{\"1\":{\"1\":1}}],[\"b∈rq\",{\"1\":{\"49\":1}}],[\"bias\",{\"1\":{\"49\":1}}],[\"bilal\",{\"1\":{\"39\":1,\"53\":3,\"54\":1}}],[\"binary\",{\"1\":{\"21\":1,\"26\":1}}],[\"better\",{\"1\":{\"42\":2,\"56\":1}}],[\"beat\",{\"1\":{\"34\":1}}],[\"best\",{\"1\":{\"33\":3}}],[\"benchmarks\",{\"1\":{\"27\":1}}],[\"buffer\",{\"1\":{\"43\":2,\"46\":1}}],[\"build\",{\"1\":{\"31\":1}}],[\"bucket\",{\"1\":{\"5\":1}}],[\"b\",{\"1\":{\"21\":2,\"43\":1,\"44\":2,\"45\":1,\"50\":4}}],[\"bonus\",{\"1\":{\"9\":1}}],[\"bot\",{\"1\":{\"1\":1}}],[\"balanced\",{\"1\":{\"35\":1}}],[\"backbone\",{\"1\":{\"26\":1,\"31\":1}}],[\"baseline\",{\"1\":{\"33\":1,\"45\":1,\"53\":3}}],[\"based\",{\"1\":{\"2\":1,\"26\":1,\"35\":1,\"56\":1}}],[\"basic\",{\"0\":{\"15\":1,\"39\":1}}],[\"bagging\",{\"1\":{\"8\":1}}],[\"blip\",{\"1\":{\"6\":1}}],[\"blog\",{\"1\":{\"0\":1}}],[\"posts\",{\"0\":{\"59\":1}}],[\"policy有什么区别\",{\"1\":{\"56\":1}}],[\"policy\",{\"1\":{\"40\":1,\"42\":1,\"46\":6,\"55\":2,\"56\":1}}],[\"pole\",{\"1\":{\"30\":1}}],[\"p\",{\"1\":{\"51\":2}}],[\"p+q\",{\"1\":{\"49\":1}}],[\"pq+q\",{\"1\":{\"49\":1}}],[\"part\",{\"1\":{\"56\":1}}],[\"parameter\",{\"0\":{\"42\":1},\"1\":{\"41\":1,\"42\":7,\"48\":2,\"56\":2}}],[\"paper\",{\"1\":{\"6\":3,\"11\":1,\"20\":1,\"32\":1,\"33\":1,\"42\":1},\"2\":{\"37\":1,\"58\":1}}],[\"pietquin\",{\"1\":{\"40\":1}}],[\"piot\",{\"1\":{\"39\":1,\"53\":3,\"54\":1}}],[\"pixel\",{\"1\":{\"19\":1}}],[\"perturbations\",{\"1\":{\"35\":1}}],[\"performance\",{\"1\":{\"31\":2,\"32\":1,\"34\":1}}],[\"person\",{\"1\":{\"23\":1,\"31\":1}}],[\"pseudo\",{\"0\":{\"20\":1},\"1\":{\"20\":3,\"21\":1,\"23\":1,\"24\":1}}],[\"private\",{\"1\":{\"9\":1}}],[\"prediction\",{\"1\":{\"20\":2}}],[\"prefix\",{\"1\":{\"7\":2}}],[\"pretrained\",{\"1\":{\"5\":1,\"26\":1}}],[\"pre\",{\"1\":{\"2\":1}}],[\"probability\",{\"1\":{\"43\":1}}],[\"proxy\",{\"1\":{\"6\":1}}],[\"project\",{\"1\":{\"2\":1}}],[\"programming\",{\"1\":{\"1\":2}}],[\"pytorch\",{\"1\":{\"1\":1}}],[\"python\",{\"1\":{\"1\":1,\"3\":1}}],[\"cityscape\",{\"1\":{\"34\":1}}],[\"cityscapes\",{\"0\":{\"28\":1,\"31\":1,\"32\":1},\"1\":{\"27\":3,\"29\":1,\"30\":1,\"33\":1}}],[\"city\",{\"1\":{\"30\":1}}],[\"cbst\",{\"1\":{\"20\":1}}],[\"classes\",{\"1\":{\"21\":1,\"28\":1,\"29\":2,\"30\":3,\"32\":1,\"33\":2}}],[\"classmix\",{\"1\":{\"21\":2,\"23\":1,\"24\":1,\"26\":1,\"34\":1,\"35\":1}}],[\"class\",{\"1\":{\"20\":2,\"23\":5,\"31\":2,\"32\":1,\"35\":1}}],[\"club\",{\"1\":{\"3\":1}}],[\"chiehchen\",{\"1\":{\"17\":1}}],[\"chalmers\",{\"1\":{\"15\":1}}],[\"cnn\",{\"1\":{\"17\":2}}],[\"critic\",{\"1\":{\"46\":1,\"56\":1}}],[\"cross\",{\"0\":{\"14\":1},\"1\":{\"24\":1,\"35\":1}}],[\"cryptography\",{\"1\":{\"1\":1}}],[\"cuda\",{\"1\":{\"6\":1}}],[\"cv\",{\"1\":{\"6\":1,\"16\":1}}],[\"cpu\",{\"1\":{\"5\":1}}],[\"carlo\",{\"1\":{\"48\":1}}],[\"car\",{\"1\":{\"31\":1}}],[\"cars\",{\"1\":{\"15\":1}}],[\"careerhack\",{\"0\":{\"4\":1},\"1\":{\"2\":1},\"2\":{\"13\":1}}],[\"camp\",{\"1\":{\"3\":2}}],[\"cs\",{\"1\":{\"3\":1}}],[\"count\",{\"1\":{\"56\":1}}],[\"cordts\",{\"1\":{\"28\":1}}],[\"corss\",{\"0\":{\"24\":1}}],[\"column\",{\"1\":{\"20\":1}}],[\"contribution\",{\"0\":{\"34\":1,\"55\":1}}],[\"context\",{\"1\":{\"19\":1}}],[\"conflation\",{\"1\":{\"23\":1}}],[\"conference\",{\"1\":{\"15\":1}}],[\"competition\",{\"1\":{\"2\":1}}],[\"computer\",{\"1\":{\"0\":1,\"1\":1,\"15\":1,\"35\":1},\"2\":{\"37\":1}}],[\"code\",{\"1\":{\"2\":1}}],[\"c++\",{\"1\":{\"1\":1,\"3\":1}}],[\"c\",{\"1\":{\"1\":1,\"3\":1}}],[\"s\",{\"1\":{\"43\":4,\"45\":16,\"50\":8}}],[\"space\",{\"0\":{\"42\":1},\"1\":{\"35\":1,\"41\":1,\"42\":8,\"48\":2,\"56\":1}}],[\"spatial\",{\"1\":{\"19\":2}}],[\"sw\",{\"1\":{\"31\":1,\"32\":1}}],[\"sky\",{\"1\":{\"31\":1}}],[\"skills\",{\"0\":{\"1\":1}}],[\"single\",{\"1\":{\"50\":1}}],[\"si​\",{\"1\":{\"46\":2}}],[\"silver\",{\"1\":{\"44\":1}}],[\"simple\",{\"1\":{\"34\":1,\"56\":1}}],[\"sidewalk\",{\"1\":{\"23\":1}}],[\"sitcon\",{\"1\":{\"3\":2}}],[\"sb​\",{\"1\":{\"21\":2}}],[\"sample\",{\"1\":{\"48\":1}}],[\"sampling\",{\"0\":{\"14\":1,\"24\":1},\"1\":{\"35\":1}}],[\"sa​\",{\"1\":{\"21\":3}}],[\"synthia\",{\"0\":{\"30\":1,\"32\":1},\"1\":{\"27\":2,\"32\":1,\"33\":1}}],[\"synthetic\",{\"1\":{\"27\":1,\"29\":1,\"30\":1}}],[\"synethic\",{\"1\":{\"16\":1,\"17\":1,\"19\":1}}],[\"sylwia\",{\"1\":{\"20\":1}}],[\"ssl\",{\"1\":{\"20\":1,\"34\":1}}],[\"supervise\",{\"1\":{\"20\":2}}],[\"supervised\",{\"1\":{\"20\":1,\"35\":5}}],[\"summer\",{\"1\":{\"3\":1}}],[\"summercamp\",{\"1\":{\"3\":1}}],[\"shift\",{\"1\":{\"16\":3,\"20\":2,\"24\":1}}],[\"solution\",{\"1\":{\"54\":2}}],[\"sota\",{\"1\":{\"34\":1}}],[\"some\",{\"0\":{\"33\":1}}],[\"source\",{\"1\":{\"16\":3,\"17\":1,\"19\":2,\"20\":2,\"24\":3,\"31\":3,\"32\":1}}],[\"sort\",{\"1\":{\"4\":1}}],[\"semi\",{\"1\":{\"20\":2,\"35\":5}}],[\"semantic\",{\"1\":{\"17\":1,\"19\":3,\"20\":1,\"21\":4,\"35\":4}}],[\"self\",{\"0\":{\"20\":1},\"1\":{\"35\":1}}],[\"segmentation\",{\"1\":{\"17\":1,\"19\":2,\"20\":1,\"21\":1,\"24\":1,\"26\":1,\"35\":5}}],[\"set\",{\"1\":{\"8\":1,\"9\":1,\"33\":5}}],[\"security\",{\"1\":{\"3\":1}}],[\"scalable\",{\"1\":{\"56\":1}}],[\"scorebaseline​\",{\"1\":{\"53\":1}}],[\"scorehuman​\",{\"1\":{\"53\":1}}],[\"script\",{\"1\":{\"5\":1}}],[\"scist\",{\"1\":{\"0\":1,\"3\":1}}],[\"st+i​\",{\"1\":{\"46\":3,\"51\":4}}],[\"state\",{\"1\":{\"43\":1,\"45\":1,\"46\":1}}],[\"start\",{\"1\":{\"2\":2}}],[\"strategies\",{\"1\":{\"56\":1}}],[\"structured\",{\"1\":{\"35\":1}}],[\"strong\",{\"1\":{\"35\":1}}],[\"string\",{\"1\":{\"7\":2}}],[\"stop\",{\"1\":{\"33\":1}}],[\"storage\",{\"1\":{\"5\":1}}],[\"stephan\",{\"1\":{\"29\":1}}],[\"領域的各種知識\",{\"1\":{\"0\":1}}],[\"領域發展\",{\"1\":{\"0\":1}}],[\"也說明了實際上這樣的更新方式是能夠依照不同的遊戲去適應的\",{\"1\":{\"54\":1}}],[\"也並不是每次加上\",{\"1\":{\"53\":1}}],[\"也可以達到類似的效果\",{\"1\":{\"49\":1}}],[\"也可以盡可能至少在\",{\"1\":{\"8\":1}}],[\"也跟最後評估的\",{\"1\":{\"33\":1}}],[\"也就不需要再使用\",{\"1\":{\"50\":1,\"51\":1}}],[\"也就意味著需要\",{\"1\":{\"49\":1}}],[\"也就如下\",{\"1\":{\"46\":1}}],[\"也就是說\",{\"1\":{\"54\":1}}],[\"也就是說最後的\",{\"1\":{\"54\":1}}],[\"也就是說對於一個參數\",{\"1\":{\"48\":1}}],[\"也就是說我們對於\",{\"1\":{\"20\":1}}],[\"也就是\",{\"1\":{\"44\":1,\"46\":1}}],[\"也就很難往下一步去發展\",{\"1\":{\"11\":1}}],[\"也獲得不錯的成果\",{\"1\":{\"17\":1}}],[\"也很期待未來也還有機會可以繼續了解和開發\",{\"1\":{\"11\":1}}],[\"也能感受到他們對我們的提問的重視\",{\"1\":{\"11\":1}}],[\"也讓我認識到\",{\"1\":{\"11\":1}}],[\"也寫了一個評分程式去評估好壞\",{\"1\":{\"7\":1}}],[\"也想說難得有不錯的運算資源\",{\"1\":{\"6\":1}}],[\"也有部分是源自於這樣的相似性帶來的好處\",{\"1\":{\"19\":1}}],[\"也有人套了\",{\"1\":{\"10\":1}}],[\"也有聽說有部分的組別\",{\"1\":{\"6\":1}}],[\"也有先提供了一些資源\",{\"1\":{\"5\":1}}],[\"也開放資源\",{\"1\":{\"6\":1}}],[\"也歡迎一起來討論\",{\"1\":{\"0\":1}}],[\"也是有些包含浮水印\",{\"1\":{\"7\":1}}],[\"也是頗有趣\",{\"1\":{\"6\":1}}],[\"也是\",{\"1\":{\"0\":1}}],[\"紀錄學習的點滴\",{\"1\":{\"0\":1}}],[\"multi\",{\"1\":{\"51\":1}}],[\"monte\",{\"1\":{\"48\":1}}],[\"mohammad\",{\"1\":{\"39\":1,\"53\":3,\"54\":1}}],[\"model\",{\"1\":{\"0\":1,\"5\":1,\"6\":1,\"20\":1,\"31\":1,\"44\":1}}],[\"mscoco\",{\"1\":{\"26\":1}}],[\"m\",{\"1\":{\"21\":3}}],[\"miou\",{\"1\":{\"32\":1}}],[\"mix\",{\"1\":{\"24\":1}}],[\"mixup\",{\"1\":{\"21\":1}}],[\"mixing\",{\"0\":{\"21\":1,\"23\":1},\"1\":{\"21\":3,\"23\":2,\"24\":2,\"26\":2}}],[\"mixed\",{\"0\":{\"14\":1,\"24\":1},\"1\":{\"23\":1,\"35\":1}}],[\"minigpt4\",{\"1\":{\"6\":2}}],[\"miscellaneous\",{\"1\":{\"1\":1}}],[\"my\",{\"1\":{\"3\":1}}],[\"myfirstctf\",{\"1\":{\"2\":1}}],[\"maximizing\",{\"1\":{\"56\":1}}],[\"maxb∈a​q\",{\"1\":{\"44\":1}}],[\"matthieu\",{\"1\":{\"40\":1}}],[\"marius\",{\"1\":{\"28\":1}}],[\"markdown\",{\"1\":{\"1\":1}}],[\"mask\",{\"1\":{\"21\":3,\"26\":1}}],[\"majchrowska\",{\"1\":{\"20\":1}}],[\"map\",{\"1\":{\"19\":1,\"21\":3}}],[\"maps\",{\"1\":{\"19\":2}}],[\"mandarin\",{\"1\":{\"1\":1}}],[\"machine\",{\"1\":{\"1\":1,\"2\":1,\"3\":1}}],[\"ml\",{\"1\":{\"0\":1}}],[\"median\",{\"1\":{\"53\":1}}],[\"medium\",{\"1\":{\"16\":2,\"46\":1}}],[\"mean\",{\"1\":{\"48\":2,\"53\":1}}],[\"meire\",{\"1\":{\"39\":1,\"53\":3,\"54\":1}}],[\"methods\",{\"1\":{\"56\":1}}],[\"method\",{\"1\":{\"34\":1}}],[\"methodology\",{\"0\":{\"22\":1,\"47\":1}}],[\"memory\",{\"1\":{\"6\":1,\"46\":1}}],[\"message\",{\"1\":{\"5\":3}}],[\"me\",{\"0\":{\"0\":1}}],[\"lˉ\",{\"1\":{\"48\":1,\"50\":2}}],[\"lv\",{\"1\":{\"46\":1,\"51\":2}}],[\"l\",{\"1\":{\"24\":1,\"43\":1,\"45\":1,\"46\":1,\"48\":2,\"50\":2}}],[\"loss\",{\"1\":{\"24\":1,\"42\":1,\"43\":1,\"45\":1,\"46\":3,\"48\":1,\"50\":1,\"54\":1}}],[\"local\",{\"1\":{\"19\":1}}],[\"lebel\",{\"1\":{\"24\":1}}],[\"level\",{\"1\":{\"19\":3}}],[\"learning\",{\"1\":{\"0\":2,\"1\":2,\"2\":1,\"3\":1,\"19\":1,\"20\":3,\"35\":5,\"43\":1,\"56\":3},\"2\":{\"58\":1}}],[\"liang\",{\"1\":{\"17\":1}}],[\"linear\",{\"1\":{\"48\":1}}],[\"line\",{\"1\":{\"1\":1}}],[\"llava\",{\"1\":{\"5\":2,\"6\":2}}],[\"llm\",{\"1\":{\"5\":1,\"6\":6,\"7\":1,\"10\":1,\"11\":2}}],[\"l4\",{\"1\":{\"5\":1}}],[\"layer\",{\"1\":{\"48\":1,\"54\":2}}],[\"layout\",{\"1\":{\"19\":2}}],[\"labelled\",{\"1\":{\"23\":1}}],[\"labelling\",{\"0\":{\"20\":1},\"1\":{\"21\":1,\"23\":1}}],[\"labeling\",{\"1\":{\"20\":1}}],[\"labeled\",{\"1\":{\"20\":2}}],[\"label\",{\"1\":{\"16\":2,\"20\":4,\"24\":1}}],[\"languagues\",{\"1\":{\"1\":1}}],[\"languages\",{\"1\":{\"1\":1}}],[\"language\",{\"1\":{\"0\":1}}],[\"large\",{\"1\":{\"0\":1}}],[\"關注的主題包含\",{\"1\":{\"0\":1}}],[\"本名林禾堃\",{\"1\":{\"0\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n}})=>{e==="suggest"?self.postMessage(st(t,v[s],n)):e==="search"?self.postMessage(et(t,v[s],n)):self.postMessage({suggestions:st(t,v[s],n),results:et(t,v[s],n)})};
//# sourceMappingURL=index.js.map
