const e=JSON.parse('{"key":"v-ed4def16","path":"/posts/Transformer.html","title":"Attention is all you need","lang":"en-US","frontmatter":{"date":"2024-05-22T00:00:00.000Z","category":["Note"],"tag":["Paper Read","NLP","Computer Vision","NeurIPS"],"description":"Attention is all you need Basic Information NIPS 2017 (former NeuralPS) Ashish Vaswani, Noam Shazeer, Niki Parmar et al. from Google Brain and Google Research 問題描述 RNN 近年來自然語言處理(Natural Language Processing, NLP)與機器翻譯等任務上時常使用 Recurrent Neural Network(RNN), Long Short-Term Memory(LSTM), Gated Recurrent Neural Network 等模型架構，我們也看到使用 Recurrent 模型以及 Encoder-Decoder 架構蔚為流行。","head":[["meta",{"property":"og:url","content":"https://mister-hope.github.io/posts/Transformer.html"}],["meta",{"property":"og:site_name","content":"Koios Blog"}],["meta",{"property":"og:title","content":"Attention is all you need"}],["meta",{"property":"og:description","content":"Attention is all you need Basic Information NIPS 2017 (former NeuralPS) Ashish Vaswani, Noam Shazeer, Niki Parmar et al. from Google Brain and Google Research 問題描述 RNN 近年來自然語言處理(Natural Language Processing, NLP)與機器翻譯等任務上時常使用 Recurrent Neural Network(RNN), Long Short-Term Memory(LSTM), Gated Recurrent Neural Network 等模型架構，我們也看到使用 Recurrent 模型以及 Encoder-Decoder 架構蔚為流行。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"article:tag","content":"Paper Read"}],["meta",{"property":"article:tag","content":"NLP"}],["meta",{"property":"article:tag","content":"Computer Vision"}],["meta",{"property":"article:tag","content":"NeurIPS"}],["meta",{"property":"article:published_time","content":"2024-05-22T00:00:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Attention is all you need\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-05-22T00:00:00.000Z\\",\\"dateModified\\":null,\\"author\\":[]}"]]},"headers":[{"level":2,"title":"Basic Information","slug":"basic-information","link":"#basic-information","children":[]},{"level":2,"title":"問題描述","slug":"問題描述","link":"#問題描述","children":[{"level":3,"title":"RNN","slug":"rnn","link":"#rnn","children":[]},{"level":3,"title":"CNN","slug":"cnn","link":"#cnn","children":[]},{"level":3,"title":"Self-Attention","slug":"self-attention","link":"#self-attention","children":[]}]},{"level":2,"title":"Related Works","slug":"related-works","link":"#related-works","children":[{"level":3,"title":"Recurrent Neural Network(RNN)","slug":"recurrent-neural-network-rnn","link":"#recurrent-neural-network-rnn","children":[]},{"level":3,"title":"Seq2Seq","slug":"seq2seq","link":"#seq2seq","children":[]},{"level":3,"title":"Attention","slug":"attention","link":"#attention","children":[]}]},{"level":2,"title":"Methodology","slug":"methodology","link":"#methodology","children":[{"level":3,"title":"Scaled Dot-Product Attention","slug":"scaled-dot-product-attention","link":"#scaled-dot-product-attention","children":[]},{"level":3,"title":"Masking","slug":"masking","link":"#masking","children":[]},{"level":3,"title":"Multi-head Attention","slug":"multi-head-attention","link":"#multi-head-attention","children":[]},{"level":3,"title":"Encoder and Decoder Stacks","slug":"encoder-and-decoder-stacks","link":"#encoder-and-decoder-stacks","children":[]},{"level":3,"title":"Position-wise Feed-Forward Networks","slug":"position-wise-feed-forward-networks","link":"#position-wise-feed-forward-networks","children":[]},{"level":3,"title":"Embeddings and Softmax","slug":"embeddings-and-softmax","link":"#embeddings-and-softmax","children":[]},{"level":3,"title":"Positional Encoding","slug":"positional-encoding","link":"#positional-encoding","children":[]}]},{"level":2,"title":"Results","slug":"results","link":"#results","children":[{"level":3,"title":"Why Self-Attention?","slug":"why-self-attention","link":"#why-self-attention","children":[]},{"level":3,"title":"實驗設定","slug":"實驗設定","link":"#實驗設定","children":[]},{"level":3,"title":"Model Variations 實驗結果","slug":"model-variations-實驗結果","link":"#model-variations-實驗結果","children":[]}]},{"level":2,"title":"Contribution","slug":"contribution","link":"#contribution","children":[]},{"level":2,"title":"值得一看的文章們","slug":"值得一看的文章們","link":"#值得一看的文章們","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"readingTime":{"minutes":16.03,"words":4810},"filePathRelative":"posts/Transformer.md","localizedDate":"May 22, 2024","excerpt":"<h1> Attention is all you need</h1>\\n<h2> Basic Information</h2>\\n<ul>\\n<li>NIPS 2017 (former NeuralPS)</li>\\n<li>Ashish Vaswani, Noam Shazeer, Niki Parmar et al. from Google Brain and Google Research</li>\\n</ul>\\n<h2> 問題描述</h2>\\n<h3> RNN</h3>\\n<p>近年來自然語言處理(Natural Language Processing, NLP)與機器翻譯等任務上時常使用 Recurrent Neural Network(RNN), Long Short-Term Memory(LSTM), Gated Recurrent Neural Network 等模型架構，我們也看到使用 Recurrent 模型以及 Encoder-Decoder 架構蔚為流行。</p>","autoDesc":true}');export{e as data};
